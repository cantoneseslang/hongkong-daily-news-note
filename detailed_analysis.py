#!/usr/bin/env python3
"""
è©³ç´°ãªè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡åˆ†æž
"""
import re

def normalize_title(t):
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–"""
    t = t.lower()
    t = re.sub(r'[^\w\s]', '', t)
    return set(t.split())

def analyze_pair(title1, title2):
    """2ã¤ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è©³ç´°åˆ†æž"""
    words1 = normalize_title(title1)
    words2 = normalize_title(title2)
    
    common = words1 & words2
    all_words = words1 | words2
    
    similarity = len(common) / len(all_words) if all_words else 0.0
    min_length = min(len(words1), len(words2))
    coverage = len(common) / min_length if min_length > 0 else 0.0
    
    return {
        'words1': words1,
        'words2': words2,
        'common': common,
        'similarity': similarity,
        'coverage': coverage,
        'min_length': min_length,
        'common_count': len(common)
    }

# 11/1ã®è¨˜äº‹
articles_11_01 = [
    "ç±³ä¸­è²¿æ˜“ä¼‘æˆ¦ã€é¦™æ¸¯ã®è¼¸å‡ºæ¥­è€…ã«ç¢ºå®Ÿæ€§ã¨å®‰å µã‚’ã‚‚ãŸã‚‰ã™",
    "ãƒãƒ­ã‚¦ã‚£ãƒ¼ãƒ³ã§ä¸­ç’°ãŒè³‘ã‚ã„ã€ãƒŠã‚¤ãƒˆãƒ©ã‚¤ãƒ•æ¥­ç•Œã¯æ´»æ³ã‚’äºˆæƒ³",
    "é¦™æ¸¯ã®æ”¾ç½®ã•ã‚ŒãŸæµ·è¾ºã®æ•·åœ°ã€ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚¨ã‚¯ã‚¤ãƒ†ã‚£ä¼æ¥­MBKã«ã‚ˆã‚Šæ–°ãŸãªå‘½ã‚’å¹ãè¾¼ã¾ã‚Œã‚‹",
    "é¦™æ¸¯ã®ãƒãƒ¼ãƒ«ãƒ»ãƒãƒ£ãƒ³è²¡æ”¿é•·å®˜ã€GDP3.8%å¢—ã§çµŒæ¸ˆã¯ã€Œé †èª¿ã€ã¨ç™ºè¨€",
    "ã€Œç§˜å¯†ã®ã‚½ãƒ¼ã‚¹ã€ãŒæ˜Žã‚‰ã‹ã«ã€ãƒ­ãƒ¼ã‚ºã‚¦ãƒƒãƒ‰é¦™æ¸¯ãŒä¸–ç•Œæœ€é«˜ã®ãƒ›ãƒ†ãƒ«ã«é¸å‡º",
    "é¦™æ¸¯ã®æ³•å»·å¼è­·å£«ãŒæœ€é«˜ã®è¡Œå‹•è¦ç¯„ã‚’ä¿ã¤ãŸã‚ã®ã€Œé»„é‡‘å¾‹ã€ã¨ã¯",
    "ç±³å›½ã€ä¸­å›½ã®ç¬¬ä¸€æ®µéšŽè²¿æ˜“å”å®šéµå®ˆçŠ¶æ³ã‚’å¼•ãç¶šãèª¿æŸ»",
    "ãƒ‡ãƒ­ã‚¤ãƒˆãƒ»ãƒãƒ£ã‚¤ãƒŠã€é¦™æ¸¯ã§1,000äººã‚’é›‡ç”¨ã—6,400ä¸‡ç±³ãƒ‰ãƒ«ã‚’æŠ•è³‡ã¸",
    "é¦™æ¸¯ã¯ã€Œã‚¢ã‚¸ã‚¢ã®ãƒ¨ãƒƒãƒˆã‚¯ãƒ©ãƒ–ã€ã«åŠ ã‚ã‚‹é“ç­‹ã‚’ã¤ã‘ãŸã€‚å®Ÿç¾ã§ãã‚‹ã‹ï¼Ÿ",
    "ä¸Šæ°´æ–°ç”°éº’éºŸæ‘ã®å€‰åº«ã§ä¸‰ç´šç«ç½ã€ç«å‹¢ã¯éŽ®åœ§ã•ã‚Œã‚‹ã‚‚ä½æ°‘ã¯çŒ«ã‚„çŠ¬ã‚’å¿ƒé…ã—æ¶™ï¼ˆæ›´æ–°ï¼‰",
    "åœŸåœ°åŽç”¨ã¯é•·æœŸçš„ãªè§£æ±ºç­–ã¨ãªã‚‹ã‹ï¼Ÿ",
    "åœ°åŒºè©•è­°ä¼šé¸æŒ™çµæžœã‹ã‚‰å­¦ã¶ã¹ãé‡è¦ãªæ•™è¨“"
]

# 11/2ã®è¨˜äº‹
articles_11_02 = [
    "é¦™æ¸¯ã®è²¡ç•Œãƒˆãƒƒãƒ—ã‚„åœ°åŸŸç¤¾ä¼šãŒå…¨å›½é‹å‹•ä¼šã®è–ç«ãƒªãƒ¬ãƒ¼ã«æº–å‚™",
    "ãƒ­ãƒ¼ã‚ºã‚¦ãƒƒãƒ‰é¦™æ¸¯ãŒä¸–ç•Œæœ€é«˜ã®ãƒ›ãƒ†ãƒ«ã«é¸å‡ºã€ã€Œç§˜ä¼ã®ã‚½ãƒ¼ã‚¹ã€ãŒæ˜Žã‚‰ã‹ã«",
    "ç±³ä¸­è²¿æ˜“åœæˆ¦ãŒé¦™æ¸¯ã®è¼¸å‡ºæ¥­è€…ã«ç¢ºå®Ÿæ€§ã¨å®‰å µã‚’ã‚‚ãŸã‚‰ã™",
    "ãƒãƒ­ã‚¦ã‚£ãƒ¼ãƒ³ã§ä¸­ç’°ãŒè³‘ã‚ã„ã€ãƒŠã‚¤ãƒˆãƒ©ã‚¤ãƒ•äº‹æ¥­è€…ã¯å¥½æ™¯æ°—ã‚’æœŸå¾…",
    "ã€Œé›²ç«¯å¯¾è«‡ã€ç¬¬ä¸‰ã‚·ãƒªãƒ¼ã‚ºç¬¬å››å›žï¼šå·¨æ¹¾æŠ€ç ”ã®è£´é‹’ç·è£ã«ç‹¬å ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ï¼ˆä¸Šï¼‰",
    "4ã¤ã®å·¨å¤§ãªç©ºæ°—æ³¨å…¥å¼å½«åˆ»ãŒãƒ“ã‚¯ãƒˆãƒªã‚¢ãƒãƒ¼ãƒãƒ¼ã‚’å·¡å›žã™ã‚‹æœ€çµ‚æ—¥ã€å¤šãã®å¸‚æ°‘ãŒè¦³è¦§",
    "å¤§åŸ”ä¸‰é–€ä»”ã§78æ­³ã®èº‰èˆ¹ä½œæ¥­å“¡ãŒæµ·ã«è»¢è½ã—æ­»äº¡",
    "ãƒ‡ãƒ­ã‚¤ãƒˆä¸­å›½ã€é¦™æ¸¯ã§1000äººã‚’é›‡ç”¨ã—6400ä¸‡ç±³ãƒ‰ãƒ«ã‚’æŠ•è³‡ã¸",
    "é¦™æ¸¯ã¯ã€Œã‚¢ã‚¸ã‚¢ã®ãƒ¨ãƒƒãƒˆã‚¯ãƒ©ãƒ–ã€ã¸ã®é“ã‚’æ­©ã‚€ã€‚ãã®å®Ÿç¾ã¯å¯èƒ½ã‹ï¼Ÿ",
    "ã‚¿ãƒ³ã‚¶ãƒ‹ã‚¢å¤§çµ±é ˜é¸çµæžœç™ºè¡¨ã€ç¾è·ãƒãƒƒã‚µãƒ³æ°ãŒé«˜å¾—ç¥¨ã§å½“é¸",
    "åœŸåœ°åŽç”¨ã¯é•·æœŸçš„ãªè§£æ±ºç­–ã¨ãªã‚‹ã‹ï¼Ÿ",
    "åŒºè­°ä¼šé¸æŒ™çµæžœã‹ã‚‰å­¦ã¶ã¹ãé‡è¦ãªæ•™è¨“",
    "ä¸­å›½ã¯ã‚ã‚‰ã‚†ã‚‹éŠ€è¡Œå±æ©Ÿã«å‚™ãˆã¦ã„ã‚‹",
    "å…¨é‹ä¼šé¦™æ¸¯è–ç«ãƒ©ãƒ³ãƒŠãƒ¼ãƒªã‚¹ãƒˆç™ºè¡¨ã€é¦™æ¸¯å“çƒã®ã‚¨ãƒ¼ã‚¹é»„éŽ®å»·ãŒç¬¬ä¸€èµ°è€…",
    "ä¸­å›½ã¨è‹±å›½ã®è­¦å¯Ÿã€å¤§è¦æ¨¡ãªä»®æƒ³é€šè²¨è©æ¬ºäº‹ä»¶ã§è³‡é‡‘å›žåŽã®ãŸã‚å”åŠ›",
    "é¦™æ¸¯ç«‹æ³•ä¼šé¸æŒ™ã®ç›´æŽ¥é¸æŒ™è­°å¸­ã‚’å·¡ã‚‹ç«¶äº‰ãŒæ¿€åŒ–",
    "ãƒ“ã‚¯ãƒˆãƒªã‚¢ãƒãƒ¼ãƒãƒ¼æµ·ä¸Šãƒ‘ãƒ¬ãƒ¼ãƒ‰ï½œã‚µãƒ—ãƒ©ã‚¤ã‚ºç™ºè¡¨ã€4ã¤ã®å·¨å¤§ãªæµ®éŠå½«åˆ»ãŒæ˜Žæ—¥ç™»å ´",
    "é¦™æ¸¯ã€ãƒŽãƒ¼ã‚¶ãƒ³ãƒ»ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹ã®è³‡é‡‘èª¿é”ã«ã‚¤ã‚¹ãƒ©ãƒ å‚µç™ºè¡Œã‚’æ¤œè¨Žï¼šãƒãƒ¼ãƒ«ãƒ»ãƒãƒ£ãƒ³"
]

print("=" * 100)
print("è©³ç´°ãªè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡åˆ†æž")
print("=" * 100)
print()

# æ˜Žã‚‰ã‹ã«åŒã˜è¨˜äº‹ã®ãƒšã‚¢ã‚’æ‰‹å‹•ã§å®šç¾©
obvious_duplicates = [
    (0, 2, "ç±³ä¸­è²¿æ˜“ä¼‘æˆ¦", "ç±³ä¸­è²¿æ˜“åœæˆ¦"),
    (1, 3, "ãƒãƒ­ã‚¦ã‚£ãƒ¼ãƒ³ä¸­ç’°", "ãƒãƒ­ã‚¦ã‚£ãƒ¼ãƒ³ä¸­ç’°"),
    (4, 1, "ãƒ­ãƒ¼ã‚ºã‚¦ãƒƒãƒ‰é¦™æ¸¯", "ãƒ­ãƒ¼ã‚ºã‚¦ãƒƒãƒ‰é¦™æ¸¯"),
    (7, 7, "ãƒ‡ãƒ­ã‚¤ãƒˆ", "ãƒ‡ãƒ­ã‚¤ãƒˆ"),
    (8, 8, "ãƒ¨ãƒƒãƒˆã‚¯ãƒ©ãƒ–", "ãƒ¨ãƒƒãƒˆã‚¯ãƒ©ãƒ–"),
    (10, 10, "åœŸåœ°åŽç”¨", "åœŸåœ°åŽç”¨"),
    (11, 11, "è©•è­°ä¼šé¸æŒ™", "åŒºè­°ä¼šé¸æŒ™"),
]

print("ã€å®Œå…¨ä¸€è‡´ï¼ˆæ­£è¦åŒ–å¾Œï¼‰ã€‘")
print("-" * 100)
for i, title1 in enumerate(articles_11_01):
    normalized1 = re.sub(r'[^\w\s]', '', title1.lower())
    for j, title2 in enumerate(articles_11_02):
        normalized2 = re.sub(r'[^\w\s]', '', title2.lower())
        if normalized1 == normalized2:
            print(f"âœ… 11/1 #{i+1} â†” 11/2 #{j+1}: å®Œå…¨ä¸€è‡´")
            print(f"   11/1: {title1}")
            print(f"   11/2: {title2}")
            print()

print("\nã€æ˜Žã‚‰ã‹ã«åŒã˜è¨˜äº‹ï¼ˆäººé–“ã®åˆ¤æ–­ï¼‰ã€‘")
print("-" * 100)
for i1_idx, i2_idx, keyword1, keyword2 in obvious_duplicates:
    title1 = articles_11_01[i1_idx]
    title2 = articles_11_02[i2_idx]
    analysis = analyze_pair(title1, title2)
    
    print(f"ðŸ” 11/1 #{i1_idx+1} â†” 11/2 #{i2_idx+1}")
    print(f"   11/1: {title1}")
    print(f"   11/2: {title2}")
    print(f"   å…±é€šå˜èªžæ•°: {analysis['common_count']} / {analysis['min_length']}")
    print(f"   é¡žä¼¼åº¦: {analysis['similarity']:.2%}")
    print(f"   ã‚«ãƒãƒ¬ãƒƒã‚¸: {analysis['coverage']:.2%}")
    print(f"   å…±é€šå˜èªž: {', '.join(sorted(analysis['common']))}")
    print()

print("\nã€ãã®ä»–ã®æ½œåœ¨çš„é‡è¤‡ã€‘")
print("-" * 100)
for i, title1 in enumerate(articles_11_01):
    for j, title2 in enumerate(articles_11_02):
        # æ—¢ã«ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ã¯ã‚¹ã‚­ãƒƒãƒ—
        is_checked = False
        for ci1, cj2, _, _ in obvious_duplicates:
            if i == ci1 and j == cj2:
                is_checked = True
                break
        
        if is_checked:
            continue
        
        normalized1 = re.sub(r'[^\w\s]', '', title1.lower())
        normalized2 = re.sub(r'[^\w\s]', '', title2.lower())
        if normalized1 == normalized2:
            continue
        
        analysis = analyze_pair(title1, title2)
        
        # é¡žä¼¼åº¦ãŒ40%ä»¥ä¸Šã¾ãŸã¯å…±é€šå˜èªžãŒ3ã¤ä»¥ä¸Šã§è¡¨ç¤º
        if analysis['similarity'] >= 0.4 or (analysis['common_count'] >= 3 and analysis['similarity'] >= 0.3):
            print(f"âš ï¸  11/1 #{i+1} â†” 11/2 #{j+1}")
            print(f"   11/1: {title1}")
            print(f"   11/2: {title2}")
            print(f"   é¡žä¼¼åº¦: {analysis['similarity']:.2%}, ã‚«ãƒãƒ¬ãƒƒã‚¸: {analysis['coverage']:.2%}, å…±é€šå˜èªž: {analysis['common_count']}å€‹")
            print(f"   å…±é€šå˜èªž: {', '.join(sorted(analysis['common']))}")
            print()

print("\nã€11/2ã®ã¿ã®æ–°è¦è¨˜äº‹ã€‘")
print("-" * 100)
checked_indices = set()
for _, j, _, _ in obvious_duplicates:
    checked_indices.add(j)

normalized_11_01 = [re.sub(r'[^\w\s]', '', t.lower()) for t in articles_11_01]

for j, title2 in enumerate(articles_11_02):
    if j in checked_indices:
        continue
    
    normalized2 = re.sub(r'[^\w\s]', '', title2.lower())
    is_new = True
    
    for normalized1 in normalized_11_01:
        if normalized1 == normalized2:
            is_new = False
            break
    
    if is_new:
        # é¡žä¼¼åº¦ãƒã‚§ãƒƒã‚¯
        for title1 in articles_11_01:
            analysis = analyze_pair(title1, title2)
            if analysis['similarity'] >= 0.4 or (analysis['common_count'] >= 4 and analysis['similarity'] >= 0.3):
                is_new = False
                break
    
    if is_new:
        print(f"âœ… 11/2 #{j+1}: {title2}")



