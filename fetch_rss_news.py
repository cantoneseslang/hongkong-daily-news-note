#!/usr/bin/env python3
"""
RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
"""

import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Set
import time
import json
from dateutil import parser as date_parser

class RSSNewsAPI:
    def __init__(self, history_file: str = 'daily-articles/processed_urls.json'):
        self.rss_feeds = {
            # ç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹
            'scmp_hongkong': 'https://www.scmp.com/rss/2/feed',
            'scmp_business': 'https://www.scmp.com/rss/5/feed',  # ãƒ“ã‚¸ãƒã‚¹
            'scmp_lifestyle': 'https://www.scmp.com/rss/322184/feed',  # ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«
            'rthk_news': 'https://rthk.hk/rthk/news/rss/e_expressnews_elocal.xml',
            'rthk_business': 'https://rthk.hk/rthk/news/rss/e_expressnews_ebusiness.xml',  # ãƒ“ã‚¸ãƒã‚¹
            'yahoo_hk': 'http://hk.news.yahoo.com/rss/hong-kong',
            'google_news_hk': 'http://news.google.com.hk/news?pz=1&cf=all&ned=hk&hl=zh-TW&output=rss',
            'chinadaily_hk': 'http://www.chinadaily.com.cn/rss/hk_rss.xml',
            'hkfp': 'https://www.hongkongfp.com/feed/',
            'hket_hk': 'https://www.hket.com/rss/hongkong',
            'hket_finance': 'https://www.hket.com/rss/finance',  # è²¡çµŒ
            'hket_property': 'https://www.hket.com/rss/property',  # ä¸å‹•ç”£
        }
        self.weather_feeds = {
            'weather_warning': 'https://rss.weather.gov.hk/rss/WeatherWarningSummaryv2_uc.xml',
            'weather_forecast': 'https://rss.weather.gov.hk/rss/LocalWeatherForecast_uc.xml',
            'current_weather': 'https://rss.weather.gov.hk/rss/CurrentWeather_uc.xml',
            'nine_day_forecast': 'https://rss.weather.gov.hk/rss/SeveralDaysWeatherForecast_uc.xml',
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.history_file = history_file
        self.processed_urls = self._load_processed_urls()
    
    def _load_processed_urls(self) -> Set[str]:
        """å‡¦ç†æ¸ˆã¿URLã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return set(data.get('urls', []))
        except FileNotFoundError:
            return set()
        except Exception as e:
            print(f"âš ï¸  å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return set()
    
    def _save_processed_urls(self):
        """å‡¦ç†æ¸ˆã¿URLã‚’ä¿å­˜"""
        try:
            # æœ€è¿‘30æ—¥åˆ†ã®ã¿ä¿æŒï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            cutoff_date = datetime.now() - timedelta(days=30)
            
            data = {
                'last_updated': datetime.now().isoformat(),
                'urls': list(self.processed_urls)
            }
            
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _is_today_news(self, published_at: str) -> bool:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒéå»24æ™‚é–“ä»¥å†…ã®ã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯"""
        if not published_at:
            return True  # æ—¥ä»˜ä¸æ˜ã¯å«ã‚ã‚‹
        
        try:
            pub_date = date_parser.parse(published_at)
            now = datetime.now()
            
            # éå»48æ™‚é–“ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å«ã‚ã‚‹ï¼ˆã‚ˆã‚Šå¤šæ§˜ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼‰
            time_diff = now - pub_date.replace(tzinfo=None)
            return time_diff.total_seconds() <= 48 * 3600
        except:
            return True  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã¯å«ã‚ã‚‹
    
    def _is_forbidden_content(self, title: str, description: str) -> bool:
        """ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ãƒã‚§ãƒƒã‚¯"""
        text = (title + ' ' + description).lower()
        
        # ç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        forbidden_keywords = [
            'wwii', 'ww2', 'world war ii', 'äºŒæ¬¡å¤§æˆ¦', 'æŠ—æ—¥', 'æ—¥æœ¬ä¾µç•¥',
            'japanese invasion', 'sino-japanese war', 'ä¸­æ—¥æˆ˜äº‰', 'æŠ—æˆ°',
            'envoys given special tour', 'exhibition on chinese victory',
            # ã‚®ãƒ£ãƒ³ãƒ–ãƒ«é–¢é€£
            'horse racing', 'jockey', 'mark six', 'lottery',
            'ç«¶é¦¬', 'è³½é¦¬', 'é¨å¸«', 'å…­åˆå½©', 'è³­åš', 'åšå½©', 'casino', 'gambling',
            'boat racing', 'ç«¶è‰‡', 'betting',
            # æ”¿æ²»é–¢é€£ï¼ˆä¸è¦ï¼‰
            '47äºº', '47 persons', '47 activists', 'democracy trial',
            'åˆ‘æœŸæº€äº†', 'prison term', 'sentence completion', 'prison release',
            'æ°‘ä¸»æ´¾', 'democratic', 'democrats', 'pro-democracy',
            'ç«‹æ³•ä¼šé¸æŒ™', 'legislative council election', 'legco election',
            'å›½å®¶å®‰å…¨å…¬ç½²', 'national security office', 'nsa', 'nsf', 'national security law',
            'å›½å®‰æ³•', 'å›½å®¶å®‰å…¨æ³•', 'national security', 'å›½å®‰å…¬ç½²',
            # æ”¿æ²»çŠ¯ç½ªé–¢é€£ï¼ˆè‹±èªï¼‰
            'jailed', 'prison', 'sentenced', 'conspiracy', 'overthrow', 'subversion',
            '2019 protest', 'pro-democracy activist', 'political prisoner'
        ]
        
        for keyword in forbidden_keywords:
            if keyword.lower() in text:
                return True
        
        # é¦™æ¸¯ç„¡é–¢ä¿‚ã®å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã§åˆ¤å®šï¼‰
        non_hk_keywords = [
            'gaza', 'israel', 'hamas', 'rafah', 'palestine',
            'iran', 'ukraine', 'russia', 'zelensky',
            'brazil', 'ecuador', 'kenya', 'afghanistan',
            'british', 'prince andrew', 'david attenborough',
            'myanmar', 'starlink',
            'åŠ è–©', 'ä»¥è‰²åˆ—', 'å“ˆç‘ªæ–¯', 'å·´å‹’æ–¯å¦',
            'çƒå…‹è˜­', 'ä¿„ç¾…æ–¯', 'æ¾¤é€£æ–¯åŸº',
            'é‡‘é˜ç', 'é™³å‰éœ†', 'å°ç£',
            'golden horse', 'taiwan election',
            'sudan', 'khartoum', 'å–€åœŸç©†', 'ã‚¹ãƒ¼ãƒ€ãƒ³',
            'trump', 'oracle', 'amazon', 'exxonmobil',
            'ãƒˆãƒ©ãƒ³ãƒ—', 'ç±³å›½ãƒ“ã‚¸ãƒã‚¹', 'ç±³ä¸­'
        ]
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã§ãƒã‚§ãƒƒã‚¯ï¼ˆé¦™æ¸¯é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°OKï¼‰
        hk_keywords = ['hong kong', 'hongkong', 'hk', 'é¦™æ¸¯', 'central', 'kowloon', 'wan chai', 'mtr', 'æ¸¯']
        has_hk = any(k in text for k in hk_keywords)
        
        if not has_hk:
            # é¦™æ¸¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã€å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            for keyword in non_hk_keywords:
                if keyword in title.lower():
                    return True
        
        return False
    
    def _is_duplicate_content(self, title: str, existing_titles: List[str]) -> bool:
        """ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹åˆ¤å®š"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ï¼‰
        def normalize_title(t):
            import re
            t = t.lower()
            # è¨˜å·ã¨æ•°å­—ã‚’é™¤å»
            t = re.sub(r'[^\w\s]', '', t)
            return set(t.split())
        
        title_words = normalize_title(title)
        
        for existing in existing_titles:
            existing_words = normalize_title(existing)
            
            # å…±é€šå˜èªã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
            common_words = title_words & existing_words
            
            # 3å˜èªä»¥ä¸Šå…±é€š ã‹ã¤ å…±é€šç‡ãŒ60%ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã¿ãªã™
            if len(common_words) >= 3:
                similarity = len(common_words) / max(len(title_words), len(existing_words))
                if similarity >= 0.6:
                    return True
        
        return False
    
    def fetch_scmp_rss(self) -> List[Dict]:
        """SCMPï¼ˆSouth China Morning Postï¼‰ã®RSSã‚’å–å¾—"""
        print("ğŸ“° SCMP RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['scmp_hongkong'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'SCMP',
                    'api_source': 'rss_scmp'
                })
                
                # å‡¦ç†æ¸ˆã¿URLã«è¿½åŠ 
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_rthk_rss(self) -> List[Dict]:
        """RTHKï¼ˆRadio Television Hong Kongï¼‰ã®RSSã‚’å–å¾—"""
        print("ğŸ“° RTHK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['rthk_news'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'RTHK',
                    'api_source': 'rss_rthk'
                })
                
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_yahoo_rss(self) -> List[Dict]:
        """Yahoo News HKã®RSSã‚’å–å¾—"""
        print("ğŸ“° Yahoo News HK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['yahoo_hk'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'Yahoo News HK',
                    'api_source': 'rss_yahoo_hk'
                })
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_google_news_rss(self) -> List[Dict]:
        """Google News HKã®RSSã‚’å–å¾—"""
        print("ğŸ“° Google News HK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['google_news_hk'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'Google News HK',
                    'api_source': 'rss_google_news_hk'
                })
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_chinadaily_rss(self) -> List[Dict]:
        """China Daily HKã®RSSã‚’å–å¾—"""
        print("ğŸ“° China Daily HK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['chinadaily_hk'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'China Daily HK',
                    'api_source': 'rss_chinadaily_hk'
                })
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_hkfp_rss(self) -> List[Dict]:
        """Hong Kong Free Pressã®RSSã‚’å–å¾—"""
        print("ğŸ“° Hong Kong Free Press RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['hkfp'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'Hong Kong Free Press',
                    'api_source': 'rss_hkfp'
                })
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_hket_rss(self) -> List[Dict]:
        """HKETï¼ˆé¦™æ¸¯çµŒæ¸ˆæ—¥å ±ï¼‰ã®RSSã‚’å–å¾—ï¼ˆUser-Agentå¿…è¦ï¼‰"""
        print("ğŸ“° HKET é¦™æ¸¯ RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            response = requests.get(self.rss_feeds['hket_hk'], headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                news_list = []
                filtered_count = 0
                
                for entry in feed.entries[:50]:
                    url = entry.get('link', '')
                    published_at = entry.get('published', entry.get('updated', ''))
                    title = entry.get('title', '')
                    description = entry.get('summary', entry.get('description', ''))
                    
                    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self._is_today_news(published_at):
                        filtered_count += 1
                        continue
                    
                    # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if self._is_forbidden_content(title, description):
                        filtered_count += 1
                        continue
                    
                    news_list.append({
                        'title': title,
                        'description': description,
                        'url': url,
                        'published_at': published_at,
                        'source': 'HKET',
                        'api_source': 'rss_hket'
                    })
                    self.processed_urls.add(url)
                
                print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
                return news_list
            else:
                print(f"  âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return []
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_generic_rss(self, feed_key: str, source_name: str, use_headers: bool = False) -> List[Dict]:
        """æ±ç”¨RSSå–å¾—é–¢æ•°"""
        print(f"ğŸ“° {source_name} RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            if use_headers:
                response = requests.get(self.rss_feeds[feed_key], headers=self.headers, timeout=5)
                feed = feedparser.parse(response.content)
            else:
                feed = feedparser.parse(self.rss_feeds[feed_key])
            
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:50]:
                url = entry.get('link', '')
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': source_name,
                    'api_source': feed_key
                })
                self.processed_urls.add(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_all_rss(self) -> List[Dict]:
        """å…¨RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—"""
        print("\nğŸš€ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹")
        print("=" * 60)
        
        all_news = []
        existing_titles = []
        duplicate_count = 0
        
        # å„RSSã‹ã‚‰å–å¾—ï¼ˆæ—¢å­˜ã®é–¢æ•°ï¼‰
        feeds_to_fetch = [
            (self.fetch_scmp_rss, None, None),
            (self.fetch_generic_rss, 'scmp_business', 'SCMP Business'),
            (self.fetch_generic_rss, 'scmp_lifestyle', 'SCMP Lifestyle'),
            (self.fetch_rthk_rss, None, None),
            (self.fetch_generic_rss, 'rthk_business', 'RTHK Business'),
            (self.fetch_yahoo_rss, None, None),
            (self.fetch_google_news_rss, None, None),
            (self.fetch_chinadaily_rss, None, None),
            (self.fetch_hkfp_rss, None, None),
            (self.fetch_hket_rss, None, None),
            (self.fetch_generic_rss, 'hket_finance', 'HKET Finance'),
            (self.fetch_generic_rss, 'hket_property', 'HKET Property'),
        ]
        
        for feed_info in feeds_to_fetch:
            func = feed_info[0]
            if feed_info[1] is None:
                # æ—¢å­˜ã®å°‚ç”¨é–¢æ•°
                news_items = func()
            else:
                # æ±ç”¨é–¢æ•°
                news_items = func(feed_info[1], feed_info[2], feed_info[1].startswith('hket'))
            
            for news in news_items:
                if not self._is_duplicate_content(news['title'], existing_titles):
                    all_news.append(news)
                    existing_titles.append(news['title'])
                else:
                    duplicate_count += 1
            
            time.sleep(0.5)  # 1ç§’ â†’ 0.5ç§’ã«çŸ­ç¸®
        
        # å‡¦ç†æ¸ˆã¿URLã‚’ä¿å­˜
        self._save_processed_urls()
        
        print("=" * 60)
        print(f"âœ… åˆè¨ˆ {len(all_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        print(f"ğŸ”„ é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶")
        print(f"ğŸ“ å‡¦ç†æ¸ˆã¿URLç·æ•°: {len(self.processed_urls)}ä»¶\n")
        
        return all_news
    
    def fetch_weather_info(self) -> Dict:
        """é¦™æ¸¯å¤©æ–‡å°ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—"""
        print("\nğŸŒ¤ï¸  å¤©æ°—æƒ…å ±å–å¾—é–‹å§‹")
        print("=" * 60)
        
        weather_data = {}
        
        for key, url in self.weather_feeds.items():
            try:
                feed = feedparser.parse(url)
                if len(feed.entries) > 0:
                    entry = feed.entries[0]
                    weather_data[key] = {
                        'title': entry.get('title', ''),
                        'description': entry.get('description', entry.get('summary', '')),
                        'published_at': entry.get('published', entry.get('updated', '')),
                    }
                    print(f"  âœ… {key} å–å¾—å®Œäº†")
            except Exception as e:
                print(f"  âŒ {key} ã‚¨ãƒ©ãƒ¼: {e}")
        
        print("=" * 60)
        print(f"âœ… å¤©æ°—æƒ…å ± {len(weather_data)}ä»¶å–å¾—\n")
        
        return weather_data

if __name__ == "__main__":
    import json
    from scrape_article import ArticleScraper
    
    rss_api = RSSNewsAPI()
    news = rss_api.fetch_all_rss()
    weather = rss_api.fetch_weather_info()
    
    if news:
        # å…¨æ–‡å–å¾—å‡¦ç†ã‚’è¿½åŠ 
        print("\nğŸ“° è¨˜äº‹å…¨æ–‡ã‚’å–å¾—ä¸­...")
        print("=" * 60)
        
        scraper = ArticleScraper()
        enriched_news = []
        
        for i, item in enumerate(news, 1):
            print(f"\n[{i}/{len(news)}] {item['title'][:60]}...")
            
            # URLã‹ã‚‰å…¨æ–‡ã‚’å–å¾—
            full_content = scraper.scrape_article(item['url'])
            
            if full_content:
                item['full_content'] = full_content
                print(f"    âœ… {len(full_content)}æ–‡å­—å–å¾—")
            else:
                # å–å¾—å¤±æ•—æ™‚ã¯descriptionã‚’ä½¿ç”¨
                item['full_content'] = item.get('description', '')
                print(f"    âš ï¸  å…¨æ–‡å–å¾—å¤±æ•—ã€descriptionã‚’ä½¿ç”¨")
            
            enriched_news.append(item)
        
        print("\n" + "=" * 60)
        print(f"âœ… å…¨æ–‡å–å¾—å®Œäº†: {len(enriched_news)}ä»¶\n")
        
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        output_path = f"daily-articles/rss_news_{timestamp}.json"
        
        data = {
            'fetch_time': datetime.now().isoformat(),
            'total_count': len(enriched_news),
            'news': enriched_news,
            'weather': weather
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_path}")
        
        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        print("\nğŸ“‹ å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
        for i, item in enumerate(enriched_news[:5], 1):
            print(f"\n{i}. {item['title']}")
            print(f"   ã‚½ãƒ¼ã‚¹: {item['source']} ({item['api_source']})")
            print(f"   å…¨æ–‡: {len(item.get('full_content', ''))}æ–‡å­—")
        
        if weather:
            print("\nğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚å–å¾—ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
