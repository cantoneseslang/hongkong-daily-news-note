#!/usr/bin/env python3
"""
RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
"""

import feedparser
import requests
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Set
import time
import json
import re
from dateutil import parser as date_parser

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

class RSSNewsAPI:
    def __init__(self, history_file: str = 'daily-articles/processed_urls.json'):
        self.rss_feeds = {
            # ç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹
            'scmp_hongkong': 'https://www.scmp.com/rss/2/feed',
            'scmp_business': 'https://www.scmp.com/rss/5/feed',  # ãƒ“ã‚¸ãƒã‚¹
            'scmp_lifestyle': 'https://www.scmp.com/rss/322184/feed',  # ãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«
            'rthk_news': 'https://rthk.hk/rthk/news/rss/e_expressnews_elocal.xml',
            'rthk_business': 'https://rthk.hk/rthk/news/rss/e_expressnews_ebusiness.xml',  # ãƒ“ã‚¸ãƒã‚¹
            'yahoo_hk': 'http://hk.news.yahoo.com/rss/hong-kong',
            # 'google_news_hk': 'http://news.google.com.hk/news?pz=1&cf=all&ned=hk&hl=zh-TW&output=rss',  # â† ä¸–ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒæ··å…¥ã™ã‚‹ãŸã‚ç„¡åŠ¹åŒ–
            'chinadaily_hk': 'http://www.chinadaily.com.cn/rss/hk_rss.xml',
            'hkfp': 'https://www.hongkongfp.com/feed/',
            'hket_hk': 'https://www.hket.com/rss/hongkong',
            'hket_finance': 'https://www.hket.com/rss/finance',  # è²¡çµŒ
            'hket_property': 'https://www.hket.com/rss/property',  # ä¸å‹•ç”£
        }
        self.weather_feeds = {
            'weather_warning': 'https://rss.weather.gov.hk/rss/WeatherWarningSummaryv2_uc.xml',
            'weather_forecast': 'https://rss.weather.gov.hk/rss/LocalWeatherForecast_uc.xml',
            'current_weather': 'https://rss.weather.gov.hk/rss/CurrentWeather_uc.xml',
            'nine_day_forecast': 'https://rss.weather.gov.hk/rss/SeveralDaysWeatherForecast_uc.xml',
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.history_file = history_file
        self.processed_urls = self._load_processed_urls()
    
    def _is_hk_related(self, title: str, description: str, url: str, source_name: str = "") -> bool:
        """é¦™æ¸¯é–¢é€£ã‹ã‚’å³æ ¼åˆ¤å®šï¼ˆã‚¿ã‚¤ãƒˆãƒ«/èª¬æ˜/URL/ã‚½ãƒ¼ã‚¹ï¼‰"""
        text = f"{title} {description}".lower()
        url_l = (url or "").lower()
        source_l = (source_name or "").lower()

        # å¼·ã„è‚¯å®šæ¡ä»¶
        positive = [
            'hong kong', 'hongkong', 'é¦™æ¸¯', 'kowloon', 'ä¹é¾', 'æ–°ç•Œ', 'é¦™æ¸¯ç‰¹åˆ¥è¡Œæ”¿å€', 'hksar',
            'tsim sha tsui', 'å°–æ²™å’€', 'wan chai', 'ç£ä»”', 'central', 'ä¸­ç’°', 'mong kok', 'æ—ºè§’',
            'rthk', 'hk01', 'hket', 'scmp', 'the standard', 'chinadaily', 'nowæ–°è', 'now news',
            'hong kong observatory', 'é¦™æ¸¯å¤©æ–‡å°', 'hkex', 'é¦™æ¸¯äº¤æ˜“æ‰€', 'mtr', 'æ¸¯éµ'
        ]
        if any(p in text for p in positive):
            return True

        # URLã§ã®è‚¯å®šï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ»ãƒ‘ã‚¹ï¼‰
        url_positive = [
            '/hong-kong', '/hongkong', '/news/hong-kong', '/category/hong-kong',
            '.hk/', '.hk?', '.hk#'
        ]
        if any(u in url_l for u in url_positive):
            return True

        # ç²¤æ¸¯æ¾³å¤§æ¹¾åŒºãƒ»åºƒæ±çœï¼ˆæ·±åœ³/æ±è/åºƒå·/ç æµ·/ä½›å±±/æƒ å·/ä¸­å±± ç­‰ï¼‰ã‚’é¦™æ¸¯åœã¨ã—ã¦è¨±å¯
        gba_terms = [
            'greater bay area', 'gba', 'ç²µæ¸¯æ¾³å¤§ç£å€', 'ç²¤æ¸¯æ¾³å¤§æ¹¾åŒº', 'å¤§æ¹¾åŒº', 'ç ä¸‰è§’',
            'guangdong', 'shenzhen', 'dongguan', 'guangzhou', 'foshan', 'zhuhai', 'huizhou', 'zhongshan', 'jiangmen', 'zhaoqing',
            'æ·±åœ³', 'æ·±ã‚»ãƒ³', 'ä¸œè', 'æ±è', 'å¹¿å·', 'åºƒå·', 'ç æµ·', 'ä½›å±±', 'æƒ å·', 'ä¸­å±±', 'æ±Ÿé–€', 'æ±Ÿé—¨', 'è‚‡æ…¶', 'è‚‡åº†'
        ]
        if any(t in text for t in gba_terms) or any(seg in url_l for seg in ['/greater-bay-area', '/gba/']):
            return True

        # SCMPã¯Business/Lifestyleãªã©ä¸–ç•Œè¨˜äº‹ãŒæ··ã–ã‚‹ãŸã‚ã€URLã§é¦™æ¸¯ãƒ‘ã‚¹å¿…é ˆ
        if 'scmp' in source_l or 'scmp.com' in url_l:
            return ('/hong-kong' in url_l) or ('/hongkong' in url_l) or ('/news/hong-kong' in url_l)

        # Yahoo HKã¯ä¸–ç•Œè¨˜äº‹ãŒæ··ã–ã‚‹ãŸã‚ã€URLã ã‘ã§ã¯è¨±å¯ã—ãªã„ï¼ˆæœ¬æ–‡/ã‚¿ã‚¤ãƒˆãƒ«ã«é¦™æ¸¯ç³»èªãŒå¿…è¦ï¼‰
        if 'yahoo' in url_l and 'hk.news.yahoo.com' in url_l:
            return any(p in text for p in positive) or any(u in url_l for u in url_positive)

        return False

    def _load_processed_urls(self) -> Dict[str, str]:
        """å‡¦ç†æ¸ˆã¿URLã‚’èª­ã¿è¾¼ã¿ï¼ˆURL â†’ ISOæ—¥æ™‚ï¼‰"""
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                urls = data.get('urls', {})
                if isinstance(urls, dict):
                    return {self._normalize_url(k): v for k, v in urls.items()}
                elif isinstance(urls, list):
                    timestamp = data.get('last_updated') or datetime.now(HKT).isoformat()
                    return {self._normalize_url(u): timestamp for u in urls if u}
                else:
                    return {}
        except FileNotFoundError:
            return {}
        except Exception as e:
            print(f"âš ï¸  å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _save_processed_urls(self):
        """å‡¦ç†æ¸ˆã¿URLã‚’ä¿å­˜"""
        try:
            cutoff_date = datetime.now(HKT) - timedelta(days=60)
            pruned = {}
            for url, ts in self.processed_urls.items():
                try:
                    ts_dt = date_parser.parse(ts)
                except Exception:
                    ts_dt = datetime.now(HKT)
                if ts_dt.replace(tzinfo=HKT) >= cutoff_date:
                    pruned[url] = ts_dt.isoformat()
            self.processed_urls = pruned
            data = {
                'last_updated': datetime.now(HKT).isoformat(),
                'urls': pruned
            }
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _mark_url_as_processed(self, url: str):
        normalized = self._normalize_url(url)
        if not normalized:
            return
        self.processed_urls[normalized] = datetime.now(HKT).isoformat()

    def _has_been_processed(self, url: str) -> bool:
        normalized = self._normalize_url(url)
        return normalized in self.processed_urls

    def _is_url_too_old(self, url: str, max_age_years: int = 2) -> bool:
        if not url:
            return False
        match = re.search(r'(20\d{2})', url)
        if match:
            try:
                year = int(match.group(1))
                current_year = datetime.now(HKT).year
                return year < current_year - max_age_years
            except ValueError:
                return False
        return False
    
    def _is_today_news(self, published_at: str) -> bool:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒéå»24æ™‚é–“ä»¥å†…ã®ã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡é˜²æ­¢ã®ãŸã‚24æ™‚é–“ã«æˆ»ã™ï¼‰"""
        if not published_at:
            return True  # æ—¥ä»˜ä¸æ˜ã¯å«ã‚ã‚‹
        
        try:
            pub_date = date_parser.parse(published_at)
            now = datetime.now(HKT)
            
            # éå»24æ™‚é–“ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ã‚’å«ã‚ã‚‹ï¼ˆ48æ™‚é–“ã ã¨åŒã˜ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ2æ—¥ç¶šã‘ã¦å–å¾—ã•ã‚Œã‚‹ï¼‰
            time_diff = now - pub_date.replace(tzinfo=None)
            return time_diff.total_seconds() <= 24 * 3600
        except:
            return True  # ãƒ‘ãƒ¼ã‚¹å¤±æ•—ã¯å«ã‚ã‚‹
    
    def _is_forbidden_content(self, title: str, description: str) -> bool:
        """ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ãƒã‚§ãƒƒã‚¯"""
        text = (title + ' ' + description).lower()
        
        # ç¦æ­¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        forbidden_keywords = [
            'wwii', 'ww2', 'world war ii', 'äºŒæ¬¡å¤§æˆ¦', 'æŠ—æ—¥', 'æ—¥æœ¬ä¾µç•¥',
            'japanese invasion', 'sino-japanese war', 'ä¸­æ—¥æˆ˜äº‰', 'æŠ—æˆ°',
            'envoys given special tour', 'exhibition on chinese victory',
            # ã‚®ãƒ£ãƒ³ãƒ–ãƒ«é–¢é€£
            'horse racing', 'jockey', 'mark six', 'lottery',
            'ç«¶é¦¬', 'è³½é¦¬', 'é¨å¸«', 'å…­åˆå½©', 'è³­åš', 'åšå½©', 'casino', 'gambling',
            'boat racing', 'ç«¶è‰‡', 'betting',
            # æ„ŸæŸ“ç—‡é–¢é€£ï¼ˆè©³ç´°è¨˜äº‹ã¯ä¸è¦ï¼‰
            'åŸºå­”è‚¯é›…ç†±', 'chikungunya', 'ç™»é©ç†±', 'dengue', 'ç–«æƒ…', 'epidemic',
            'ç¢ºè¨º', 'confirmed case', 'è¼¸å…¥å€‹æ¡ˆ', 'æœ¬åœ°æ„ŸæŸ“', 'local infection',
            # æ”¿æ²»é–¢é€£ï¼ˆä¸è¦ï¼‰
            '47äºº', '47 persons', '47 activists', 'democracy trial',
            'åˆ‘æœŸæº€äº†', 'prison term', 'sentence completion', 'prison release',
            'æ°‘ä¸»æ´¾', 'democratic', 'democrats', 'pro-democracy',
            'ç«‹æ³•ä¼šé¸æŒ™', 'legislative council election', 'legco election',
            'å›½å®¶å®‰å…¨å…¬ç½²', 'national security office', 'nsa', 'nsf', 'national security law',
            'å›½å®‰æ³•', 'å›½å®¶å®‰å…¨æ³•', 'national security', 'å›½å®‰å…¬ç½²',
            # æ”¿æ²»çŠ¯ç½ªé–¢é€£ï¼ˆè‹±èªï¼‰
            'jailed', 'prison', 'sentenced', 'conspiracy', 'overthrow', 'subversion',
            '2019 protest', 'pro-democracy activist', 'political prisoner',
            # åºƒå‘Šãƒ»å®£ä¼è¨˜äº‹é–¢é€£
            'presented', 'sponsored', 'advertisement', 'advertorial', 'promotional',
            'åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼', 'åºƒå‘Šè¨˜äº‹', 'ã‚¹ãƒãƒ³ã‚µãƒ¼è¨˜äº‹', 'PRè¨˜äº‹', 'presented news',
            'building stronger communities through sports'  # å…·ä½“çš„ãªåºƒå‘Šè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
        ]
        # æ¡ç”¨ãƒ»å‹Ÿé›†ï¼ˆæ±‚äºº/è·ç¼º/æ‹›è˜/å¾µæ‰ï¼‰ç³»ã¯é™¤å¤–
        recruit_keywords = [
            'recruit', 'recruiting', 'recruitment', 'hiring', 'we are hiring', 'career', 'job opening', 'vacancies', 'vacancy',
            'å‹Ÿé›†', 'æ±‚äºº', 'æ¡ç”¨', 'äººæå‹Ÿé›†', 'è·ç¨®å‹Ÿé›†', 'ã‚­ãƒ£ãƒªã‚¢', 'æ¡ç”¨æƒ…å ±', 'æ¡ç”¨ã®ãŠçŸ¥ã‚‰ã›',
            'æ‹›è˜', 'æ‹›è˜å•Ÿäº‹', 'è·ä½ç©ºç¼º', 'è·ç¼º', 'å¾µæ‰', 'æ‹›å‹Ÿ', 'æ‹›è³¢ç´å£«'
        ]
        
        for keyword in forbidden_keywords + recruit_keywords:
            if keyword.lower() in text:
                return True
        
        # é¦™æ¸¯ç„¡é–¢ä¿‚ã®å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã§åˆ¤å®šï¼‰
        non_hk_keywords = [
            'gaza', 'israel', 'hamas', 'rafah', 'palestine',
            'iran', 'ukraine', 'russia', 'zelensky',
            'brazil', 'ecuador', 'kenya', 'afghanistan',
            'british', 'prince andrew', 'david attenborough',
            'myanmar', 'starlink',
            'cuba', 'haiti', 'jamaica', 'hurricane', 'melissa',
            'cote d\'ivoire', 'ivory coast', 'wattara', 'ouattara',
            'rio de janeiro', 'drug', 'cartel', 'operation',
            'åŠ è–©', 'ä»¥è‰²åˆ—', 'å“ˆç‘ªæ–¯', 'å·´å‹’æ–¯å¦',
            'çƒå…‹è˜­', 'ä¿„ç¾…æ–¯', 'æ¾¤é€£æ–¯åŸº',
            'é‡‘é˜ç', 'é™³å‰éœ†', 'å°ç£',
            'golden horse', 'taiwan election',
            'sudan', 'khartoum', 'å–€åœŸç©†', 'ã‚¹ãƒ¼ãƒ€ãƒ³',
            'trump', 'oracle', 'amazon', 'exxonmobil',
            'ãƒˆãƒ©ãƒ³ãƒ—', 'ç±³å›½ãƒ“ã‚¸ãƒã‚¹', 'ç±³ä¸­',
            'ã‚­ãƒ¥ãƒ¼ãƒ', 'ãƒã‚¤ãƒ', 'ã‚¸ãƒ£ãƒã‚¤ã‚«', 'ãƒãƒªã‚±ãƒ¼ãƒ³',
            'ã‚³ãƒ¼ãƒˆã‚¸ãƒœãƒ¯ãƒ¼ãƒ«', 'ãƒ–ãƒ©ã‚¸ãƒ«', 'ãƒªã‚ªãƒ‡ã‚¸ãƒ£ãƒã‚¤ãƒ­'
        ]
        
        # é¦™æ¸¯é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ2024-2025å¹´æœ€æ–°ç‰ˆï¼‰
        hk_keywords = [
            # åŸºæœ¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            'hong kong', 'hongkong', 'hk', 'é¦™æ¸¯', 'æ¸¯',
            
            # ä¸»è¦åœ°åŒºãƒ»åœ°å
            'central', 'kowloon', 'wan chai', 'causeway bay', 'tai koo', 'admiralty',
            'tsim sha tsui', 'victoria harbour', 'lantau', 'kwai chung', 'tin shui wai', 
            'tiu keng leng', 'sha tin', 'mong kok', 'yau ma tei', 'jordan', 'tai po',
            'ä¸­ç’°', 'ä¹é¾', 'ç£ä»”', 'éŠ…é‘¼ç£', 'å¤ªå¤', 'é‡‘é˜', 'å°–æ²™å’€', 'æ—ºè§’',
            'æ²¹éº»åœ°', 'ä½æ•¦', 'å¤§åŸ”', 'è‘µæ¶Œ', 'å¤©æ°´åœ', 'èª¿æ™¯å¶º', 'æ²™ç”°',
            'ç¶­å¤šåˆ©äºæ¸¯', 'å¤§å¶¼å±±', 'é’è¡£', 'å±¯é–€', 'å…ƒæœ—', 'ä¸Šæ°´', 'ç²‰å¶º',
            
            # äº¤é€šãƒ»ã‚¤ãƒ³ãƒ•ãƒ©
            'mtr', 'æ¸¯éµ', 'hong kong international airport', 'é¦™æ¸¯åœ‹éš›æ©Ÿå ´',
            'hong kong tramways', 'é¦™æ¸¯é›»è»Š', 'star ferry', 'å¤©æ˜Ÿå°è¼ª',
            'hong kong zhuhai macau bridge', 'æ¸¯ç æ¾³å¤§æ©‹', 'high speed rail', 'é«˜éµ',
            
            # æ”¿æ²»ãƒ»è¡Œæ”¿ï¼ˆæœ€æ–°ï¼‰
            'legco', 'legislative council', 'ç«‹æ³•æœƒ', 'hksar', 'é¦™æ¸¯ç‰¹åˆ¥è¡Œæ”¿å€',
            'john lee', 'æå®¶è¶…', 'è¡Œæ”¿é•·å®˜', 'chief executive',
            'hong kong government', 'é¦™æ¸¯æ”¿åºœ', 'hong kong police', 'é¦™æ¸¯è­¦å¯Ÿ',
            'hong kong observatory', 'é¦™æ¸¯å¤©æ–‡å°', 'hong kong monetary authority', 'é‡‘ç®¡å±€',
            
            # çµŒæ¸ˆãƒ»é‡‘è
            'hkex', 'hong kong stock exchange', 'é¦™æ¸¯äº¤æ˜“æ‰€', 'hong kong dollar', 'æ¸¯å¹£',
            'greater bay area', 'ç²µæ¸¯æ¾³å¤§ç£å€', 'hong kong finance', 'é¦™æ¸¯é‡‘è',
            
            # æ–‡åŒ–ãƒ»è¦³å…‰
            'm+ museum', 'è¥¿ä¹æ–‡åŒ–å€', 'west kowloon cultural district', 'hong kong disneyland',
            'é¦™æ¸¯è¿ªå£«å°¼', 'ocean park', 'æµ·æ´‹å…¬åœ’', 'hong kong arts festival', 'é¦™æ¸¯è—è¡“ç¯€',
            'hong kong international film festival', 'é¦™æ¸¯åœ‹éš›é›»å½±ç¯€',
            
            # æ•™è‚²ãƒ»å¤§å­¦
            'university of hong kong', 'é¦™æ¸¯å¤§å­¸', 'chinese university of hong kong', 'é¦™æ¸¯ä¸­æ–‡å¤§å­¸',
            'hong kong university of science and technology', 'é¦™æ¸¯ç§‘æŠ€å¤§å­¸',
            'city university of hong kong', 'é¦™æ¸¯åŸå¸‚å¤§å­¸',
            
            # ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹
            'scmp', 'south china morning post', 'å—è¯æ—©å ±', 'rthk', 'é¦™æ¸¯é›»å°',
            'chinadaily', 'hket', 'the standard', 'ming pao', 'æ˜å ±',
            'hong kong free press', 'hk01', 'now news', 'nowæ–°è',
            
            # ãã®ä»–é¦™æ¸¯é–¢é€£
            'hong kong dollar', 'hkd', 'hong kong identity card', 'é¦™æ¸¯èº«ä»½è­‰',
            'hong kong passport', 'é¦™æ¸¯è­·ç…§', 'hong kong dollar', 'æ¸¯å¹£',
            'hong kong housing authority', 'é¦™æ¸¯æˆ¿å±‹å§”å“¡æœƒ', 'hong kong housing society', 'é¦™æ¸¯æˆ¿å±‹å”æœƒ'
        ]
        has_hk = any(k in text for k in hk_keywords)
        
        if not has_hk:
            # é¦™æ¸¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã€å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            for keyword in non_hk_keywords:
                if keyword in title.lower():
                    return True
        
        return False
    
    def _normalize_url(self, url: str) -> str:
        """URLã‚’æ­£è¦åŒ–ï¼ˆã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦ãƒ™ãƒ¼ã‚¹URLã®ã¿æŠ½å‡ºï¼‰"""
        if not url:
            return ""
        try:
            from urllib.parse import urlparse, urlunparse
            parsed = urlparse(url)
            # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
            normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
            return normalized
        except:
            return url
    
    def _is_duplicate_content(self, title: str, existing_titles: List[str]) -> bool:
        """ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹åˆ¤å®šï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        import re
        
        def normalize_title(t):
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ã€å˜èªåˆ†å‰²ï¼‰
            t = t.lower()
            t = re.sub(r'[^\w\s]', '', t)
            return set(t.split())
        
        title_words = normalize_title(title)
        
        if not title_words:
            return False
        
        for existing in existing_titles:
            existing_words = normalize_title(existing)
            
            if not existing_words:
                continue
            
            common_words = title_words & existing_words
            shortest_len = min(len(title_words), len(existing_words))
            
            if shortest_len <= 4:
                min_common = max(2, shortest_len)
            else:
                min_common = 2
            
            if len(common_words) < min_common:
                continue
            
            all_words = title_words | existing_words
            similarity = len(common_words) / len(all_words) if all_words else 0.0
            coverage = len(common_words) / shortest_len if shortest_len else 0.0
            
            if similarity >= 0.5 and coverage >= 0.6:
                return True
        
        return False
    
    def fetch_scmp_rss(self) -> List[Dict]:
        """SCMPï¼ˆSouth China Morning Postï¼‰ã®RSSã‚’å–å¾—"""
        print("ğŸ“° SCMP RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['scmp_hongkong'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                # é¦™æ¸¯é–¢é€£ã®å³æ ¼åˆ¤å®š
                if not self._is_hk_related(title, description, url, 'SCMP'):
                    filtered_count += 1
                    continue

                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'SCMP',
                    'api_source': 'rss_scmp'
                })
                
                # å‡¦ç†æ¸ˆã¿URLã«è¿½åŠ 
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_rthk_rss(self) -> List[Dict]:
        """RTHKï¼ˆRadio Television Hong Kongï¼‰ã®RSSã‚’å–å¾—"""
        print("ğŸ“° RTHK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['rthk_news'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                if not self._is_hk_related(title, description, url, 'RTHK'):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'RTHK',
                    'api_source': 'rss_rthk'
                })
                
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_yahoo_rss(self) -> List[Dict]:
        """Yahoo News HKã®RSSã‚’å–å¾—"""
        print("ğŸ“° Yahoo News HK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['yahoo_hk'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                if not self._is_hk_related(title, description, url, 'Yahoo News HK'):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'Yahoo News HK',
                    'api_source': 'rss_yahoo_hk'
                })
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_google_news_rss(self) -> List[Dict]:
        """Google News HKã®RSSã‚’å–å¾—"""
        print("ğŸ“° Google News HK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['google_news_hk'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                if not self._is_hk_related(title, description, url, 'Google News HK'):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'Google News HK',
                    'api_source': 'rss_google_news_hk'
                })
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_chinadaily_rss(self) -> List[Dict]:
        """China Daily HKã®RSSã‚’å–å¾—"""
        print("ğŸ“° China Daily HK RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['chinadaily_hk'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                if not self._is_hk_related(title, description, url, 'China Daily HK'):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'China Daily HK',
                    'api_source': 'rss_chinadaily_hk'
                })
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_hkfp_rss(self) -> List[Dict]:
        """Hong Kong Free Pressã®RSSã‚’å–å¾—"""
        print("ğŸ“° Hong Kong Free Press RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            feed = feedparser.parse(self.rss_feeds['hkfp'])
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                if not self._is_hk_related(title, description, url, 'Hong Kong Free Press'):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': 'Hong Kong Free Press',
                    'api_source': 'rss_hkfp'
                })
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_hket_rss(self) -> List[Dict]:
        """HKETï¼ˆé¦™æ¸¯çµŒæ¸ˆæ—¥å ±ï¼‰ã®RSSã‚’å–å¾—ï¼ˆUser-Agentå¿…è¦ï¼‰"""
        print("ğŸ“° HKET é¦™æ¸¯ RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            response = requests.get(self.rss_feeds['hket_hk'], headers=self.headers, timeout=5)
            
            if response.status_code == 200:
                feed = feedparser.parse(response.content)
                news_list = []
                filtered_count = 0
                
                for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                    url = entry.get('link', '')
                    if self._has_been_processed(url):
                        filtered_count += 1
                        continue
                    if self._is_url_too_old(url):
                        filtered_count += 1
                        self._mark_url_as_processed(url)
                        continue
                    published_at = entry.get('published', entry.get('updated', ''))
                    title = entry.get('title', '')
                    description = entry.get('summary', entry.get('description', ''))
                    
                    # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if not self._is_today_news(published_at):
                        filtered_count += 1
                        continue
                    
                    # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    if self._is_forbidden_content(title, description):
                        filtered_count += 1
                        continue

                    if not self._is_hk_related(title, description, url, 'HKET'):
                        filtered_count += 1
                        continue
                    
                    news_list.append({
                        'title': title,
                        'description': description,
                        'url': url,
                        'published_at': published_at,
                        'source': 'HKET',
                        'api_source': 'rss_hket'
                    })
                    self._mark_url_as_processed(url)
                
                print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
                return news_list
            else:
                print(f"  âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return []
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_generic_rss(self, feed_key: str, source_name: str, use_headers: bool = False) -> List[Dict]:
        """æ±ç”¨RSSå–å¾—é–¢æ•°"""
        print(f"ğŸ“° {source_name} RSS ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
        try:
            if use_headers:
                response = requests.get(self.rss_feeds[feed_key], headers=self.headers, timeout=5)
                feed = feedparser.parse(response.content)
            else:
                feed = feedparser.parse(self.rss_feeds[feed_key])
            
            news_list = []
            filtered_count = 0
            
            for entry in feed.entries[:100]:  # 50 â†’ 100ã«å¢—åŠ 
                url = entry.get('link', '')
                if self._has_been_processed(url):
                    filtered_count += 1
                    continue
                if self._is_url_too_old(url):
                    filtered_count += 1
                    self._mark_url_as_processed(url)
                    continue
                published_at = entry.get('published', entry.get('updated', ''))
                title = entry.get('title', '')
                description = entry.get('summary', entry.get('description', ''))
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if not self._is_today_news(published_at):
                    filtered_count += 1
                    continue
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if self._is_forbidden_content(title, description):
                    filtered_count += 1
                    continue

                if not self._is_hk_related(title, description, url, source_name):
                    filtered_count += 1
                    continue
                
                news_list.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at,
                    'source': source_name,
                    'api_source': feed_key
                })
                self._mark_url_as_processed(url)
            
            print(f"  âœ… {len(news_list)}ä»¶å–å¾—ï¼ˆ{filtered_count}ä»¶ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ï¼‰")
            return news_list
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def fetch_all_rss(self) -> List[Dict]:
        """å…¨RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚‚å«ã‚€ï¼‰"""
        print("\nğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—é–‹å§‹ï¼ˆRSS + ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰")
        print("=" * 60)
        
        all_news = []
        existing_titles = []
        existing_urls = set(self.processed_urls.keys())  # æ­£è¦åŒ–ã•ã‚ŒãŸURLã®ã‚»ãƒƒãƒˆ
        duplicate_count = 0
        url_duplicate_count = 0
        title_duplicate_count = 0
        
        # Phase 1: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆå„ªå…ˆï¼‰
        print("\nğŸ“° Phase 1: Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°")
        print("-" * 60)
        try:
            from scrape_news_list import NewsListScraper
            scraper = NewsListScraper()
            scraped_news = scraper.fetch_all_news()
            
            # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’è¿½åŠ 
            scraped_filtered = {
                'total': len(scraped_news),
                'duplicate_url': 0,
                'duplicate_title': 0,
                'old_date': 0,
                'forbidden': 0,
                'non_hk': 0,
                'added': 0
            }
            
            for news in scraped_news:
                url = news.get('url', '')
                title = news.get('title', '')
                if not title or len(title) < 5:
                    continue
                
                normalized_url = self._normalize_url(url)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if normalized_url and normalized_url in existing_urls:
                    scraped_filtered['duplicate_url'] += 1
                    continue
                if self._is_duplicate_content(title, existing_titles):
                    scraped_filtered['duplicate_title'] += 1
                    continue
                
                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯æ—¥ä»˜ãŒä¸æ˜ãªå ´åˆãŒå¤šã„ã®ã§ç·©å’Œï¼‰
                published_at = news.get('published_at', '')
                if published_at:
                    if not self._is_today_news(published_at):
                        scraped_filtered['old_date'] += 1
                        continue
                # æ—¥ä»˜ãŒä¸æ˜ãªå ´åˆã¯ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ã†
                
                # ç¦æ­¢ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                description = news.get('description', title)
                if self._is_forbidden_content(title, description):
                    scraped_filtered['forbidden'] += 1
                    continue
                
                # é¦™æ¸¯é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯
                if not self._is_hk_related(title, description, url, news.get('source', '')):
                    scraped_filtered['non_hk'] += 1
                    continue
                
                all_news.append({
                    'title': title,
                    'description': description,
                    'url': url,
                    'published_at': published_at or datetime.now(HKT).isoformat(),
                    'source': news.get('source', 'Scraped'),
                    'api_source': 'web_scraping'
                })
                existing_urls.add(normalized_url)
                existing_titles.append(title)
                scraped_filtered['added'] += 1
            
            print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {scraped_filtered['total']}ä»¶å–å¾—")
            print(f"   - è¿½åŠ : {scraped_filtered['added']}ä»¶")
            print(f"   - é™¤å¤–: é‡è¤‡URL={scraped_filtered['duplicate_url']}, é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«={scraped_filtered['duplicate_title']}, å¤ã„æ—¥ä»˜={scraped_filtered['old_date']}, ç¦æ­¢={scraped_filtered['forbidden']}, é¦™æ¸¯ç„¡é–¢ä¿‚={scraped_filtered['non_hk']}")
        except ImportError as e:
            print(f"âš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
            print("   RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã¿ã§ç¶šè¡Œã—ã¾ã™...")
        except Exception as e:
            print(f"âš ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            print("   RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã¿ã§ç¶šè¡Œã—ã¾ã™...")
        
        # Phase 2: RSSãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆè£œå®Œï¼‰
        print("\nğŸ“¡ Phase 2: RSSãƒ•ã‚£ãƒ¼ãƒ‰")
        print("-" * 60)
        
        # å„RSSã‹ã‚‰å–å¾—ï¼ˆæ—¢å­˜ã®é–¢æ•°ï¼‰
        feeds_to_fetch = [
            (self.fetch_scmp_rss, None, None),
            (self.fetch_generic_rss, 'scmp_business', 'SCMP Business'),
            (self.fetch_generic_rss, 'scmp_lifestyle', 'SCMP Lifestyle'),
            (self.fetch_rthk_rss, None, None),
            (self.fetch_generic_rss, 'rthk_business', 'RTHK Business'),
            (self.fetch_yahoo_rss, None, None),
            # (self.fetch_google_news_rss, None, None),  # â† ä¸–ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒæ··å…¥ã™ã‚‹ãŸã‚ç„¡åŠ¹åŒ–
            (self.fetch_chinadaily_rss, None, None),
            (self.fetch_hkfp_rss, None, None),
            (self.fetch_hket_rss, None, None),
            (self.fetch_generic_rss, 'hket_finance', 'HKET Finance'),
            (self.fetch_generic_rss, 'hket_property', 'HKET Property'),
        ]
        
        for feed_info in feeds_to_fetch:
            func = feed_info[0]
            if feed_info[1] is None:
                # æ—¢å­˜ã®å°‚ç”¨é–¢æ•°
                news_items = func()
            else:
                # æ±ç”¨é–¢æ•°
                news_items = func(feed_info[1], feed_info[2], feed_info[1].startswith('hket'))
            
            for news in news_items:
                url = news.get('url', '')
                title = news.get('title', '')
                
                # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œã®URLã§æ¯”è¼ƒï¼‰
                normalized_url = self._normalize_url(url)
                if normalized_url and normalized_url in existing_urls:
                    url_duplicate_count += 1
                    duplicate_count += 1
                    continue
                
                # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯
                if self._is_duplicate_content(title, existing_titles):
                    title_duplicate_count += 1
                    duplicate_count += 1
                    continue
                
                # é‡è¤‡ãªã—ã®å ´åˆã€ãƒªã‚¹ãƒˆã«è¿½åŠ 
                all_news.append(news)
                existing_titles.append(title)
                if normalized_url:
                    existing_urls.add(normalized_url)
            
            time.sleep(0.5)  # 1ç§’ â†’ 0.5ç§’ã«çŸ­ç¸®
        
        # å‡¦ç†æ¸ˆã¿URLã‚’ä¿å­˜
        self._save_processed_urls()
        
        print("=" * 60)
        print(f"âœ… åˆè¨ˆ {len(all_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        print(f"ğŸ”„ é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶ï¼ˆURLé‡è¤‡: {url_duplicate_count}ä»¶ã€ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼: {title_duplicate_count}ä»¶ï¼‰")
        print(f"ğŸ“ å‡¦ç†æ¸ˆã¿URLç·æ•°: {len(self.processed_urls)}ä»¶\n")
        
        return all_news
    
    def fetch_weather_info(self) -> Dict:
        """é¦™æ¸¯å¤©æ–‡å°ã®å¤©æ°—æƒ…å ±ã‚’å–å¾—"""
        print("\nğŸŒ¤ï¸  å¤©æ°—æƒ…å ±å–å¾—é–‹å§‹")
        print("=" * 60)
        
        weather_data = {}
        
        for key, url in self.weather_feeds.items():
            try:
                feed = feedparser.parse(url)
                if len(feed.entries) > 0:
                    entry = feed.entries[0]
                    weather_data[key] = {
                        'title': entry.get('title', ''),
                        'description': entry.get('description', entry.get('summary', '')),
                        'published_at': entry.get('published', entry.get('updated', '')),
                    }
                    print(f"  âœ… {key} å–å¾—å®Œäº†")
            except Exception as e:
                print(f"  âŒ {key} ã‚¨ãƒ©ãƒ¼: {e}")
        
        print("=" * 60)
        print(f"âœ… å¤©æ°—æƒ…å ± {len(weather_data)}ä»¶å–å¾—\n")
        
        return weather_data

if __name__ == "__main__":
    import json
    from scrape_article import ArticleScraper
    
    rss_api = RSSNewsAPI()
    news = rss_api.fetch_all_rss()
    weather = rss_api.fetch_weather_info()
    
    if news:
        # å…¨æ–‡å–å¾—å‡¦ç†ã‚’è¿½åŠ 
        print("\nğŸ“° è¨˜äº‹å…¨æ–‡ã‚’å–å¾—ä¸­...")
        print("=" * 60)
        
        scraper = ArticleScraper()
        enriched_news = []
        
        for i, item in enumerate(news, 1):
            print(f"\n[{i}/{len(news)}] {item['title'][:60]}...")
            
            # URLã‹ã‚‰å…¨æ–‡ã‚’å–å¾—
            full_content = scraper.scrape_article(item['url'])
            
            if full_content:
                item['full_content'] = full_content
                print(f"    âœ… {len(full_content)}æ–‡å­—å–å¾—")
            else:
                # å–å¾—å¤±æ•—æ™‚ã¯descriptionã‚’ä½¿ç”¨
                item['full_content'] = item.get('description', '')
                print(f"    âš ï¸  å…¨æ–‡å–å¾—å¤±æ•—ã€descriptionã‚’ä½¿ç”¨")
            
            enriched_news.append(item)
        
        print("\n" + "=" * 60)
        print(f"âœ… å…¨æ–‡å–å¾—å®Œäº†: {len(enriched_news)}ä»¶\n")
        
        timestamp = datetime.now(HKT).strftime('%Y-%m-%d_%H-%M-%S')
        output_path = f"daily-articles/rss_news_{timestamp}.json"
        
        data = {
            'fetch_time': datetime.now(HKT).isoformat(),
            'total_count': len(enriched_news),
            'news': enriched_news,
            'weather': weather
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_path}")
        
        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
        print("\nğŸ“‹ å–å¾—ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
        for i, item in enumerate(enriched_news[:5], 1):
            print(f"\n{i}. {item['title']}")
            print(f"   ã‚½ãƒ¼ã‚¹: {item['source']} ({item['api_source']})")
            print(f"   å…¨æ–‡: {len(item.get('full_content', ''))}æ–‡å­—")
        
        if weather:
            print("\nğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚å–å¾—ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
