#!/usr/bin/env python3
"""
é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆåºƒæ±èªå­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»˜ãï¼‰

ã€çµ¶å¯¾å¤‰æ›´ç¦æ­¢è­¦å‘Šã€‘å¤©æ°—æƒ…å ±ç¿»è¨³å‡¦ç†ã«ã¤ã„ã¦
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸  âš ï¸  âš ï¸  ç·Šæ€¥è­¦å‘Šï¼šã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„ã§ãã ã•ã„ï¼âš ï¸  âš ï¸  âš ï¸

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€å¤©æ°—æƒ…å ±ã®åºƒæ±èª/ä¸­æ–‡ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹é‡è¦ãªå‡¦ç†ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã®ã‚³ãƒ¼ãƒ‰ã¯GitHub Actionsã§å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚å¤‰æ›´ã™ã‚‹ã¨å¿…ãšå•é¡ŒãŒå†ç™ºã—ã¾ã™ã€‚

ã€çµ¶å¯¾å¤‰æ›´ç¦æ­¢ã‚»ã‚¯ã‚·ãƒ§ãƒ³ - å¤‰æ›´ã™ã‚‹å ´åˆã¯å¿…ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèªã€‘
- format_weather_info() ãƒ¡ã‚½ãƒƒãƒ‰
- _llm_translate_text() ãƒ¡ã‚½ãƒƒãƒ‰  
- _has_chinese_chars() ãƒ¡ã‚½ãƒƒãƒ‰
- _is_already_japanese() ãƒ¡ã‚½ãƒƒãƒ‰

ã€éå»ã®å•é¡Œå±¥æ­´ - ä½•åº¦ã‚‚åŒã˜å•é¡ŒãŒå†ç™ºã€‘
- ã‚³ãƒ¼ãƒ‰ã‚’ã€Œå¾®èª¿æ•´ã€â†’ æ¯å›åºƒæ±èªæ··åœ¨å•é¡ŒãŒå†ç™º
- ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ  â†’ å‹•ä½œãŒå¤‰ã‚ã‚‹
- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†è¿½åŠ  â†’ è¤‡é›‘ã«ãªã‚Šå•é¡Œç™ºç”Ÿ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¤‰æ›´ â†’ åºƒæ±èªãŒè¡¨ç¤ºã•ã‚Œã‚‹

ã€å¤‰æ›´ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¸ã€‘
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹å ´åˆï¼š
1. å¤‰æ›´ã‚’å³åº§ã«ä¸­æ­¢ã—ã¦ãã ã•ã„
2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ç¢ºèªã‚’å–ã£ã¦ãã ã•ã„
3. ã€Œå‹•ä½œç¢ºèªæ¸ˆã¿ã®ã‚³ãƒ¼ãƒ‰ã€ã‚’å¤‰æ›´ã™ã‚‹ç†ç”±ã‚’æ˜ç¢ºã«èª¬æ˜ã—ã¦ãã ã•ã„
4. å¤‰æ›´ãŒå¿…è¦ãªå ´åˆã§ã‚‚ã€æœ€å°é™ã®å¤‰æ›´ã«ã¨ã©ã‚ã¦ãã ã•ã„

ã€å¿…é ˆæ©Ÿèƒ½ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰ã€‘
1. ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ï¼ˆæœ€å¤§3å›ï¼‰ - å‰Šé™¤ã™ã‚‹ã¨ä¸€æ™‚çš„ãªAPIã‚¨ãƒ©ãƒ¼ã§åºƒæ±èªãŒè¡¨ç¤ºã•ã‚Œã‚‹
2. ç¿»è¨³çµæœã®æ¤œè¨¼å‡¦ç†ï¼ˆ_has_chinese_charsï¼‰ - å‰Šé™¤ã™ã‚‹ã¨ä¸å®Œå…¨ãªç¿»è¨³ãŒé€šã‚‹
3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆåŸæ–‡ã‚’è¿”ã•ãªã„å‡¦ç†ï¼‰ - å‰Šé™¤ã™ã‚‹ã¨åºƒæ±èªãŒè¨˜äº‹ã«è¡¨ç¤ºã•ã‚Œã‚‹

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import json
import requests
import re
import time
from datetime import datetime, timedelta, timezone
from typing import List, Dict

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

class GrokArticleGenerator:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # APIé¸æŠï¼ˆGemini â†’ Claude â†’ Grok ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'gemini_api' in self.config and self.config['gemini_api'].get('api_key'):
            self.api_key = self.config['gemini_api']['api_key']
            self.api_url = self.config['gemini_api']['api_url']
            self.use_gemini = True
        elif 'claude_api' in self.config and self.config['claude_api'].get('api_key'):
            self.api_key = self.config['claude_api']['api_key']
            self.api_url = self.config['claude_api']['api_url']
            self.use_gemini = False
        else:
            # Grok APIã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨
            self.api_key = self.config['grok_api']['api_key']
            self.api_url = self.config['grok_api']['api_url']
            self.use_gemini = None
        
    def generate_article(self, news_data: List[Dict]) -> Dict:
        """Gemini/Claude/Grok APIã§æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ"""
        if self.use_gemini is True:
            api_name = "Google Gemini"
        elif self.use_gemini is False:
            api_name = "Claude API"
        else:
            api_name = "Grok API"
        print(f"\nğŸ¤– {api_name}ã§è¨˜äº‹ç”Ÿæˆä¸­...")
        print("=" * 60)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        news_text = self._format_news_for_prompt(news_data)
        
        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆHKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼‰
        today_date = datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')
        today_date_iso = datetime.now(HKT).strftime('%Y-%m-%d')
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯é¦™æ¸¯ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ç¿»è¨³ãƒ«ãƒ¼ãƒ«ï¼š
- ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³
- é¦™æ¸¯ã®åœ°åã€äººåã€çµ„ç¹”åã¯é©åˆ‡ã«ç¿»è¨³
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹
- èª­ã¿ã‚„ã™ã„è¨˜äº‹å½¢å¼ã§æ§‹æˆ

è¨˜äº‹æ§‹æˆï¼š
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’### è¦‹å‡ºã—ã§åŒºåˆ‡ã‚‹
- å†…å®¹ã‚’è©³ã—ãç¿»è¨³
- å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã‚’é©åˆ‡ã«é…ç½®
- åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–

å¼•ç”¨æƒ…å ±ã®å½¢å¼ï¼ˆé‡è¦ï¼‰ï¼š
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ€å¾Œã«å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼š

**å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹åï¼ˆä¾‹ï¼šSCMPã€RTHKã€é¦™æ¸¯01ç­‰ï¼‰
**ãƒªãƒ³ã‚¯**: å®Œå…¨ãªURL

ä¾‹ï¼š
**å¼•ç”¨å…ƒ**: SCMP
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/news/hong-kong/law-and-crime/article/3330816/hong-kongs-scameter-app-gets-upgrade-ai-tools-tackle-social-media-scams

é‡è¦ï¼šJSONå½¢å¼ã§ã¯ãªãã€Markdownå½¢å¼ã§è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã¯å¿…ãšåˆ¥ã€…ã®è¡Œã«è¨˜è¼‰ã—ã€**ã§å›²ã‚“ã§ãã ã•ã„ã€‚"""

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        user_prompt = f"""ä»¥ä¸‹ã®é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€è¨˜äº‹ã¨ã—ã¦æ§‹æˆã—ã¦ãã ã•ã„ï¼š

ã€é‡è¦ã€‘ä»Šæ—¥ã®æ—¥ä»˜ã¯ {today_date}ï¼ˆ{today_date_iso}ï¼‰ã§ã™ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã«ã¯å¿…ãšã€Œæ¯æ—¥AIãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({today_date})ã€ã¨ã„ã†å½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

{news_text}

è¨˜äº‹ã®è¦ä»¶ï¼š
1. ã‚¿ã‚¤ãƒˆãƒ«ã¯å¿…ãšã€Œ# æ¯æ—¥AIãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({today_date})ã€ã¨ã„ã†å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„
2. å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’### è¦‹å‡ºã—ã§åŒºåˆ‡ã‚‹
3. å†…å®¹ã‚’è©³ã—ãç¿»è¨³
4. å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã‚’é©åˆ‡ã«é…ç½®
5. åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–
6. Markdownå½¢å¼ã§å‡ºåŠ›

å¼•ç”¨æƒ…å ±ã®å½¢å¼ï¼ˆé‡è¦ï¼‰ï¼š
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ€å¾Œã«å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼š

**å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹åï¼ˆä¾‹ï¼šSCMPã€RTHKã€é¦™æ¸¯01ç­‰ï¼‰
**ãƒªãƒ³ã‚¯**: å®Œå…¨ãªURL

ä¾‹ï¼š
**å¼•ç”¨å…ƒ**: SCMP
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/news/hong-kong/law-and-crime/article/3330816/hong-kongs-scameter-app-gets-upgrade-ai-tools-tackle-social-media-scams

é‡è¦ï¼šå¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã¯å¿…ãšåˆ¥ã€…ã®è¡Œã«è¨˜è¼‰ã—ã€**ã§å›²ã‚“ã§ãã ã•ã„ã€‚

è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š"""

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆGemini/Claude/Grokå¯¾å¿œï¼‰
        if self.use_gemini is True:
            # Gemini API
            headers = {
                "Content-Type": "application/json"
            }
            # APIã‚­ãƒ¼ã‚’URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
            api_url_with_key = f"{self.api_url}?key={self.api_key}"
            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"{system_prompt}\n\n{user_prompt}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 50000
                }
            }
        else:
            # Claude/Grok API
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            if self.use_gemini is False:  # Claude API
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "messages": [
                        {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50000
                }
            else:  # Grok API
                payload = {
                    "model": "grok-beta",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50000
                }
        
        if self.use_gemini is True:
            print("ğŸ“¤ Google Geminiã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        elif self.use_gemini is False:
            print("ğŸ“¤ Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        else:
            print("ğŸ“¤ Grok APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        
        try:
            # Gemini APIã®å ´åˆã¯URLã«APIã‚­ãƒ¼ã‚’è¿½åŠ 
            url = api_url_with_key if self.use_gemini is True else self.api_url
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if self.use_gemini is True:
                    # Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    content = result['candidates'][0]['content']['parts'][0]['text']
                else:
                    # Claude/Grok APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    if self.use_gemini is False:  # Claude API
                        content = result['content'][0]['text']
                    else:  # Grok API
                        content = result['choices'][0]['message']['content']
                
                print("âœ… è¨˜äº‹ç”Ÿæˆå®Œäº†")
                
                # è¨˜äº‹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–
                return self._parse_article_content(content)
                
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                print(f"   è©³ç´°: {response.text}")
                
                # Gemini APIãŒåœ°åŸŸåˆ¶é™ã®å ´åˆã¯Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if (response.status_code == 403 or response.status_code == 400) and self.use_gemini is True:
                    print("ğŸ”„ Gemini APIåœ°åŸŸåˆ¶é™ã®ãŸã‚Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_grok(news_data)
                
                # Grok APIãŒã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®å ´åˆã¯Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if response.status_code == 429 and self.use_gemini is None:
                    print("ğŸ”„ Grok APIã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®ãŸã‚Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_claude(news_data)
                
                return None
                
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")
            return None
    
    def _fallback_to_grok(self, news_data: List[Dict]) -> Dict:
        """Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Grok APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Grok APIã®è¨­å®š
        self.api_key = self.config['grok_api']['api_key']
        self.api_url = self.config['grok_api']['api_url']
        self.use_gemini = None
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _fallback_to_claude(self, news_data: List[Dict]) -> Dict:
        """Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Claude APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Claude APIã®è¨­å®š
        self.api_key = self.config['claude_api']['api_key']
        self.api_url = self.config['claude_api']['api_url']
        self.use_gemini = False
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _parse_article_content(self, content: str) -> Dict:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
        lines = content.split('\n')
        title = lines[0].replace('#', '').strip() if lines else "é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        
        # æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’é™¤ãï¼‰
        if lines and lines[0].startswith('#'):
            body = '\n'.join(lines[1:])
        else:
            body = content
        
        return {
            "title": title,
            "lead": "",
            "body": body,
            "tags": "é¦™æ¸¯,ãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
        }
    
    def _format_news_for_prompt(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢"""
        formatted = []
        for i, news in enumerate(news_data, 1):
            title = news.get('title', '')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', '')
            published = news.get('published', '')
            
            formatted.append(f"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹ {i}:
ã‚¿ã‚¤ãƒˆãƒ«: {title}
å†…å®¹: {description}
URL: {url}
ã‚½ãƒ¼ã‚¹: {source}
å…¬é–‹æ—¥æ™‚: {published}
""")
        
        return '\n'.join(formatted)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ã€é‡è¦ãƒ»å¤‰æ›´ç¦æ­¢ã€‘å¤©æ°—æƒ…å ±ç¿»è¨³å‡¦ç†
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 
    # ã€è­¦å‘Šã€‘ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å¤‰æ›´ãƒ»å‰Šé™¤ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ãªã„ã§ãã ã•ã„ï¼
    # 
    # é‡è¦ç†ç”±ï¼š
    # 1. åºƒæ±èª/ä¸­æ–‡ã®å¤©æ°—æƒ…å ±ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹å¿…é ˆå‡¦ç†ã§ã™
    # 2. éå»ã«ä½•åº¦ã‚‚å•é¡ŒãŒå†ç™ºã—ã¦ãŠã‚Šã€ã“ã®å‡¦ç†ã‚’å¤‰æ›´ã™ã‚‹ã¨å¿…ãšåºƒæ±èªãŒæ··åœ¨ã—ã¾ã™
    # 3. ç¿»è¨³å¤±æ•—æ™‚ã«åŸæ–‡ã‚’è¿”ã™ã¨ã€è¨˜äº‹ã«åºƒæ±èªãŒãã®ã¾ã¾è¡¨ç¤ºã•ã‚Œã‚‹è‡´å‘½çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã™
    # 
    # å¤‰æ›´å±¥æ­´ã«ã‚ˆã‚‹å•é¡Œã®å†ç™ºï¼š
    # - ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’ã€Œå¾®èª¿æ•´ã€ã™ã‚‹ã¨ã€æ¯å›åºƒæ±èªå•é¡ŒãŒå†ç™ºã—ã¦ã„ã¾ã™
    # - ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã€æ¤œè¨¼å‡¦ç†ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¯å¿…é ˆã§ã™
    # - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§åŸæ–‡ã‚’è¿”ã™å‡¦ç†ã¯çµ¶å¯¾ã«è¿½åŠ ã—ãªã„ã§ãã ã•ã„
    # 
    # å¤‰æ›´ãŒå¿…è¦ãªå ´åˆï¼š
    # 1. å¿…ãšã“ã®ã‚³ãƒ¡ãƒ³ãƒˆå…¨ä½“ã‚’èª­ã‚“ã§ã‹ã‚‰å¤‰æ›´ã—ã¦ãã ã•ã„
    # 2. å¤‰æ›´å‰å¾Œã§ç¿»è¨³å‡¦ç†ãŒç¢ºå®Ÿã«å®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„
    # 3. åºƒæ±èª/ä¸­æ–‡ã®æ¤œè¨¼å‡¦ç†ã‚’å‰Šé™¤ã—ãªã„ã§ãã ã•ã„
    # 4. ã‚¨ãƒ©ãƒ¼æ™‚ã«åŸæ–‡ã‚’è¿”ã™å‡¦ç†ã‚’è¿½åŠ ã—ãªã„ã§ãã ã•ã„
    # 
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def format_weather_info(self, weather_data: Dict) -> str:
        """å¤©æ°—æƒ…å ±ã‚’Markdownå½¢å¼ã«æ•´å½¢ï¼ˆå¤‰æ›´ç¦æ­¢ï¼šåºƒæ±èªç¿»è¨³ã®å¿…é ˆå‡¦ç†ï¼‰"""
        if not weather_data:
            return ""
        
        import re
        
        def clean_weather_text(text: str) -> str:
            """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            if not text:
                return ""
            # HTMLã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            text = re.sub(r'<br\s*/?>', '\n', text)
            # ä»–ã®HTMLã‚¿ã‚°ã‚’é™¤å»
            text = re.sub(r'<[^>]+>', '', text)
            # å„è¡Œã”ã¨ã«å‡¦ç†
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                # è¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                line = re.sub(r'\s+', ' ', line).strip()
                if line:
                    cleaned_lines.append(line)
            return '\n'.join(cleaned_lines)
        
        # åœ°åŸŸå¤©æ°—äºˆå ±ã®ã¿å‡¦ç†
        if 'weather_forecast' not in weather_data:
            return ""
        
        forecast = weather_data['weather_forecast']
        title = forecast.get('title', 'N/A')
        desc = clean_weather_text(forecast.get('description', ''))
        
        # å¤©æ°—æƒ…å ±ã¯LLMã§ä¸€æ‹¬æ—¥æœ¬èªç¿»è¨³ï¼ˆè¾æ›¸ç½®æ›ã¯ä½¿ã‚ãªã„ï¼‰
        translated_title = self._llm_translate_text(title)
        translated_desc = self._llm_translate_text(desc)
        
        # ã€çµ¶å¯¾å¿…é ˆã€‘ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºã¯è¨˜äº‹ã«å‡ºã•ãªã„
        if '[ç¿»è¨³ã‚¨ãƒ©ãƒ¼' in translated_title or '[ç¿»è¨³ã‚¨ãƒ©ãƒ¼' in translated_desc:
            return ""

        # Descriptionã¯å³æ ¼ã«æ—¥æœ¬èªåˆ¤å®šï¼ˆå¤±æ•—ãªã‚‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨éè¡¨ç¤ºï¼‰
        if not self._is_japanese(translated_desc):
            return ""

        # Titleã¯çŸ­æ–‡ãƒ»æ¼¢å­—æ¯”ç‡ãŒé«˜ãã¦ã‚‚è¨±å®¹ã€‚å¼±ã„å ´åˆã¯å®‰å…¨ãªæ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ã«å·®ã—æ›¿ãˆ
        if not self._is_japanese_title(translated_title):
            translated_title = "é¦™æ¸¯å¤©æ–‡å°ãŒç™ºè¡¨ã—ãŸå¤©æ°—äºˆå ±"
        
        # ç¿»è¨³æˆåŠŸæ™‚ã®ã¿å¤©æ°—äºˆå ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿”ã™ï¼ˆãƒŸãƒƒã‚·ãƒ§ãƒ³ï¼šä¸­å›½èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã•ã‚ŒãŸè¨˜äº‹ã‚’æŠ•ç¨¿ï¼‰
        weather_section = f"## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—\n\n### å¤©æ°—äºˆå ±\n{translated_title}\n{translated_desc}\n\n**å¼•ç”¨å…ƒ**: é¦™æ¸¯å¤©æ–‡å°"
        return weather_section
    
    def _translate_weather_text(self, text: str) -> str:
        """ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼ˆæœªä½¿ç”¨ï¼‰ã€‚LLMãƒ™ãƒ¼ã‚¹ç¿»è¨³ã«åˆ‡æ›¿æ¸ˆã¿ã€‚"""
        return self._llm_translate_text(text)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ã€é‡è¦ãƒ»å¤‰æ›´ç¦æ­¢ã€‘åºƒæ±èª/ä¸­æ–‡ç¿»è¨³å‡¦ç†ã®ã‚³ã‚¢é–¢æ•°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 
    # ã€è­¦å‘Šã€‘ã“ã®é–¢æ•°ã¯çµ¶å¯¾ã«å¤‰æ›´ãƒ»ç°¡ç•¥åŒ–ã—ãªã„ã§ãã ã•ã„ï¼
    # 
    # é‡è¦ãƒã‚¤ãƒ³ãƒˆï¼š
    # 1. ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ï¼ˆæœ€å¤§3å›ï¼‰ã¯å¿…é ˆã§ã™
    # 2. ç¿»è¨³çµæœã®æ¤œè¨¼ï¼ˆ_has_chinese_charsï¼‰ã¯å¿…é ˆã§ã™
    # 3. ã‚¨ãƒ©ãƒ¼æ™‚ã«åŸæ–‡ã‚’è¿”ã™å‡¦ç†ã¯çµ¶å¯¾ã«è¿½åŠ ã—ãªã„ã§ãã ã•ã„
    # 4. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã“ã¨ã§ã€åºƒæ±èªæ··åœ¨ã‚’é˜²ãã¾ã™
    # 
    # éå»ã®å•é¡Œï¼š
    # - ãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã‚’å‰Šé™¤ â†’ ç¿»è¨³å¤±æ•—ã§åºƒæ±èªãŒæ®‹ã‚‹
    # - æ¤œè¨¼å‡¦ç†ã‚’å‰Šé™¤ â†’ ä¸å®Œå…¨ãªç¿»è¨³ãŒé€šã‚‹
    # - ã‚¨ãƒ©ãƒ¼æ™‚ã«åŸæ–‡ã‚’è¿”ã™ â†’ å¿…ãšåºƒæ±èªãŒè¨˜äº‹ã«è¡¨ç¤ºã•ã‚Œã‚‹
    # 
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def _llm_translate_text(self, text: str) -> str:
        """LLMã§åºƒæ±èª/ä¸­æ–‡ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ä¸€ç™ºç¿»è¨³ï¼ˆæ—¥æœ¬èªä»¥å¤–æ··åœ¨ç¦æ­¢ï¼‰"""
        if not text:
            return ""
        
        prompt = (
            "ä»¥ä¸‹ã®åºƒæ±èª/ä¸­æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"\
            "è¨˜å·ã‚„æ•°å€¤ã¯ä¿æŒã—ã€æ—¥æœ¬èªä»¥å¤–ï¼ˆä¸­æ–‡ã®èªå½™ãƒ»å¥èª­ç‚¹ãƒ»è‹±èªï¼‰ãŒæ®‹ã‚‰ãªã„ã‚ˆã†ã«ã€‚\n\n" + text
        )

        # ã€çµ¶å¯¾å¿…é ˆã€‘ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ï¼šGemini â†’ Claude â†’ Grok ã®é †ã§è©¦è¡Œ
        # ãƒŸãƒƒã‚·ãƒ§ãƒ³ï¼šç¿»è¨³ã‚’100%æˆåŠŸã•ã›ã‚‹ï¼ˆã„ãšã‚Œã‹ã®APIã§å¿…ãšæˆåŠŸã•ã›ã‚‹ï¼‰
        # APIã‚­ãƒ¼ãŒã‚ã‚‹APIã®ã¿ã‚’å„ªå…ˆé †ä½é †ã«è¿½åŠ 
        apis_to_try = []
        
        # å„ªå…ˆé †ä½1: Gemini APIï¼ˆAPIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if 'gemini_api' in self.config and self.config['gemini_api'].get('api_key') and self.config['gemini_api']['api_key'].strip():
            apis_to_try.append(('gemini', self.config['gemini_api']['api_key'], 
                               self.config['gemini_api']['api_url'], True))
        
        # å„ªå…ˆé †ä½2: Claude APIï¼ˆAPIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if 'claude_api' in self.config and self.config['claude_api'].get('api_key') and self.config['claude_api']['api_key'].strip():
            apis_to_try.append(('claude', self.config['claude_api']['api_key'], 
                               self.config['claude_api']['api_url'], False))
        
        # å„ªå…ˆé †ä½3: Grok APIï¼ˆAPIã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        if 'grok_api' in self.config and self.config['grok_api'].get('api_key') and self.config['grok_api']['api_key'].strip():
            apis_to_try.append(('grok', self.config['grok_api']['api_key'], 
                               self.config['grok_api']['api_url'], None))
        
        # è©¦è¡Œã™ã‚‹APIãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if not apis_to_try:
            print(f"âŒ æœ‰åŠ¹ãªAPIã‚­ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç¿»è¨³ã§ãã¾ã›ã‚“ã€‚")
            return "[ç¿»è¨³ã‚¨ãƒ©ãƒ¼: å¤©æ°—æƒ…å ±ã®ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ]"
        
        # å„APIã§é †ç•ªã«è©¦è¡Œ
        for api_name, api_key, api_url, use_gemini_flag in apis_to_try:
            if not api_key:
                continue
                
            try:
                if use_gemini_flag is True:
                    headers = {"Content-Type": "application/json"}
                    api_url_with_key = f"{api_url}?key={api_key}"
                    payload = {
                        "contents": [{"parts": [{"text": prompt}]}],
                        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 2048},
                    }
                    resp = requests.post(api_url_with_key, headers=headers, json=payload, timeout=60)
                    if resp.status_code == 200:
                        txt = resp.json()['candidates'][0]['content']['parts'][0]['text']
                        translated = txt.strip()
                        if self._is_japanese(translated):
                            print(f"âœ… å¤©æ°—ç¿»è¨³æˆåŠŸ ({api_name})")
                            return translated
                        else:
                            print(f"âš ï¸  {api_name}ç¿»è¨³çµæœãŒæ—¥æœ¬èªã¨ã—ã¦ä¸ååˆ†ã€‚æ¬¡ã®APIã‚’è©¦è¡Œ...")
                            continue
                    else:
                        print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼ ({api_name}): HTTP {resp.status_code}")
                        continue
                else:
                    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
                    if use_gemini_flag is False:
                        payload = {"model": "claude-3-5-sonnet-20241022", "messages": [{"role": "user", "content": prompt}], "temperature": 0.1, "max_tokens": 2048}
                    else:
                        payload = {"model": "grok-beta", "messages": [{"role": "system", "content": "Translate to natural Japanese only."}, {"role": "user", "content": prompt}], "temperature": 0.1, "max_tokens": 2048}
                    resp = requests.post(api_url, headers=headers, json=payload, timeout=60)
                    if resp.status_code == 200:
                        if use_gemini_flag is False:
                            txt = resp.json()['content'][0]['text']
                        else:
                            txt = resp.json()['choices'][0]['message']['content']
                        translated = txt.strip()
                        if self._is_japanese(translated):
                            print(f"âœ… å¤©æ°—ç¿»è¨³æˆåŠŸ ({api_name})")
                            return translated
                        else:
                            print(f"âš ï¸  {api_name}ç¿»è¨³çµæœãŒæ—¥æœ¬èªã¨ã—ã¦ä¸ååˆ†ã€‚æ¬¡ã®APIã‚’è©¦è¡Œ...")
                            continue
                    else:
                        print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼ ({api_name}): HTTP {resp.status_code}")
                        continue
            except Exception as e:
                print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼ ({api_name}): {e}")
                continue
        
        # ã™ã¹ã¦ã®APIã§å¤±æ•—ã—ãŸå ´åˆ
        print(f"âŒ ã™ã¹ã¦ã®APIã§å¤©æ°—ç¿»è¨³ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚åŸæ–‡ã‚’è¿”å´ã—ã¾ã›ã‚“ã€‚")
        return "[ç¿»è¨³ã‚¨ãƒ©ãƒ¼: å¤©æ°—æƒ…å ±ã®ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ]"
    
    # ã€é‡è¦ãƒ»å¤‰æ›´ç¦æ­¢ã€‘åºƒæ±èª/ä¸­æ–‡æ¤œè¨¼é–¢æ•°
    # ã“ã‚Œã‚‰ã®é–¢æ•°ã‚’å‰Šé™¤ãƒ»ç„¡åŠ¹åŒ–ã™ã‚‹ã¨ã€ç¿»è¨³å¤±æ•—ã‚’æ¤œå‡ºã§ããšåºƒæ±èªãŒæ®‹ã‚Šã¾ã™
    def _has_chinese_chars(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆã«åºƒæ±èª/ä¸­æ–‡æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰"""
        import re
        # ç¹ä½“å­—ãƒ»ç°¡ä½“å­—ã®ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆUnicodeç¯„å›²: \u4e00-\u9fffï¼‰
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        return bool(chinese_pattern.search(text))
    
    def _is_japanese(self, text: str) -> bool:
        """ç¿»è¨³çµæœãŒæ—¥æœ¬èªã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒ11æ–‡å­—ä»¥ä¸Šå«ã¾ã‚Œã¦ã„ã‚‹ã‹ï¼‰ï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰"""
        import re
        # ã²ã‚‰ãŒãªï¼ˆ\u3040-\u309Fï¼‰ã¾ãŸã¯ã‚«ã‚¿ã‚«ãƒŠï¼ˆ\u30A0-\u30FFï¼‰ã®æ–‡å­—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        hiragana_katakana_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF]')
        matches = hiragana_katakana_pattern.findall(text)
        count = len(matches)
        # 11æ–‡å­—ä»¥ä¸Šã®å ´åˆã®ã¿æ—¥æœ¬èªã¨åˆ¤å®š
        return count >= 11
    
    def _is_japanese_title(self, text: str) -> bool:
        """ã‚¿ã‚¤ãƒˆãƒ«ç”¨ã®ç·©å’Œåˆ¤å®šï¼šã²ã‚‰ãŒãª/ã‚«ã‚¿ã‚«ãƒŠ1æ–‡å­—ä»¥ä¸Šã€ã¾ãŸã¯æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€"""
        if not text:
            return False
        import re
        kana_count = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF]', text))
        if kana_count >= 1:
            return True
        keywords = ['å¤©æ°—', 'å¤©æ°—äºˆå ±', 'æ°—è±¡', 'é¦™æ¸¯å¤©æ–‡å°', 'äºˆå ±', 'å¤©å€™']
        return any(k in text for k in keywords)
    def _is_already_japanese(self, text: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆãŒæ—¢ã«æ—¥æœ¬èªã®ã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåºƒæ±èª/ä¸­æ–‡ãŒå«ã¾ã‚Œã¦ã„ãªã„ï¼‰ï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰"""
        return not self._has_chinese_chars(text)
    
    def _generate_cantonese_section(self) -> str:
        """åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’ç”Ÿæˆï¼ˆå›ºå®šå†…å®¹ãƒ»å¤‰æ›´ç¦æ­¢ï¼‰"""
        # ã“ã®å®šå‹æ–‡ã¯è¨˜äº‹ã®æœ€å¾Œã«å¿…ãšè¿½åŠ ã•ã‚Œã‚‹å›ºå®šå†…å®¹ã§ã™
        # å†…å®¹ã‚’å¤‰æ›´ã—ãªã„ã§ãã ã•ã„
        return """## åºƒæ±èªå­¦ç¿’è€…å‘ã‘æƒ…å ±

åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã«LINEãŒè‰¯ã„ã€ä¾¿åˆ©ã¨ã„ã†æ–¹ã‚‚ã„ã‚‹ã§ã—ã‚‡ã†ã‹ã‚‰ã€ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚‚ã‚ã‚Šã¾ã™ã®ã§ã“ã¡ã‚‰ã”ç™»éŒ²ã—ã¦ã‹ã‚‰ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ã“ã¡ã‚‰ã‚‚LEDã®ãƒãƒ£ãƒƒãƒˆbotå½¢å¼ã§ç§˜æ›¸ã®ãƒªãƒ¼ã•ã‚“ãŒåºƒæ±èªã«ã¤ã„ã¦ãªã‚“ã§ã‚‚å›ç­”ã—ã¦ãã‚Œã¾ã™ã®ã§ãœã²ä½¿ã£ã¦ã¿ã¦ãã ã•ã„

(ä»Šç¾åœ¨400åä»¥ä¸Šã®æ–¹ã«ç™»éŒ²ã—ã¦ã„ãŸã ã„ã¦ãŠã‚Šã¾ã™ï¼‰

[![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](shared/line-img1.jpg)](https://line.me/R/ti/p/@298mwivr)

[![LINEã§ãŠå•åˆã›](shared/line-qr.png)](https://line.me/R/ti/p/@298mwivr)

## åºƒæ±èª| åºƒæ±èªè¶…åŸºç¤ã€€è¶…ç°¡å˜ï¼åˆã‚ã¦ã®åºƒæ±èªã€Œ9å£°6èª¿ã€

@https://youtu.be/RAWZAJUrvOU?si=WafOkQixyLiwMhUW"""
    
    def remove_advertisement_content(self, body: str) -> str:
        """è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šãƒ»å®£ä¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»"""
        import re
        
        # åºƒå‘Šãƒ»å®£ä¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        ad_patterns = [
            r'æœ€æ–°ã®å‹•ç”»ç´¹ä»‹ï¼š.*?ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'TOPick.*?ãƒãƒ£ãƒ³ãƒãƒ«.*?ãƒ•ã‚©ãƒ­ãƒ¼.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ç„¡æ–™ã®.*?ä¼šå“¡.*?ä»Šã™ã.*?ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
            r'ä¼šå“¡æ–°è¦å‹Ÿé›†.*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š',
            r'https://whatsapp\.com/channel/.*?',
            r'https://onelink\.to/.*?',
            r'https://event\.hket\.com/.*?',
            r'ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'ç”³ã—è¾¼ã¿å—ä»˜ä¸­',
            r'ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š.*?',
            r'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š.*?',
            r'ğŸ””.*?ãƒ•ã‚©ãƒ­ãƒ¼',
            r'ç„¡æ–™.*?ä¼šå“¡.*?å‚åŠ ã—ã¾ã—ã‚‡ã†',
            r'æ–°è¦ä¼šå“¡ç™»éŒ².*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ',
            # åºƒå‘Šè¨˜äº‹ã®é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'ã“ã®è¨˜äº‹ã¯åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Š.*?ç¿»è¨³ã—ã¾ã›ã‚“ã€‚',
            r'åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸ.*?åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–',
            r'presented.*?news.*?åºƒå‘Š',
            r'ã‚¹ãƒãƒ³ã‚µãƒ¼è¨˜äº‹',
            r'åºƒå‘Šè¨˜äº‹',
            r'PRè¨˜äº‹',
            r'presented.*?content'
        ]
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆAIãŒè‡ªå‹•ç”Ÿæˆã™ã‚‹ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆï¼‰
        unwanted_patterns = [
            r'### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚',
            r'### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹.*?',
            r'### ä»¥ä¸Š.*?',
            r'### çµ‚äº†.*?',
            r'### è¨˜äº‹ã¯ä»¥ä¸Šã§ã™ã€‚',
            r'### ä»¥ä¸ŠãŒ.*?ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚',
            r'### ä»¥ä¸Šã§.*?ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’çµ‚äº†ã—ã¾ã™ã€‚'
        ]
        
        # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã®è¡¨ç¤ºã‚’ä¿®æ­£ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        fix_patterns = [
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒä¸€è¡Œã«ã¾ã¨ã¾ã£ã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£
            (r'\*\*å¼•ç”¨å…ƒ\*\*:\s*([^*]+)\*\*\*ãƒªãƒ³ã‚¯\*\*:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒ*ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£
            (r'\*å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒ*ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£ï¼ˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            (r'\*å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^\s]+)ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^:]+):\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP**ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP*ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP*ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: [URL](URL) â†’ å¼•ç”¨å…ƒè¡Œ + URLç‹¬ç«‹è¡Œ
            (r'å¼•ç”¨å…ƒ:\s*([^\s]+)ãƒªãƒ³ã‚¯:\s*\[((?:https?|ftp)://[^\]]+)\]\(([^\)]+)\)', r'**å¼•ç”¨å…ƒ**: \1\n\n\3'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒåŒä¸€è¡Œï¼ˆ[]()ä»˜ããƒ»å¤ªå­—ã§ãªã„ï¼‰â†’ å¼•ç”¨å…ƒè¡Œ + URLç‹¬ç«‹è¡Œ
            (r'å¼•ç”¨å…ƒ:\s*([^\n]+?)\s*ãƒªãƒ³ã‚¯:\s*\[([^\]]+)\]\(([^\)]+)\)', r'**å¼•ç”¨å…ƒ**: \1\n\n\3'),
            # å¼•ç”¨å…ƒ: SCMP**ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP*ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # HTMLæ®µè½ã§å‡ºåŠ›ã•ã‚ŒãŸå¼•ç”¨æƒ…å ±ã‚’Markdown2è¡Œã«æ­£è¦åŒ–
            (r'<p[^>]*>\s*<strong>å¼•ç”¨å…ƒ</strong>:\s*([^<]+)<br\s*/?>\s*<strong>ãƒªãƒ³ã‚¯</strong>:\s*(https?://[^\s<]+)\s*</p>', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # strongã‚¿ã‚°æ··åœ¨ã®å˜è¡Œè¡¨è¨˜ã‚’æ­£è¦åŒ–
            (r'<strong>å¼•ç”¨å…ƒ</strong>:\s*([^<]+)\s*<strong>ãƒªãƒ³ã‚¯</strong>:\s*(https?://[^\s<]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2')
        ]

        # HTMLæ®‹éª¸ã®å‰Šé™¤ï¼ˆæ±ç”¨ï¼‰
        html_cleanup_patterns = [
            r'<p[^>]*>\s*</p>',                 # ç©ºã®p
            r'</?br\s*/?>',                    # brã‚¿ã‚°
            # [![...]](...) ã‚’åŒ…ã‚€ p/span ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’å‰¥ãŒã™
            r'<p[^>]*>\s*<span[^>]*>(\[!\[.*?\]\(.*?\)\]\(.*?\))\s*</span>\s*</p>'
        ]
        
        # åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»
        cleaned_body = body
        for pattern in ad_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        for pattern in unwanted_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã®è¡¨ç¤ºã‚’ä¿®æ­£
        for pattern, replacement in fix_patterns:
            cleaned_body = re.sub(pattern, replacement, cleaned_body, flags=re.DOTALL | re.IGNORECASE)

        # æ±ç”¨HTMLã‚¿ã‚°ã®æƒé™¤ï¼ˆå¿…è¦æœ€å°é™ï¼‰
        for pattern in html_cleanup_patterns:
            # ãƒ©ãƒƒãƒ‘ãƒ¼é™¤å»ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¯ç½®æ›å¯¾è±¡ã‚’æ®‹ã™
            if '(' in pattern and '\\[!\\[' in pattern:
                cleaned_body = re.sub(pattern, r'\1', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
            else:
                cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)

        # noteå´ã®è‡ªå‹•ãƒªãƒ³ã‚¯åŒ–ã«ä»»ã›ã‚‹ãŸã‚ã€URLã¯ãƒ—ãƒ¬ãƒ¼ãƒ³ã§ç‹¬ç«‹è¡Œã«ã™ã‚‹
        # **ãƒªãƒ³ã‚¯**: [text](url) â†’ ç©ºè¡Œ + url
        cleaned_body = re.sub(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*\[[^\]]+\]\((https?://[^\)]+)\)', r'\n\n\1', cleaned_body)
        # **ãƒªãƒ³ã‚¯**: url â†’ ç©ºè¡Œ + url
        cleaned_body = re.sub(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://\S+)', r'\n\n\1', cleaned_body)
        # ãƒªãƒ³ã‚¯: url â†’ ç©ºè¡Œ + url
        cleaned_body = re.sub(r'(?m)^ãƒªãƒ³ã‚¯:\s*(https?://\S+)', r'\n\n\1', cleaned_body)
        
        # è¡Œæœ«ã®ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ï¼ˆæ”¹è¡Œå‰ã®2ã‚¹ãƒšãƒ¼ã‚¹ãªã©ï¼‰
        cleaned_body = re.sub(r'[ \t]+$', '', cleaned_body, flags=re.MULTILINE)

        # é€£ç¶šé‡è¤‡ã™ã‚‹å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯ã‚’1ã¤ã«åœ§ç¸®ï¼ˆURLç‹¬ç«‹è¡Œå¯¾å¿œï¼‰
        cleaned_body = re.sub(r'(\*\*å¼•ç”¨å…ƒ\*\*: .*?\n+https?://\S+)\n+\1', r'\1', cleaned_body, flags=re.DOTALL)

        # åºƒæ±èªå­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é‡è¤‡è¡Œã‚’1å›ã«åœ§ç¸®ï¼ˆç”»åƒãƒªãƒ³ã‚¯2ç¨®ï¼‰
        cantonese_img1 = re.escape('[![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-img1.jpg)](https://line.me/R/ti/p/@298mwivr)')
        cantonese_img2 = re.escape('[![LINEã§ãŠå•åˆã›](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-qr.png)](https://line.me/R/ti/p/@298mwivr)')
        cleaned_body = re.sub(fr'(?:{cantonese_img1})\n+(?:{cantonese_img1})', r'\g<0>'.replace('\\g<0>','\1'), cleaned_body)
        cleaned_body = re.sub(fr'(?:{cantonese_img2})\n+(?:{cantonese_img2})', r'\g<0>'.replace('\\g<0>','\1'), cleaned_body)

        # ä¸Šè¨˜ã®ãƒãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ç”ŸæˆãŒé›£ã—ã„ãŸã‚æ˜ç¤ºç½®æ›ï¼ˆ2å›ä»¥ä¸Šã®é€£ç¶šã‚’1å›ã¸ï¼‰
        cleaned_body = re.sub(fr'(?:{cantonese_img1})(?:\n+{cantonese_img1})+', r'[![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-img1.jpg)](https://line.me/R/ti/p/@298mwivr)', cleaned_body)
        cleaned_body = re.sub(fr'(?:{cantonese_img2})(?:\n+{cantonese_img2})+', r'[![LINEã§ãŠå•åˆã›](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-qr.png)](https://line.me/R/ti/p/@298mwivr)', cleaned_body)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«
        cleaned_body = re.sub(r'\n{3,}', '\n\n', cleaned_body)
        
        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºè¡Œã‚’é™¤å»
        cleaned_body = cleaned_body.strip()
        
        return cleaned_body
    
    def remove_duplicate_articles(self, body: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–"""
        import re
        
        # ### ã§å§‹ã¾ã‚‹è¨˜äº‹ã‚’åˆ†å‰²
        articles = re.split(r'\n### ', body)
        
        # æœ€åˆã®è¦ç´ ã¯ç©ºã¾ãŸã¯å¤©æ°—æƒ…å ±ãªã®ã§ãã®ã¾ã¾ä¿æŒ
        if not articles:
            return body
        
        result = [articles[0]]
        seen_titles = []  # é¡ä¼¼åº¦åˆ¤å®šç”¨ã«ä¿æŒ
        seen_urls = set()  # æ­£è¦åŒ–URLã®é‡è¤‡æ’é™¤
        duplicate_count = 0
        
        def _normalize_title(t: str) -> str:
            return re.sub(r'[^\w\s]', '', t.lower()).strip()
        
        for article in articles[1:]:
            lines = article.split('\n')
            title = lines[0].strip() if lines else ''
            norm_title = _normalize_title(title)
            
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®æœ€åˆã®ç‹¬ç«‹URLè¡Œã‚’æŠ½å‡º
            block = '### ' + article
            url_match = re.search(r'(?m)^(https?://\S+)$', block)
            if url_match:
                from urllib.parse import urlparse, urlunparse
                try:
                    p = urlparse(url_match.group(1))
                    norm_url = urlunparse((p.scheme, p.netloc, p.path, '', '', ''))
                except Exception:
                    norm_url = url_match.group(1)
            else:
                norm_url = None
            
            # URLé‡è¤‡ã§é™¤å¤–
            if norm_url and norm_url in seen_urls:
                duplicate_count += 1
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«ãŒçŸ­ã™ãã‚‹å ´åˆã¯ãã®ã¾ã¾è¨±å®¹
            if len(norm_title) < 10:
                result.append(article)
                if norm_url:
                    seen_urls.add(norm_url)
                seen_titles.append(norm_title)
                continue
            
            # æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«ã¨é¡ä¼¼åº¦0.6ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã—ã¦é™¤å¤–
            is_dup = False
            for st in seen_titles:
                if calculate_title_similarity(norm_title, st) >= 0.6:
                    is_dup = True
                    break
            if is_dup:
                duplicate_count += 1
                continue
            
            result.append(article)
            seen_titles.append(norm_title)
            if norm_url:
                seen_urls.add(norm_url)
        
        if duplicate_count > 0:
            print(f"ğŸ”„ é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–: {duplicate_count}ä»¶")
        
        # å†çµåˆï¼ˆè¦‹å‡ºã—ã®å‰ã«ç©ºè¡Œã‚’å…¥ã‚Œã‚‹ï¼‰
        if len(result) > 1:
            return result[0] + '\n\n### ' + '\n\n### '.join(result[1:])
        else:
            return result[0]
    
    def save_article(self, article: Dict, weather_data: Dict = None, output_path: str = None) -> str:
        """ç”Ÿæˆã—ãŸè¨˜äº‹ã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        if output_path is None:
            timestamp = datetime.now(HKT).strftime('%Y-%m-%d')
            output_path = f"daily-articles/hongkong-news_{timestamp}.md"
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨é‡è¤‡ã‚’é™¤å¤–
        article['body'] = self.remove_advertisement_content(article['body'])
        article['body'] = self.remove_duplicate_articles(article['body'])
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åŒºåˆ‡ã‚Šç·šã‚’å‰Šé™¤ã—ã€è¦‹å‡ºã—å‰ã«ç©ºè¡Œã‚’è¿½åŠ 
        import re
        article['body'] = re.sub(r'\n+---\n+', '\n', article['body'])
        article['body'] = re.sub(r'\n{3,}', '\n\n', article['body'])
        # è¦‹å‡ºã—ã®å‰ã«å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
        article['body'] = re.sub(r'([^\n])\n(###)', r'\1\n\n\2', article['body'])
        
        # å¤©æ°—æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        weather_section = self.format_weather_info(weather_data) if weather_data else ""
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’çµ„ã¿ç«‹ã¦ï¼ˆç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ”¹è¡Œã‚’æŒŸã¾ãªã„ï¼‰
        content_parts = []
        if weather_section:
            content_parts.append(weather_section)
        if article['lead']:
            content_parts.append(article['lead'])
        content_parts.append(article['body'])
        
        # Markdownç”Ÿæˆ
        content_str = '\n\n'.join(content_parts)
        
        # åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’è¿½åŠ 
        cantonese_section = self._generate_cantonese_section()
        
        # bodyã®æœ€åˆã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ï¼ˆ1è¡Œç›®ãŒç©ºè¡Œã«ãªã‚Šã€ã“ã“ã«ç›®æ¬¡ã‚’æŒ¿å…¥ï¼‰
        markdown = f"""# {article['title']}

{content_str}

{cantonese_section}
----
**ã‚¿ã‚°**: {article['tags']}
**ç”Ÿæˆæ—¥æ™‚**: {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"ğŸ’¾ è¨˜äº‹ã‚’ä¿å­˜: {output_path}")
        return output_path

def normalize_url(url: str) -> str:
    """URLã‚’æ­£è¦åŒ–ï¼ˆã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦ãƒ™ãƒ¼ã‚¹URLã®ã¿æŠ½å‡ºï¼‰"""
    if not url:
        return ""
    try:
        from urllib.parse import urlparse, urlunparse
        parsed = urlparse(url)
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
        normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
        return normalized
    except:
        # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å…ƒã®URLã‚’è¿”ã™
        return url

def calculate_title_similarity(title1: str, title2: str) -> float:
    """2ã¤ã®ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰"""
    import re
    
    def normalize_title(t):
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ã€å˜èªåˆ†å‰²ï¼‰
        t = t.lower()
        t = re.sub(r'[^\w\s]', '', t)
        return set(t.split())
    
    words1 = normalize_title(title1)
    words2 = normalize_title(title2)
    
    if not words1 or not words2:
        return 0.0
    
    # å…±é€šå˜èªã®æ•°
    common_words = words1 & words2
    if len(common_words) < 3:
        return 0.0
    
    # Jaccardé¡ä¼¼åº¦ï¼ˆå…±é€šå˜èª / å…¨å˜èªï¼‰
    all_words = words1 | words2
    similarity = len(common_words) / len(all_words) if all_words else 0.0
    
    # ã‚ˆã‚Šå³å¯†ãªãƒã‚§ãƒƒã‚¯: å…±é€šç‡ãŒ60%ä»¥ä¸Šã‹ã¤ã€çŸ­ã„æ–¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã®70%ä»¥ä¸ŠãŒå…±é€š
    min_length = min(len(words1), len(words2))
    if min_length > 0:
        coverage = len(common_words) / min_length
        if similarity >= 0.6 and coverage >= 0.7:
            return similarity
    
    return 0.0

def preprocess_news(news_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ"""
    import re
    import os
    from collections import defaultdict
    from datetime import datetime, timedelta
    
    # 0. éå»ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å‡ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    past_urls = set()  # æ­£è¦åŒ–ã•ã‚ŒãŸURLã®ã‚»ãƒƒãƒˆ
    past_urls_original = set()  # å…ƒã®URLã‚‚ä¿æŒï¼ˆæŠ½å‡ºç”¨ï¼‰
    past_titles = []
    
    # éå»7æ—¥åˆ†ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ3æ—¥â†’7æ—¥ã«å»¶é•·ï¼‰
    for days_ago in range(1, 8):
        past_date = datetime.now(HKT) - timedelta(days=days_ago)
        past_file = f"daily-articles/hongkong-news_{past_date.strftime('%Y-%m-%d')}.md"
        
        if os.path.exists(past_file):
            print(f"ğŸ“‚ éå»è¨˜äº‹ãƒã‚§ãƒƒã‚¯: {past_file}")
            try:
                with open(past_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # URLã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®å½¢å¼ã«å¯¾å¿œï¼‰
                    # å½¢å¼1: **ãƒªãƒ³ã‚¯**: https://...ï¼ˆåŒã˜è¡Œï¼‰
                    url_matches1 = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://[^\s\n]+)', content)
                    # å½¢å¼2: **ãƒªãƒ³ã‚¯**: ã®å¾Œã®ç‹¬ç«‹è¡Œã®URLï¼ˆæ”¹è¡Œå¾Œã™ãã®URLï¼‰
                    url_matches2 = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:[^\n]*\n+\n*(https?://[^\s\n]+)', content)
                    # å½¢å¼3: **å¼•ç”¨å…ƒ**: ã®å¾Œã®ç‹¬ç«‹è¡Œã®URLï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ãªå½¢å¼ã€æ”¹è¡Œå¾Œã«URLãŒæ¥ã‚‹ï¼‰
                    url_matches3 = re.findall(r'\*\*å¼•ç”¨å…ƒ\*\*:[^\n]+\n+\n*(https?://[^\s\n]+)', content)
                    # å½¢å¼4: ### è¦‹å‡ºã—ã®å¾Œã®æ®µè½ã§ã€**å¼•ç”¨å…ƒ**: ã¾ãŸã¯ **ãƒªãƒ³ã‚¯**: ã®ç›´å¾Œã«æ¥ã‚‹ç‹¬ç«‹è¡Œã®URL
                    url_matches4 = re.findall(r'(?:\*\*å¼•ç”¨å…ƒ\*\*:|\*\*ãƒªãƒ³ã‚¯\*\*:)[^\n]*(?:\n+)(https?://[^\s\n]+)', content)
                    
                    all_urls = url_matches1 + url_matches2 + url_matches3 + url_matches4
                    # é‡è¤‡ã‚’é™¤å»ï¼ˆåŒã˜URLãŒè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æŠ½å‡ºã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰
                    all_urls = list(set(all_urls))
                    for url in all_urls:
                        # å…ƒã®URLã‚‚ä¿æŒ
                        past_urls_original.add(url.strip())
                        # æ­£è¦åŒ–ã—ãŸURLã‚’è¿½åŠ 
                        normalized = normalize_url(url.strip())
                        if normalized:
                            past_urls.add(normalized)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆ### ã®å¾Œã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                    title_matches = re.findall(r'^### (.+)$', content, re.MULTILINE)
                    # å¤©æ°—äºˆå ±ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    filtered_titles = [t for t in title_matches if 'å¤©æ°—' not in t and 'weather' not in t.lower() and 'å¤©æ°—äºˆå ±' not in t]
                    past_titles.extend(filtered_titles)
                    
                print(f"  âœ“ æ—¢å‡ºURL: {len(all_urls)}ä»¶ï¼ˆæ­£è¦åŒ–å¾Œ: {len(past_urls)}ä»¶ï¼‰ã€æ—¢å‡ºã‚¿ã‚¤ãƒˆãƒ«: {len(filtered_titles)}ä»¶")
            except Exception as e:
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    if past_urls:
        print(f"ğŸ” éå»è¨˜äº‹ã‹ã‚‰åˆè¨ˆ {len(past_urls_original)} ä»¶ã®URLï¼ˆæ­£è¦åŒ–å¾Œ: {len(past_urls)}ä»¶ï¼‰ã¨ {len(past_titles)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º")
    
    # éå»è¨˜äº‹ã¨ã®é‡è¤‡ã‚’é™¤å¤–
    filtered_news = []
    duplicate_count = 0
    url_duplicate_count = 0
    title_duplicate_count = 0
    
    for news in news_list:
        url = news.get('url', '')
        title = news.get('title', '')
        description = news.get('description', '')
        
        # å¤©æ°—é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
        weather_keywords = ['æ°—æ¸©', 'å¤©æ°—', 'å¤©æ–‡å°', 'æ°—è±¡', 'å¤©å€™', 'temperature', 'weather', 'observatory', 'forecast', 'â„ƒ', 'åº¦', 'tropical', 'storm', 'typhoon', 'å°é¢¨']
        if any(keyword in title.lower() or keyword in description.lower() for keyword in weather_keywords):
            duplicate_count += 1
            continue
        
        # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œã®URLã§æ¯”è¼ƒï¼‰
        normalized_url = normalize_url(url)
        if normalized_url and normalized_url in past_urls:
            url_duplicate_count += 1
            duplicate_count += 1
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆé¡ä¼¼åº¦ãŒ0.6ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã¿ãªã™ï¼‰
        is_similar = False
        for past_title in past_titles:
            similarity = calculate_title_similarity(title, past_title)
            if similarity >= 0.6:
                is_similar = True
                title_duplicate_count += 1
                break
        
        if is_similar:
            duplicate_count += 1
            continue
        
        filtered_news.append(news)
    
    if duplicate_count > 0:
        print(f"ğŸš« éå»è¨˜äº‹ã¨ã®é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶ï¼ˆURLé‡è¤‡: {url_duplicate_count}ä»¶ã€ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼: {title_duplicate_count}ä»¶ï¼‰")
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(news_list)} â†’ {len(filtered_news)}ä»¶")
    
    # 1. åŒæ—¥å†…é‡è¤‡é™¤å¤–ï¼ˆURLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸¡æ–¹ã§ãƒã‚§ãƒƒã‚¯ï¼‰
    seen_titles_normalized = set()  # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ï¼ˆé«˜é€Ÿãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
    seen_titles_original = []  # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆé¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
    seen_urls = set()  # æ­£è¦åŒ–ã•ã‚ŒãŸURLã®ã‚»ãƒƒãƒˆ
    unique_news = []
    same_day_duplicates = 0
    same_day_url_duplicates = 0
    same_day_title_duplicates = 0
    
    def is_hk_related_news(item):
        title = item.get('title', '').lower()
        description = item.get('description', '').lower()
        url = item.get('url', '').lower()
        source = item.get('source', '').lower()
        positive = [
            'hong kong', 'hongkong', 'é¦™æ¸¯', 'kowloon', 'ä¹é¾', 'æ–°ç•Œ', 'hksar', 'å°–æ²™å’€', 'ç£ä»”', 'ä¸­ç’°', 'æ—ºè§’',
            'é¦™æ¸¯å¤©æ–‡å°', 'hong kong observatory', 'mtr', 'æ¸¯éµ', 'hkex', 'é¦™æ¸¯äº¤æ˜“æ‰€'
        ]
        if any(p in (title + ' ' + description) for p in positive):
            return True
        if any(seg in url for seg in ['/hong-kong', '/hongkong', '/news/hong-kong']) or '.hk/' in url:
            return True
        if any(s in source for s in ['rthk', 'hk01', 'hket', 'the standard', 'chinadaily hk', 'yahoo news hk']):
            return True
        if 'scmp' in source or 'scmp.com' in url:
            return ('/hong-kong' in url) or ('/hongkong' in url) or ('/news/hong-kong' in url)
        return False

    for news in filtered_news:
        url = news.get('url', '')
        title = news.get('title', '')
        normalized_title = re.sub(r'[^\w\s]', '', title.lower())
        normalized_url = normalize_url(url)
        
        # é¦™æ¸¯é–¢é€£ä»¥å¤–ã¯é™¤å¤–ï¼ˆSCMPãƒ“ã‚¸ãƒã‚¹ç­‰ã®ä¸–ç•Œè¨˜äº‹ã®æ··å…¥ã‚’é˜²ãï¼‰
        if not is_hk_related_news(news):
            same_day_duplicates += 1
            continue

        # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
        is_url_duplicate = normalized_url and normalized_url in seen_urls
        
        # ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–å¾Œã®å®Œå…¨ä¸€è‡´ï¼‰
        is_title_duplicate = normalized_title in seen_titles_normalized
        
        # ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ã‚‚å®Ÿè¡Œï¼ˆæ—¢ã«è¿½åŠ æ¸ˆã¿ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨æ¯”è¼ƒï¼‰
        if not is_title_duplicate:
            for existing_title in seen_titles_original:
                similarity = calculate_title_similarity(title, existing_title)
                if similarity >= 0.6:
                    is_title_duplicate = True
                    break
        
        if is_url_duplicate or is_title_duplicate:
            same_day_duplicates += 1
            if is_url_duplicate:
                same_day_url_duplicates += 1
            if is_title_duplicate:
                same_day_title_duplicates += 1
            continue
        
        # é‡è¤‡ãªã—ã®å ´åˆã€ãƒªã‚¹ãƒˆã«è¿½åŠ 
        unique_news.append(news)
        seen_titles_normalized.add(normalized_title)
        seen_titles_original.append(title)  # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ä¿æŒ
        if normalized_url:
            seen_urls.add(normalized_url)
    
    if same_day_duplicates > 0:
        print(f"ğŸ“Š åŒæ—¥å†…é‡è¤‡é™¤å¤–: {len(filtered_news)} â†’ {len(unique_news)}ä»¶ï¼ˆURLé‡è¤‡: {same_day_url_duplicates}ä»¶ã€ã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼: {same_day_title_duplicates}ä»¶ï¼‰")
    
    # 2. ã‚¤ãƒ™ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆåŒä¸€å‡ºæ¥äº‹ã‚’1æœ¬ã«çµ±åˆï¼‰
    clustered = []
    cluster_titles = []
    for item in unique_news:
        title = item.get('title', '')
        norm_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
        is_same_event = False
        for ct in cluster_titles:
            if calculate_title_similarity(norm_title, ct) >= 0.85:
                is_same_event = True
                break
        if is_same_event:
            # ä»£è¡¨ã®æƒ…å ±é‡ã§ç½®æ›ï¼ˆã‚ˆã‚Šæœ¬æ–‡/èª¬æ˜ãŒé•·ã„æ–¹ã‚’æ¡ç”¨ï¼‰
            prev = clustered[-1]
            prev_len = len(prev.get('full_content', prev.get('description', '')))
            curr_len = len(item.get('full_content', item.get('description', '')))
            if curr_len > prev_len:
                clustered[-1] = item
                cluster_titles[-1] = norm_title
        else:
            clustered.append(item)
            cluster_titles.append(norm_title)
    print(f"ğŸ§® ã‚¤ãƒ™ãƒ³ãƒˆçµ±åˆ: {len(unique_news)} â†’ {len(clustered)}ä»¶ï¼ˆã‚¿ã‚¤ãƒˆãƒ«é¡ä¼¼â‰¥0.85ã§1æœ¬åŒ–ï¼‰")

    # 3. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    categorized = defaultdict(list)
    
    for news in clustered:
        title = news.get('title', '').lower()
        description = news.get('description', '').lower()
        content = f"{title} {description}"
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®š
        if any(keyword in content for keyword in ['ãƒ“ã‚¸ãƒã‚¹', 'çµŒæ¸ˆ', 'é‡‘è', 'æ ªå¼', 'æŠ•è³‡', 'business', 'economy', 'finance', 'stock', 'investment', 'ipo', 'ä¸Šå ´', 'å–å¼•æ‰€', 'éŠ€è¡Œ', 'ä¿é™º']):
            category = 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ'
        elif any(keyword in content for keyword in ['ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'ai', 'äººå·¥çŸ¥èƒ½', 'ãƒ­ãƒœãƒƒãƒˆ', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢', 'technology', 'digital', 'app', 'software', 'hardware', 'ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼']):
            category = 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif any(keyword in content for keyword in ['åŒ»ç™‚', 'å¥åº·', 'ç—…é™¢', 'åŒ»å¸«', 'è–¬', 'æ²»ç™‚', 'medical', 'health', 'hospital', 'doctor', 'medicine', 'treatment', 'covid', 'ã‚³ãƒ­ãƒŠ', 'ãƒ¯ã‚¯ãƒãƒ³']):
            category = 'åŒ»ç™‚ãƒ»å¥åº·'
        elif any(keyword in content for keyword in ['æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'å­¦ç”Ÿ', 'æ•™å¸«', 'education', 'school', 'university', 'student', 'teacher', 'å­¦ç¿’', 'ç ”ç©¶']):
            category = 'æ•™è‚²'
        elif any(keyword in content for keyword in ['ä¸å‹•ç”£', 'ä½å®…', 'ãƒãƒ³ã‚·ãƒ§ãƒ³', 'åœŸåœ°', 'è³ƒè²¸', 'real estate', 'property', 'housing', 'apartment', 'rent', 'åœŸåœ°', 'å»ºç‰©']):
            category = 'ä¸å‹•ç”£'
        elif any(keyword in content for keyword in ['äº¤é€š', 'é›»è»Š', 'ãƒã‚¹', 'ã‚¿ã‚¯ã‚·ãƒ¼', 'ç©ºæ¸¯', 'transport', 'train', 'bus', 'taxi', 'airport', 'mtr', 'åœ°ä¸‹é‰„', 'è·¯ç·š']):
            category = 'äº¤é€š'
        elif any(keyword in content for keyword in ['çŠ¯ç½ª', 'é€®æ•', 'è­¦å¯Ÿ', 'è£åˆ¤', 'åˆ‘å‹™æ‰€', 'crime', 'arrest', 'police', 'court', 'prison', 'é•æ³•', 'äº‹ä»¶', 'æœæŸ»']):
            category = 'æ²»å®‰ãƒ»çŠ¯ç½ª'
        elif any(keyword in content for keyword in ['äº‹æ•…', 'ç½å®³', 'ç«äº‹', 'åœ°éœ‡', 'å°é¢¨', 'accident', 'disaster', 'fire', 'earthquake', 'typhoon', 'ç·Šæ€¥', 'æ•‘åŠ©']):
            category = 'äº‹æ•…ãƒ»ç½å®³'
        elif any(keyword in content for keyword in ['æ”¿æ²»', 'æ”¿åºœ', 'è­°å“¡', 'é¸æŒ™', 'æ”¿ç­–', 'politics', 'government', 'minister', 'election', 'policy', 'è¡Œæ”¿', 'è­°ä¼š']):
            category = 'æ”¿æ²»ãƒ»è¡Œæ”¿'
        elif any(keyword in content for keyword in ['æ–‡åŒ–', 'èŠ¸èƒ½', 'ã‚¹ãƒãƒ¼ãƒ„', 'æ˜ ç”»', 'éŸ³æ¥½', 'ã‚¢ãƒ¼ãƒˆ', 'culture', 'entertainment', 'sports', 'movie', 'music', 'art', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ç¥­ã‚Š', 'ä¼çµ±']):
            category = 'ã‚«ãƒ«ãƒãƒ£ãƒ¼'
        else:
            category = 'ç¤¾ä¼šãƒ»ãã®ä»–'
        
        categorized[category].append(news)
    
    print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ä»¶æ•°:")
    for cat, items in sorted(categorized.items(), key=lambda x: -len(x[1])):
        print(f"  {cat}: {len(items)}ä»¶")
    
    # 4. ãƒãƒ©ãƒ³ã‚¹é¸æŠï¼ˆå³ã—ã‚1æœ¬/ã‚¤ãƒ™ãƒ³ãƒˆ + ãƒãƒ©ã‚¨ãƒ†ã‚£ç¢ºä¿ï¼‰
    selected = []
    target_count = 18
    max_per_source = 6
    per_source_counts = defaultdict(int)
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã”ã¨ã®å„ªå…ˆé †ä½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šé †ï¼‰
    priority_cats = [
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ',      # 1ä½: 46ä»¶
        'ç¤¾ä¼šãƒ»ãã®ä»–',        # 2ä½: 19ä»¶  
        'ã‚«ãƒ«ãƒãƒ£ãƒ¼',          # 3ä½: 15ä»¶
        'ä¸å‹•ç”£',             # 4ä½: 13ä»¶
        'æ”¿æ²»ãƒ»è¡Œæ”¿',          # 5ä½: 8ä»¶
        'åŒ»ç™‚ãƒ»å¥åº·',          # 6ä½: 3ä»¶
        'æ²»å®‰ãƒ»çŠ¯ç½ª',          # 7ä½: 6ä»¶
        'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼',        # 8ä½: 76ä»¶
        'äº‹æ•…ãƒ»ç½å®³',          # 9ä½: 1ä»¶
        'äº¤é€š'                # 10ä½: 1ä»¶
    ]
    
    # 4-1. å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æœ€ä½1ä»¶ãšã¤ï¼ˆåœ¨åº«ãŒã‚ã‚Œã°ï¼‰
    for cat in priority_cats:
        if cat in categorized and categorized[cat]:
            for item in categorized[cat][:]:
                src = item.get('source', 'unknown')
                if per_source_counts[src] >= max_per_source:
                    continue
                selected.append(item)
                per_source_counts[src] += 1
                categorized[cat].remove(item)
                break
        if len(selected) >= target_count:
            break

    # 4-2. å„ªå…ˆé †ä½ã«åŸºã¥ãæ®‹ã‚Šã‚’å……å½“ï¼ˆã‚½ãƒ¼ã‚¹ä¸Šé™ã¨åœ¨åº«å°Šé‡ï¼‰
    for cat in priority_cats:
        if len(selected) >= target_count:
            break
        if cat not in categorized or not categorized[cat]:
            continue
        for item in categorized[cat][:]:
            if len(selected) >= target_count:
                break
            src = item.get('source', 'unknown')
            if per_source_counts[src] >= max_per_source:
                continue
            selected.append(item)
            per_source_counts[src] += 1
            categorized[cat].remove(item)
    
    # 4-3. ã¾ã è¶³ã‚Šãªã„å ´åˆã¯æ®‹ã‚Šã‹ã‚‰å……å½“ï¼ˆã‚½ãƒ¼ã‚¹ä¸Šé™ã‚’ç¶­æŒï¼‰
    if len(selected) < target_count:
        for cat in priority_cats:
            if len(selected) >= target_count:
                break
            if cat not in categorized or not categorized[cat]:
                continue
            for item in categorized[cat][:]:
                if len(selected) >= target_count:
                    break
                src = item.get('source', 'unknown')
                if per_source_counts[src] >= max_per_source:
                    continue
                selected.append(item)
                per_source_counts[src] += 1
                categorized[cat].remove(item)
    
    print(f"\nâœ… é¸æŠå®Œäº†: {len(selected)}ä»¶ï¼ˆå„ªå…ˆé †ä½èª¿æ•´æ¸ˆã¿ï¼‰")
    
    # é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³ã‚’è¡¨ç¤º
    selected_categories = defaultdict(int)
    for news in selected:
        category = news.get('category', 'æœªåˆ†é¡')
        selected_categories[category] += 1
    
    print("ğŸ“Š é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³:")
    for cat in priority_cats:
        if cat in selected_categories:
            print(f"  {cat}: {selected_categories[cat]}ä»¶")
    
    return selected

if __name__ == "__main__":
    import sys
    import os
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_article.py <raw_news.json>")
        sys.exit(1)
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’HKTã«è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°HKTï¼‰
    os.environ['TZ'] = os.environ.get('TZ', 'Asia/Hong_Kong')
    
    news_file = sys.argv[1]
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’è¡¨ç¤º
    today = datetime.now(HKT).strftime('%Y-%m-%d')
    print(f"\nğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (HKT): {today}")
    print(f"ğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (æ—¥æœ¬èª): {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    print("=" * 60)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹äº‹å‰å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ
    news_data = preprocess_news(data['news'])
    
    print("=" * 60)
    
    # ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ‘ã‚¹ã®æ±ºå®šï¼ˆå„ªå…ˆé †ä½: ç’°å¢ƒå¤‰æ•°CONFIG_PATH > config.local.json > config.jsonï¼‰
    config_path = os.environ.get('CONFIG_PATH')
    if not config_path:
        if os.path.exists('config.local.json'):
            config_path = 'config.local.json'
        else:
            config_path = 'config.json'

    generator = GrokArticleGenerator(config_path)
    article = generator.generate_article(news_data)
    
    if article:
        # å¤©æ°—æƒ…å ±ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        weather_data = data.get('weather', None)
        saved_path = generator.save_article(article, weather_data)
        
        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥ä»˜ã‚’ç¢ºèª
        expected_date = datetime.now(HKT).strftime('%Y-%m-%d')
        file_date = saved_path.split('_')[-1].replace('.md', '')
        
        print(f"\nâœ… è¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_path}")
        print(f"ğŸ“… ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜: {file_date}")
        print(f"ğŸ“… æœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜: {expected_date}")
        
        if file_date != expected_date:
            print(f"âš ï¸  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜ãŒæœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜ã¨ä¸€è‡´ã—ã¾ã›ã‚“ï¼")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {file_date}, æœŸå¾…: {expected_date}")
        
        print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
        if weather_data:
            print(f"ğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

