#!/usr/bin/env python3
"""
Grok APIã‚’ä½¿ç”¨ã—ã¦é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ
â€» è¦ç´„ã‚„çŸ­ç¸®ã¯ä¸€åˆ‡è¡Œã‚ãšã€å…ƒã®æƒ…å ±ã‚’ãã®ã¾ã¾ç¿»è¨³ã™ã‚‹
"""

import requests
import json
from datetime import datetime
from typing import List, Dict

class GrokArticleGenerator:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        self.api_key = self.config['grok_api']['api_key']
        self.api_url = self.config['grok_api']['api_url']
        
    def generate_article(self, news_data: List[Dict]) -> Dict:
        """Grok APIã§æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ"""
        print("\nğŸ¤– Grok APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        print("=" * 60)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        news_text = self._format_news_for_prompt(news_data)
        
        # Grok APIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯é¦™æ¸¯ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ•´å½¢ã™ã‚‹ç¿»è¨³è€…ã§ã™ã€‚

ã€æœ€é‡è¦ã€‘è¦ç´„ã‚„çŸ­ç¸®ã¯çµ¶å¯¾ç¦æ­¢ã€‚å…ƒã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹(Full Content)ã‚’ãã®ã¾ã¾å…¨æ–‡æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹æ§‹æˆã€‘
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ä»¥ä¸‹ã®Markdownå½¢å¼ã§è¨˜è¼‰ï¼ˆç•ªå·ãªã—ï¼‰:

### [ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«]

[Full Contentã‚’å…¨æ–‡ç¿»è¨³ã€æ®µè½ã¯ç©ºç™½è¡Œã§åŒºåˆ‡ã‚‹]

**å¼•ç”¨å…ƒ**: [sourceå], [æ—¥ä»˜]  
**ãƒªãƒ³ã‚¯**: [å®Œå…¨ãªURL]  
**å‚™è€ƒ**: [é‡è¦æ€§ã‚„æ—¥æœ¬ã¨ã®é–¢é€£æ€§ã‚’1æ–‡ã§]

---

### [æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«]

[Full Contentã‚’å…¨æ–‡ç¿»è¨³]

**å¼•ç”¨å…ƒ**: [sourceå], [æ—¥ä»˜]  
**ãƒªãƒ³ã‚¯**: [å®Œå…¨ãªURL]  
**å‚™è€ƒ**: [é‡è¦æ€§ã‚„æ—¥æœ¬ã¨ã®é–¢é€£æ€§ã‚’1æ–‡ã§]

ã€å‡ºåŠ›ä¾‹ã€‘
### é¦™æ¸¯ç«‹æ³•ä¼šã€ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢è¦åˆ¶æ³•æ¡ˆå¯æ±º

é¦™æ¸¯ã®ç«‹æ³•ä¼šã¯ã€ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢ã‚µãƒ¼ãƒ“ã‚¹ã‚’è¦åˆ¶ã™ã‚‹æ³•æ¡ˆã‚’å¯æ±ºã—ã¾ã—ãŸã€‚

é‹è¼¸å±€ã®é™³ç¾å®å±€é•·ã¯ã€ã“ã®æ³•æ¡ˆãŒäº¤é€šã‚µãƒ¼ãƒ“ã‚¹ã‚’è¿‘ä»£åŒ–ã™ã‚‹ã¨è¿°ã¹ã¾ã—ãŸã€‚

ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢äº‹æ¥­è€…ã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å–å¾—ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

**å¼•ç”¨å…ƒ**: SCMP, 2025å¹´10æœˆ16æ—¥  
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/...  
**å‚™è€ƒ**: äº¤é€šæ”¿ç­–ã®è»¢æ©Ÿã§å³æ™‚æ€§ãŒé«˜ã„ã€‚

---

### å°æ¹¾ã§é¦™æ¸¯äººè¦³å…‰å®¢ãŒå¼·å§¦ã•ã‚Œã‚‹äº‹ä»¶å¾Œã€å…¬è¡†å®‰å…¨ã®æ”¹å–„ã‚’æ±‚ã‚ã‚‹ã‚¢ãƒ‰ãƒœã‚«ã‚·ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—

é¦™æ¸¯ã®è¦³å…‰å®¢ãŒæ˜¼é–“ã€å°åŒ—é§…ã§çŸ¥äººã§ã‚ã‚Šé€ƒäº¡ä¸­ã®ç”·ã«ã‚ˆã£ã¦å¼·å§¦ã•ã‚ŒãŸã¨ã•ã‚Œã‚‹äº‹ä»¶å¾Œã€ã‚¢ãƒ‰ãƒœã‚«ã‚·ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã¯å…¬è¡†å®‰å…¨ã¨å‚è¦³è€…ã®ä»‹å…¥ã®æ”¹å–„ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚

**å¼•ç”¨å…ƒ**: SCMP, 2025å¹´10æœˆ16æ—¥  
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/...  
**å‚™è€ƒ**: é¦™æ¸¯äººè¦³å…‰å®¢ã®å®‰å…¨ã«é–¢ã‚ã‚‹äº‹ä»¶ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã‚‹ã€‚

ã€é‡è¦ã€‘
- ç•ªå·ï¼ˆ1. 2. ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œ### ã‚¿ã‚¤ãƒˆãƒ«åã€å½¢å¼ã§å°è¦‹å‡ºã—ã«ã™ã‚‹
- ãƒ‹ãƒ¥ãƒ¼ã‚¹é–“ã¯ã€Œ---ã€ã§åŒºåˆ‡ã‚‹
- å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã¯ã€Œ**é …ç›®å**:ã€å½¢å¼ã§å¤ªå­—ã«ã™ã‚‹

ã€ç¿»è¨³ãƒ«ãƒ¼ãƒ«ã€‘
- åœ°å: å¤©æ°´åœ(ãƒ†ã‚£ãƒ³ãƒ»ã‚·ãƒ¥ã‚¤ãƒ»ãƒ¯ã‚¤)ã€èª¿æ™¯å¶º(ãƒ†ã‚£ã‚¦ãƒ»ã‚±ãƒ³ãƒ»ãƒ¬ãƒ³)ã®ã‚ˆã†ã«æ¼¢å­—+èª­ã¿ä»®å
- äººå: æè‹±è¯(Li Ying-wah)ã®ã‚ˆã†ã«æ¼¢å­—+è‹±èªèª­ã¿
- çµ„ç¹”å: å»‰æ”¿å…¬ç½²(ICAC)ã®ã‚ˆã†ã«æ¼¢å­—+ç•¥ç§°
- æ•°å­—ãƒ»é‡‘é¡: ãã®ã¾ã¾ç¿»è¨³
- å›ºæœ‰åè©: åŸèªã‚’å°Šé‡

ã€çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ã€‘
- Full Contentã®çŸ­ç¸®ãƒ»è¦ç´„ã¯çµ¶å¯¾ç¦æ­¢
- å…ƒã®æƒ…å ±ã‚’ã™ã¹ã¦å«ã‚ã‚‹
- æ–‡å­—æ•°åˆ¶é™ãªã—
- é¦™æ¸¯é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿é¸æŠï¼ˆæœ€å¤§20ä»¶ï¼‰
- é‡è¦åº¦ã®é«˜ã„é †ã«ä¸¦ã¹ã‚‹
- åºƒå‘Šè¨˜äº‹ï¼ˆpresented, sponsoredå«ã‚€URLï¼‰ã¯é™¤å¤–

ã€å‡ºåŠ›å½¢å¼ã€‘
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚JSONä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
{
  "title": "æ¯æ—¥AIãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹(2025å¹´10æœˆ19æ—¥)",
  "lead": "",
  "body": "ä¸Šè¨˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®Markdownè¨˜äº‹",
  "tags": "é¦™æ¸¯,ãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
}

ã€é‡è¦ã€‘JSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã€ä»–ã®èª¬æ˜æ–‡ã‚„è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

        user_prompt = f"""ä»¥ä¸‹ã¯{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}ã®é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’å…ƒã«ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªè¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€æœ€é‡è¦ã€‘Full Contentã®å†…å®¹ã¯çµ¶å¯¾ã«çŸ­ç¸®ãƒ»è¦ç´„ã›ãšã€å…¨æ–‡ãã®ã¾ã¾æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
- 1ã¤ã®è¨˜äº‹ã®å†…å®¹ã¯æœ€ä½500æ–‡å­—ä»¥ä¸Šã«ã™ã‚‹
- å…ƒã®æƒ…å ±ã‚’ã™ã¹ã¦å«ã‚ã‚‹
- å…·ä½“çš„ãªå›ºæœ‰åè©ã€æ•°å­—ã€æ—¥ä»˜ã‚’ã™ã¹ã¦å«ã‚ã‚‹

{news_text}

ä¸Šè¨˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é¦™æ¸¯é–¢é€£ã®ã‚‚ã®ã‚’20ä»¶é¸ã³ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚20ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å¿…ãšä½œæˆã—ã¦ãã ã•ã„ã€‚
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã€Œå†…å®¹ã€ã¯å…ƒã®Full Contentã‚’å…¨æ–‡ç¿»è¨³ã—ã¦ãã ã•ã„ï¼ˆçŸ­ç¸®ç¦æ­¢ï¼‰ã€‚

ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šé‡è¤‡ã®æ’é™¤ã€‘
1. **åŒã˜äº‹ä»¶ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã«ã¤ã„ã¦ã€è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ï¼ˆSCMPã€RTHKã€Yahooç­‰ï¼‰ã‹ã‚‰å ±é“ã•ã‚Œã¦ã„ã‚‹å ´åˆã€æœ€ã‚‚è©³ç´°ãª1ã¤ã ã‘ã‚’é¸æŠã—ã¦ãã ã•ã„**
2. **ä¾‹ï¼šã€Œè¯æ‡‹ã‚¿ãƒ¯ãƒ¼ç«ç½ã€ãªã©ã€åŒã˜ç«ç½äº‹ä»¶ã¯1ã¤ã ã‘**
3. å„è¨˜äº‹ã¯ç•°ãªã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ»äº‹ä»¶ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

ã€é‡è¦ã€‘åˆ©ç”¨å¯èƒ½ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰**ç•°ãªã‚‹äº‹ä»¶ãƒ»ãƒˆãƒ”ãƒƒã‚¯**ã‚’å¿…ãš20ä»¶é¸ã³ã€è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚20ä»¶ã‚’ç›®æ¨™ã«ã€ã§ãã‚‹ã ã‘å¤šãã®è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"""

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "model": "grok-2-latest",
            "stream": False,
            "temperature": 0.1  # æ­£ç¢ºæ€§ã‚’æœ€å„ªå…ˆ
        }
        
        try:
            print("ğŸ“¤ Grok APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=300  # 120ç§’ â†’ 300ç§’ï¼ˆ5åˆ†ï¼‰ã«å»¶é•·
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                print("âœ… è¨˜äº‹ç”Ÿæˆå®Œäº†")
                
                # JSONãƒ‘ãƒ¼ã‚¹
                try:
                    import re
                    
                    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                    if '```json' in content:
                        content = content.split('```json')[1].split('```')[0].strip()
                    elif '```' in content:
                        content = content.split('```')[1].split('```')[0].strip()
                    
                    # JSONã¨ã—ã¦æœ€åˆã® { ã‹ã‚‰æœ€å¾Œã® } ã‚’æŠ½å‡º
                    if '{' in content and '}' in content:
                        start = content.find('{')
                        end = content.rfind('}') + 1
                        json_str = content[start:end]
                        
                        # åˆ¶å¾¡æ–‡å­—ã‚’å®Œå…¨ã«é™¤å»
                        json_str = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\t')
                        
                        article = json.loads(json_str)
                        return article
                    else:
                        raise json.JSONDecodeError("No JSON object found", content, 0)
                        
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                    print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {content[:500]}...")
                    print("   ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡ºä¸­...")
                    
                    # JSONå½¢å¼ã‚’è«¦ã‚ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
                    lines = content.split('\n')
                    title_line = [l for l in lines if 'title' in l.lower() and ':' in l]
                    if title_line:
                        title = title_line[0].split(':', 1)[1].strip().strip('"').strip(',').strip('"')
                    else:
                        title = f"æ¯æ—¥AIã‚¹ãƒ©ãƒ³ã‚°ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')})"
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡º
                    lines = content.split('\n')
                    title_line = [l for l in lines if 'title' in l.lower() and ':' in l]
                    if title_line:
                        title = title_line[0].split(':', 1)[1].strip().strip('"').strip(',').strip('"')
                    else:
                        title = f"æ¯æ—¥AIã‚¹ãƒ©ãƒ³ã‚°ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')})"
                    
                    # bodyã‹ã‚‰Markdownè¨˜äº‹ã‚’æŠ½å‡º
                    body_start = content.find('"body":')
                    if body_start != -1:
                        body_start = content.find('"', body_start + 7) + 1
                        body_end = content.rfind('"')
                        if body_end > body_start:
                            body = content[body_start:body_end]
                            # JSONã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è§£é™¤
                            body = body.replace('\\n', '\n').replace('\\"', '"').replace('\\/', '/')
                        else:
                            body = content
                    else:
                        body = content
                    
                    return {
                        "title": title,
                        "lead": "",
                        "body": body,
                        "tags": "é¦™æ¸¯,ãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
                    }
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                print(f"   è©³ç´°: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")
            return None
    
    def _format_news_for_prompt(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢"""
        formatted = []
        for i, news in enumerate(news_data, 1):  # å…¨ä»¶ä½¿ç”¨
            # full_contentãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°description
            content = news.get('full_content', news.get('description', 'N/A'))
            formatted.append(f"""
ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹{i}ã€‘
Title: {news.get('title', 'N/A')}
Full Content: {content}
Source: {news.get('source', 'N/A')}
URL: {news.get('url', 'N/A')}
Published: {news.get('published_at', 'N/A')}
""")
        return "\n".join(formatted)
    
    def format_weather_info(self, weather_data: Dict) -> str:
        """å¤©æ°—æƒ…å ±ã‚’Markdownå½¢å¼ã«æ•´å½¢"""
        if not weather_data:
            return ""
        
        import re
        
        def clean_weather_text(text: str) -> str:
            """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            if not text:
                return ""
            # HTMLã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            text = re.sub(r'<br\s*/?>', '\n', text)
            # ä»–ã®HTMLã‚¿ã‚°ã‚’é™¤å»
            text = re.sub(r'<[^>]+>', '', text)
            # å„è¡Œã”ã¨ã«å‡¦ç†
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                # è¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                line = re.sub(r'\s+', ' ', line).strip()
                if line:  # ç©ºè¡Œã¯é™¤å¤–
                    cleaned_lines.append(line)
            # é©åˆ‡ãªæ”¹è¡Œã§çµåˆ
            return ' '.join(cleaned_lines)
        
        weather_section = "## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—"
        
        # å¤©æ°—è­¦å ±
        if 'weather_warning' in weather_data:
            warning = weather_data['weather_warning']
            title = warning.get('title', 'N/A')
            desc = clean_weather_text(warning.get('description', ''))
            
            # å¤©æ°—è­¦å ±ã‚’å®Œå…¨ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆã€é…·ç†±å¤©æ°£è­¦å‘Šãªã©ï¼‰
            if title and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in title and "é…·ç†±å¤©æ°£è­¦å‘Š" not in title and "ç™¼å‡º" not in title:
                weather_section += f"\n### å¤©æ°—è­¦å ±{title}"
                if desc and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in desc and "é…·ç†±å¤©æ°£è­¦å‘Š" not in desc:
                    weather_section += f"{desc}"
        
        # åœ°åŸŸå¤©æ°—äºˆå ±ã®ã¿è¡¨ç¤º
        if 'weather_forecast' in weather_data:
            forecast = weather_data['weather_forecast']
            title = forecast.get('title', 'N/A')
            desc = clean_weather_text(forecast.get('description', ''))
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            translated_title = self._translate_weather_text(title)
            
            # æœ¬æ–‡ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            if desc and len(desc) < 1000:  # é•·ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                translated_desc = self._translate_weather_text(desc)
                
                # æ”¹è¡Œãªã—ã§ä¸€è¡Œã«ã¾ã¨ã‚ã‚‹
                weather_section += f"\n### å¤©æ°—äºˆå ±{translated_title} {translated_desc}*æƒ…å ±æä¾›: é¦™æ¸¯å¤©æ–‡å°*"
            else:
                weather_section += f"\n### å¤©æ°—äºˆå ±{translated_title}*æƒ…å ±æä¾›: é¦™æ¸¯å¤©æ–‡å°*"
        
        return weather_section
    
    def _translate_weather_text(self, text: str) -> str:
        """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸­å›½èªãƒ»åºƒæ±èªã‹ã‚‰æ—¥æœ¬èªã«ç¿»è¨³"""
        if not text or len(text.strip()) == 0:
            return text
        
        try:
            import requests
            import json
            
            # Google Translate APIã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'zh',  # ä¸­å›½èªã‹ã‚‰
                'tl': 'ja',  # æ—¥æœ¬èªã¸
                'dt': 't',
                'q': text
            }
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result and len(result) > 0 and len(result[0]) > 0:
                    translated = ''.join([item[0] for item in result[0] if item[0]])
                    return translated.strip()
            
            return text  # ç¿»è¨³ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
            
        except Exception as e:
            print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            return text  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    
    def remove_duplicate_articles(self, body: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–"""
        import re
        
        # ### ã§å§‹ã¾ã‚‹è¨˜äº‹ã‚’åˆ†å‰²
        articles = re.split(r'\n### ', body)
        
        # æœ€åˆã®è¦ç´ ã¯ç©ºã¾ãŸã¯å¤©æ°—æƒ…å ±ãªã®ã§ãã®ã¾ã¾ä¿æŒ
        if articles:
            result = [articles[0]]
            seen_titles = set()
            duplicate_count = 0
            
            for article in articles[1:]:
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
                lines = article.split('\n', 1)
                if len(lines) > 0:
                    title = lines[0].strip()
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã®æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ï¼‰
                    normalized_title = re.sub(r'[^\w\s]', '', title.lower())
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if normalized_title not in seen_titles and normalized_title:
                        seen_titles.add(normalized_title)
                        result.append(article)
                    else:
                        duplicate_count += 1
            
            if duplicate_count > 0:
                print(f"ğŸ”„ é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–: {duplicate_count}ä»¶")
            
            # å†çµåˆ
            if len(result) > 1:
                return result[0] + '\n### ' + '\n### '.join(result[1:])
            else:
                return result[0]
        
        return body
    
    def save_article(self, article: Dict, weather_data: Dict = None, output_path: str = None) -> str:
        """ç”Ÿæˆã—ãŸè¨˜äº‹ã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        if output_path is None:
            timestamp = datetime.now().strftime('%Y-%m-%d')
            output_path = f"daily-articles/hongkong-news_{timestamp}.md"
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡ã‚’é™¤å¤–
        article['body'] = self.remove_duplicate_articles(article['body'])
        
        # å¤©æ°—æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        weather_section = self.format_weather_info(weather_data) if weather_data else ""
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’çµ„ã¿ç«‹ã¦ï¼ˆç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ”¹è¡Œã‚’æŒŸã¾ãªã„ï¼‰
        content_parts = []
        if weather_section:
            content_parts.append(weather_section)
        if article['lead']:
            content_parts.append(article['lead'])
        content_parts.append(article['body'])
        
        # Markdownç”Ÿæˆ
        content_str = '\n'.join(content_parts)
        markdown = f"""# {article['title']}
{content_str}
---
**ã‚¿ã‚°**: {article['tags']}
**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"ğŸ’¾ è¨˜äº‹ã‚’ä¿å­˜: {output_path}")
        return output_path

def preprocess_news(news_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ"""
    import re
    import os
    from collections import defaultdict
    from datetime import datetime, timedelta
    
    # 0. éå»ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å‡ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    past_urls = set()
    past_titles = []
    
    # éå»3æ—¥åˆ†ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    for days_ago in range(1, 4):
        past_date = datetime.now() - timedelta(days=days_ago)
        past_file = f"daily-articles/hongkong-news_{past_date.strftime('%Y-%m-%d')}.md"
        
        if os.path.exists(past_file):
            print(f"ğŸ“‚ éå»è¨˜äº‹ãƒã‚§ãƒƒã‚¯: {past_file}")
            try:
                with open(past_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # URLã‚’æŠ½å‡ºï¼ˆ**ãƒªãƒ³ã‚¯**: ã®å¾Œã®URLï¼‰
                    url_matches = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://[^\s]+)', content)
                    past_urls.update(url_matches)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆ### ã®å¾Œã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                    title_matches = re.findall(r'^### (.+)$', content, re.MULTILINE)
                    # å¤©æ°—äºˆå ±ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    past_titles.extend([t for t in title_matches if 'å¤©æ°—' not in t and 'weather' not in t.lower()])
                    
                print(f"  âœ“ æ—¢å‡ºURL: {len(url_matches)}ä»¶ã€æ—¢å‡ºã‚¿ã‚¤ãƒˆãƒ«: {len([t for t in title_matches if 'å¤©æ°—' not in t])}ä»¶")
            except Exception as e:
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    if past_urls:
        print(f"ğŸ” éå»è¨˜äº‹ã‹ã‚‰åˆè¨ˆ {len(past_urls)} ä»¶ã®URLã¨ {len(past_titles)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º")
    
    # éå»è¨˜äº‹ã¨ã®é‡è¤‡ã‚’é™¤å¤–
    filtered_news = []
    duplicate_count = 0
    
    for news in news_list:
        url = news.get('url', '')
        title = news.get('title', '')
        
        # URLã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in past_urls:
            duplicate_count += 1
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
        is_similar = False
        for past_title in past_titles:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
            def normalize_title(t):
                t = t.lower()
                t = re.sub(r'[^\w\s]', '', t)
                return set(t.split())
            
            title_words = normalize_title(title)
            past_title_words = normalize_title(past_title)
            
            # å…±é€šå˜èªã‚’ãƒã‚§ãƒƒã‚¯
            common_words = title_words & past_title_words
            
            # 3å˜èªä»¥ä¸Šå…±é€š ã‹ã¤ å…±é€šç‡ãŒ70%ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã¿ãªã™
            if len(common_words) >= 3:
                similarity = len(common_words) / max(len(title_words), len(past_title_words), 1)
                if similarity >= 0.7:
                    is_similar = True
                    break
        
        if is_similar:
            duplicate_count += 1
            continue
        
        filtered_news.append(news)
    
    if duplicate_count > 0:
        print(f"ğŸš« éå»è¨˜äº‹ã¨ã®é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶")
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(news_list)} â†’ {len(filtered_news)}ä»¶")
    
    # 1. ã‚¤ãƒ™ãƒ³ãƒˆç½²åã«ã‚ˆã‚‹é‡è¤‡é™¤å¤–
    unique_news = []
    seen_signatures = set()
    
    for news in filtered_news:
        title = news['title'].lower()
        desc = news.get('description', '').lower()
        text = title + ' ' + desc
        
        # ã‚¤ãƒ™ãƒ³ãƒˆç½²åã‚’ç”Ÿæˆ
        signature = []
        
        # ç«ç½é–¢é€£
        if ('fire' in text or 'ç«ç½' in text or 'ç«' in text) and ('chinachem' in text or 'è¯æ‡‹' in text or 'central' in text or 'ä¸­ç’°' in text):
            signature.append('central_fire')
        
        # ã‚µãƒƒã‚«ãƒ¼é–¢é€£
        if 'liverpool' in text or 'manchester united' in text or 'åˆ©ç‰©æµ¦' in text or 'æ›¼è¯' in text or 'é›™ç´…æœƒ' in text:
            signature.append('football_legends')
        
        # æ¥ŠæŒ¯å¯§é–¢é€£
        if 'æ¥ŠæŒ¯å¯§' in text or 'yang zhenning' in text:
            signature.append('yang_zhenning')
        
        # åŠ‡ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        if 'æˆ‘å€‘æœ€å¿«æ¨‚' in text or 'gay-themed play' in text:
            signature.append('play_cancel')
        
        # ãƒšãƒƒãƒˆç—…é™¢è¨ªå•
        if 'pet' in text and 'hospital' in text:
            signature.append('pet_hospital')
        
        # ç½²åã‚’æ–‡å­—åˆ—åŒ–
        sig_str = '_'.join(sorted(signature)) if signature else None
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if sig_str and sig_str in seen_signatures:
            continue
        
        if sig_str:
            seen_signatures.add(sig_str)
        
        unique_news.append(news)
    
    print(f"ğŸ“Š åŒæ—¥å†…é‡è¤‡é™¤å¤–: {len(filtered_news)} â†’ {len(unique_news)}ä»¶")
    
    # 2. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    categorized = defaultdict(list)
    
    for news in unique_news:
        title = news['title'].lower()
        desc = news.get('description', '').lower()
        text = title + ' ' + desc
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®š
        if any(k in text for k in ['business', 'economy', 'finance', 'market', 'stock', 'trade', 'yuan', 'dollar', 'ç¶“æ¿Ÿ', 'è²¡ç¶“', 'è‚¡', 'è²¿æ˜“']):
            cat = 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ'
        elif any(k in text for k in ['property', 'housing', 'home', 'flat', 'homebuyer', 'æ¨“', 'æˆ¿å±‹', 'ç‰©æ¥­']):
            cat = 'ä¸å‹•ç”£'
        elif any(k in text for k in ['tech', 'technology', 'ai', 'digital', 'robot', 'ç§‘æŠ€', 'æ©Ÿæ¢°äºº']):
            cat = 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif any(k in text for k in ['health', 'medical', 'hospital', 'doctor', 'é†«ç™‚', 'å¥åº·', 'é†«é™¢']):
            cat = 'åŒ»ç™‚ãƒ»å¥åº·'
        elif any(k in text for k in ['education', 'university', 'school', 'student', 'hostel', 'æ•™è‚²', 'å¤§å­¸', 'å­¸ç”Ÿ']):
            cat = 'æ•™è‚²'
        elif any(k in text for k in ['art', 'culture', 'entertainment', 'exhibition', 'drama', 'æ–‡åŒ–', 'è—è¡“', 'å±•è¦½', 'æ¼”å‡º']):
            cat = 'ã‚«ãƒ«ãƒãƒ£ãƒ¼'
        elif any(k in text for k in ['weather', 'storm', 'typhoon', 'å¤©æ°£', 'é¢¨æš´', 'é¢±é¢¨']):
            cat = 'å¤©æ°—'
        elif any(k in text for k in ['traffic', 'transport', 'car', 'road', 'äº¤é€š', 'é“è·¯', 'è»Š']):
            cat = 'äº¤é€š'
        elif any(k in text for k in ['police', 'arrest', 'crime', 'vice', 'è­¦', 'æ‹˜æ•', 'ç½ªæ¡ˆ']):
            cat = 'æ²»å®‰ãƒ»çŠ¯ç½ª'
        elif any(k in text for k in ['fire', 'dead', 'accident', 'ç«ç½', 'æ­»äº¡', 'æ„å¤–']):
            cat = 'äº‹æ•…ãƒ»ç½å®³'
        elif any(k in text for k in ['government', 'policy', 'election', 'official', 'æ”¿åºœ', 'æ”¿ç­–', 'é¸èˆ‰', 'å®˜å“¡']):
            cat = 'æ”¿æ²»ãƒ»è¡Œæ”¿'
        else:
            cat = 'ç¤¾ä¼šãƒ»ãã®ä»–'
        
        categorized[cat].append(news)
    
    print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ä»¶æ•°:")
    for cat, items in sorted(categorized.items(), key=lambda x: -len(x[1])):
        print(f"  {cat}: {len(items)}ä»¶")
    
    # 3. ãƒãƒ©ãƒ³ã‚¹é¸æŠï¼ˆå„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰å‡ç­‰ã«é¸ã¶ï¼‰
    selected = []
    target_count = 30  # 30ä»¶ã«çµã‚‹ï¼ˆAPIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé˜²æ­¢ï¼‰
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã”ã¨ã®å„ªå…ˆé †ä½
    priority_cats = [
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ', 'ä¸å‹•ç”£', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'åŒ»ç™‚ãƒ»å¥åº·', 
        'æ•™è‚²', 'ã‚«ãƒ«ãƒãƒ£ãƒ¼', 'äº¤é€š', 'æ²»å®‰ãƒ»çŠ¯ç½ª', 'äº‹æ•…ãƒ»ç½å®³',
        'æ”¿æ²»ãƒ»è¡Œæ”¿', 'å¤©æ°—', 'ç¤¾ä¼šãƒ»ãã®ä»–'
    ]
    
    # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰é¸æŠ
    while len(selected) < target_count:
        added = False
        for cat in priority_cats:
            if cat in categorized and categorized[cat]:
                selected.append(categorized[cat].pop(0))
                added = True
                if len(selected) >= target_count:
                    break
        if not added:
            break
    
    print(f"\nâœ… é¸æŠå®Œäº†: {len(selected)}ä»¶ï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿ï¼‰")
    
    return selected

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_article.py <raw_news.json>")
        sys.exit(1)
    
    news_file = sys.argv[1]
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹äº‹å‰å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ
    news_data = preprocess_news(data['news'])
    
    print("=" * 60)
    
    generator = GrokArticleGenerator()
    article = generator.generate_article(news_data)
    
    if article:
        # å¤©æ°—æƒ…å ±ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        weather_data = data.get('weather', None)
        saved_path = generator.save_article(article, weather_data)
        print(f"\nâœ… è¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_path}")
        print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
        if weather_data:
            print(f"ğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
