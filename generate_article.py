#!/usr/bin/env python3
"""
é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆåºƒæ±èªå­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
"""

import json
import requests
import re
from datetime import datetime, timedelta, timezone
from typing import List, Dict

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

class GrokArticleGenerator:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # APIé¸æŠï¼ˆGemini â†’ Claude â†’ Grok ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'gemini_api' in self.config and self.config['gemini_api'].get('api_key'):
            self.api_key = self.config['gemini_api']['api_key']
            self.api_url = self.config['gemini_api']['api_url']
            self.use_gemini = True
        elif 'claude_api' in self.config and self.config['claude_api'].get('api_key'):
            self.api_key = self.config['claude_api']['api_key']
            self.api_url = self.config['claude_api']['api_url']
            self.use_gemini = False
        else:
            # Grok APIã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨
            self.api_key = self.config['grok_api']['api_key']
            self.api_url = self.config['grok_api']['api_url']
            self.use_gemini = None
        
    def generate_article(self, news_data: List[Dict]) -> Dict:
        """Gemini/Claude/Grok APIã§æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ"""
        if self.use_gemini is True:
            api_name = "Google Gemini"
        elif self.use_gemini is False:
            api_name = "Claude API"
        else:
            api_name = "Grok API"
        print(f"\nğŸ¤– {api_name}ã§è¨˜äº‹ç”Ÿæˆä¸­...")
        print("=" * 60)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        news_text = self._format_news_for_prompt(news_data)
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯é¦™æ¸¯ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ç¿»è¨³ãƒ«ãƒ¼ãƒ«ï¼š
- ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³
- é¦™æ¸¯ã®åœ°åã€äººåã€çµ„ç¹”åã¯é©åˆ‡ã«ç¿»è¨³
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹
- èª­ã¿ã‚„ã™ã„è¨˜äº‹å½¢å¼ã§æ§‹æˆ

è¨˜äº‹æ§‹æˆï¼š
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’### è¦‹å‡ºã—ã§åŒºåˆ‡ã‚‹
- å†…å®¹ã‚’è©³ã—ãç¿»è¨³
- å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã‚’é©åˆ‡ã«é…ç½®
- åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–

å¼•ç”¨æƒ…å ±ã®å½¢å¼ï¼ˆé‡è¦ï¼‰ï¼š
- å¼•ç”¨å…ƒ: **å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹å
- ãƒªãƒ³ã‚¯: **ãƒªãƒ³ã‚¯**: URL
- å‚™è€ƒ: **å‚™è€ƒ**: èª¬æ˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

é‡è¦ï¼šJSONå½¢å¼ã§ã¯ãªãã€Markdownå½¢å¼ã§è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã¯å¿…ãšåˆ¥ã€…ã®è¡Œã«è¨˜è¼‰ã—ã€**ã§å›²ã‚“ã§ãã ã•ã„ã€‚"""

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        user_prompt = f"""ä»¥ä¸‹ã®é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€è¨˜äº‹ã¨ã—ã¦æ§‹æˆã—ã¦ãã ã•ã„ï¼š

{news_text}

è¨˜äº‹ã®è¦ä»¶ï¼š
1. å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’### è¦‹å‡ºã—ã§åŒºåˆ‡ã‚‹
2. å†…å®¹ã‚’è©³ã—ãç¿»è¨³
3. å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã‚’é©åˆ‡ã«é…ç½®
4. åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–
5. Markdownå½¢å¼ã§å‡ºåŠ›

å¼•ç”¨æƒ…å ±ã®å½¢å¼ï¼ˆé‡è¦ï¼‰ï¼š
- å¼•ç”¨å…ƒ: **å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹åï¼ˆä¾‹ï¼šSCMPã€RTHKç­‰ï¼‰
- ãƒªãƒ³ã‚¯: **ãƒªãƒ³ã‚¯**: URLï¼ˆå®Œå…¨ãªURLï¼‰
- å‚™è€ƒ: **å‚™è€ƒ**: èª¬æ˜ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ€å¾Œã«å¿…ãšå¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã‚’åˆ¥ã€…ã®è¡Œã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š"""

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆGemini/Claude/Grokå¯¾å¿œï¼‰
        if self.use_gemini is True:
            # Gemini API
            headers = {
                "Content-Type": "application/json"
            }
            # APIã‚­ãƒ¼ã‚’URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
            api_url_with_key = f"{self.api_url}?key={self.api_key}"
            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"{system_prompt}\n\n{user_prompt}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 50000
                }
            }
        else:
            # Claude/Grok API
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            if self.use_gemini is False:  # Claude API
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "messages": [
                        {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50000
                }
            else:  # Grok API
                payload = {
                    "model": "grok-beta",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50000
                }
        
        if self.use_gemini is True:
            print("ğŸ“¤ Google Geminiã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        elif self.use_gemini is False:
            print("ğŸ“¤ Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        else:
            print("ğŸ“¤ Grok APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        
        try:
            # Gemini APIã®å ´åˆã¯URLã«APIã‚­ãƒ¼ã‚’è¿½åŠ 
            url = api_url_with_key if self.use_gemini is True else self.api_url
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if self.use_gemini is True:
                    # Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    content = result['candidates'][0]['content']['parts'][0]['text']
                else:
                    # Claude/Grok APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    if self.use_gemini is False:  # Claude API
                        content = result['content'][0]['text']
                    else:  # Grok API
                        content = result['choices'][0]['message']['content']
                
                print("âœ… è¨˜äº‹ç”Ÿæˆå®Œäº†")
                
                # è¨˜äº‹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–
                return self._parse_article_content(content)
                
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                print(f"   è©³ç´°: {response.text}")
                
                # Gemini APIãŒåœ°åŸŸåˆ¶é™ã®å ´åˆã¯Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if (response.status_code == 403 or response.status_code == 400) and self.use_gemini is True:
                    print("ğŸ”„ Gemini APIåœ°åŸŸåˆ¶é™ã®ãŸã‚Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_grok(news_data)
                
                # Grok APIãŒã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®å ´åˆã¯Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if response.status_code == 429 and self.use_gemini is None:
                    print("ğŸ”„ Grok APIã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®ãŸã‚Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_claude(news_data)
                
                return None
                
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")
            return None
    
    def _fallback_to_grok(self, news_data: List[Dict]) -> Dict:
        """Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Grok APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Grok APIã®è¨­å®š
        self.api_key = self.config['grok_api']['api_key']
        self.api_url = self.config['grok_api']['api_url']
        self.use_gemini = None
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _fallback_to_claude(self, news_data: List[Dict]) -> Dict:
        """Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Claude APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Claude APIã®è¨­å®š
        self.api_key = self.config['claude_api']['api_key']
        self.api_url = self.config['claude_api']['api_url']
        self.use_gemini = False
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _parse_article_content(self, content: str) -> Dict:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
        lines = content.split('\n')
        title = lines[0].replace('#', '').strip() if lines else "é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        
        # æœ¬æ–‡ã‚’æŠ½å‡º
        body = content
        
        return {
            "title": title,
            "lead": "",
            "body": body,
            "tags": "é¦™æ¸¯,ãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
        }
    
    def _format_news_for_prompt(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢"""
        formatted = []
        for i, news in enumerate(news_data, 1):
            title = news.get('title', '')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', '')
            published = news.get('published', '')
            
            formatted.append(f"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹ {i}:
ã‚¿ã‚¤ãƒˆãƒ«: {title}
å†…å®¹: {description}
URL: {url}
ã‚½ãƒ¼ã‚¹: {source}
å…¬é–‹æ—¥æ™‚: {published}
""")
        
        return '\n'.join(formatted)
    
    def format_weather_info(self, weather_data: Dict) -> str:
        """å¤©æ°—æƒ…å ±ã‚’Markdownå½¢å¼ã«æ•´å½¢"""
        if not weather_data:
            return ""
        
        import re
        
        def clean_weather_text(text: str) -> str:
            """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            if not text:
                return ""
            # HTMLã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            text = re.sub(r'<br\s*/?>', '\n', text)
            # ä»–ã®HTMLã‚¿ã‚°ã‚’é™¤å»
            text = re.sub(r'<[^>]+>', '', text)
            # å„è¡Œã”ã¨ã«å‡¦ç†
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                # è¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                line = re.sub(r'\s+', ' ', line).strip()
                if line:
                    cleaned_lines.append(line)
            return '\n'.join(cleaned_lines)
        
        weather_section = "## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—\n"
        
        # å¤©æ°—è­¦å ±
        if 'weather_warning' in weather_data:
            warning = weather_data['weather_warning']
            title = warning.get('title', 'N/A')
            desc = clean_weather_text(warning.get('description', ''))
            
            if title and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in title and "é…·ç†±å¤©æ°£è­¦å‘Š" not in title and "ç™¼å‡º" not in title:
                weather_section += f"\n### å¤©æ°—è­¦å ±{title}"
                if desc and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in desc and "é…·ç†±å¤©æ°£è­¦å‘Š" not in desc:
                    weather_section += f"{desc}"
        
        # åœ°åŸŸå¤©æ°—äºˆå ±ã®ã¿è¡¨ç¤º
        if 'weather_forecast' in weather_data:
            forecast = weather_data['weather_forecast']
            title = forecast.get('title', 'N/A')
            desc = clean_weather_text(forecast.get('description', ''))
            
            # å¤©æ°—æƒ…å ±ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            translated_title = self._translate_weather_text(title)
            translated_desc = self._translate_weather_text(desc)
            weather_section += f"\n### å¤©æ°—äºˆå ±\n{translated_title}\n{translated_desc}\n\n**å¼•ç”¨å…ƒ**: é¦™æ¸¯å¤©æ–‡å°"
        
        return weather_section
    
    def _translate_weather_text(self, text: str) -> str:
        """å¤©æ°—æƒ…å ±ã®åºƒæ±èªã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
        if not text:
            return ""
        
        # åºƒæ±èªã®å¤©æ°—æƒ…å ±ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹è¾æ›¸
        weather_translations = {
            "é¦™æ¸¯å¤©æ–‡å°æ–¼": "é¦™æ¸¯å¤©æ–‡å°ãŒ",
            "ç™¼å‡ºä¹‹å¤©æ°£å ±å‘Š": "ç™ºè¡¨ã—ãŸå¤©æ°—å ±å‘Š",
            "ä¸€è‚¡æ¸…å‹çš„åæ±æ°£æµæ­£å½±éŸ¿å»£æ±æ²¿å²¸": "æ¸…æ¶¼ãªæ±é¢¨ãŒåºƒæ±æ²¿å²¸ã«å½±éŸ¿ã—ã¦ã„ã¾ã™",
            "æ­¤å¤–": "ã¾ãŸ",
            "ä¸€é“é›²å¸¶æ­£è¦†è“‹è¯å—æ²¿å²¸": "é›²ãŒè¯å—æ²¿å²¸ã‚’è¦†ã£ã¦ã„ã¾ã™",
            "æœ¬æ¸¯åœ°å€ä»Šæ—¥å¤©æ°£é æ¸¬": "é¦™æ¸¯åœ°åŒºã®ä»Šæ—¥ã®å¤©æ°—äºˆå ±",
            "å¤§è‡´å¤šé›²": "æ¦‚ã­æ›‡ã‚Š",
            "æœ‰ä¸€å…©é™£å¾®é›¨": "æ™‚ã€…å°é›¨",
            "æ—¥é–“çŸ­æš«æ™‚é–“æœ‰é™½å…‰": "æ—¥ä¸­ã¯çŸ­æ™‚é–“æ™´ã‚Œé–“",
            "æœ€é«˜æ°£æº«ç´„": "æœ€é«˜æ°—æ¸©ç´„",
            "åº¦": "åº¦",
            "å¹å’Œç·©è‡³æ¸…å‹æ±è‡³æ±åŒ—é¢¨": "æ±ã‹ã‚‰åŒ—æ±ã®é¢¨ãŒã‚„ã‚„å¼·ãå¹ã",
            "å±•æœ›": "ä»Šå¾Œã®è¦‹é€šã—",
            "æ˜æ—¥æ—¥é–“ç‚ç†±": "æ˜æ—¥ã®æ—¥ä¸­ã¯æš‘ã„",
            "é€±æœ«æœŸé–“æ°£æº«ç¨ç‚ºä¸‹é™": "é€±æœ«ã¯æ°—æ¸©ãŒã‚„ã‚„ä¸‹ãŒã‚Š",
            "å¤©æ°£ä¹¾ç‡¥": "å¤©æ°—ã¯ä¹¾ç‡¥",
            "ä¸‹é€±åˆé¢¨å‹¢é —å¤§": "æ¥é€±åˆã‚ã¯é¢¨ãŒå¼·ã„"
        }
        
        # ç¿»è¨³ã‚’é©ç”¨
        translated_text = text
        for chinese, japanese in weather_translations.items():
            translated_text = translated_text.replace(chinese, japanese)
        
        return translated_text
    
    def _generate_cantonese_section(self) -> str:
        """åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’ç”Ÿæˆ"""
        return """## åºƒæ±èªå­¦ç¿’è€…å‘ã‘æƒ…å ±

åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã«LINEãŒè‰¯ã„ã€ä¾¿åˆ©ã¨ã„ã†æ–¹ã‚‚ã„ã‚‹ã§ã—ã‚‡ã†ã‹ã‚‰ã€ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚‚ã‚ã‚Šã¾ã™ã®ã§ã“ã¡ã‚‰ã”ç™»éŒ²ã—ã¦ã‹ã‚‰ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ã“ã¡ã‚‰ã‚‚LEDã®ãƒãƒ£ãƒƒãƒˆbotå½¢å¼ã§ç§˜æ›¸ã®ãƒªãƒ¼ã•ã‚“ãŒåºƒæ±èªã«ã¤ã„ã¦ãªã‚“ã§ã‚‚å›ç­”ã—ã¦ãã‚Œã¾ã™ã®ã§ãœã²ä½¿ã£ã¦ã¿ã¦ãã ã•ã„

(ä»Šç¾åœ¨400åä»¥ä¸Šã®æ–¹ã«ç™»éŒ²ã—ã¦ã„ãŸã ã„ã¦ãŠã‚Šã¾ã™ï¼‰

[ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](https://line.me/R/ti/p/@298mwivr)
![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-img1.jpg)

[LINEã§ãŠå•åˆã›](https://line.me/R/ti/p/@298mwivr)
![LINEã§ãŠå•åˆã›](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-qr.png)

## åºƒæ±èª| åºƒæ±èªè¶…åŸºç¤ã€€è¶…ç°¡å˜ï¼åˆã‚ã¦ã®åºƒæ±èªã€Œ9å£°6èª¿ã€

@https://youtu.be/RAWZAJUrvOU?si=WafOkQixyLiwMhUW"""
    
    def remove_advertisement_content(self, body: str) -> str:
        """è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šãƒ»å®£ä¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»"""
        import re
        
        # åºƒå‘Šãƒ»å®£ä¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        ad_patterns = [
            r'æœ€æ–°ã®å‹•ç”»ç´¹ä»‹ï¼š.*?ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'TOPick.*?ãƒãƒ£ãƒ³ãƒãƒ«.*?ãƒ•ã‚©ãƒ­ãƒ¼.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ç„¡æ–™ã®.*?ä¼šå“¡.*?ä»Šã™ã.*?ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
            r'ä¼šå“¡æ–°è¦å‹Ÿé›†.*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š',
            r'https://whatsapp\.com/channel/.*?',
            r'https://onelink\.to/.*?',
            r'https://event\.hket\.com/.*?',
            r'ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'ç”³ã—è¾¼ã¿å—ä»˜ä¸­',
            r'ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š.*?',
            r'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š.*?',
            r'ğŸ””.*?ãƒ•ã‚©ãƒ­ãƒ¼',
            r'ç„¡æ–™.*?ä¼šå“¡.*?å‚åŠ ã—ã¾ã—ã‚‡ã†',
            r'æ–°è¦ä¼šå“¡ç™»éŒ².*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ',
            # åºƒå‘Šè¨˜äº‹ã®é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'ã“ã®è¨˜äº‹ã¯åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Š.*?ç¿»è¨³ã—ã¾ã›ã‚“ã€‚',
            r'åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸ.*?åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–',
            r'presented.*?news.*?åºƒå‘Š',
            r'ã‚¹ãƒãƒ³ã‚µãƒ¼è¨˜äº‹',
            r'åºƒå‘Šè¨˜äº‹',
            r'PRè¨˜äº‹',
            r'presented.*?content'
        ]
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆAIãŒè‡ªå‹•ç”Ÿæˆã™ã‚‹ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆï¼‰
        unwanted_patterns = [
            r'### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚',
            r'### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹.*?',
            r'### ä»¥ä¸Š.*?',
            r'### çµ‚äº†.*?',
            r'### è¨˜äº‹ã¯ä»¥ä¸Šã§ã™ã€‚',
            r'### ä»¥ä¸ŠãŒ.*?ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚',
            r'### ä»¥ä¸Šã§.*?ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’çµ‚äº†ã—ã¾ã™ã€‚'
        ]
        
        # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã®è¡¨ç¤ºã‚’ä¿®æ­£ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        fix_patterns = [
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒä¸€è¡Œã«ã¾ã¨ã¾ã£ã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£
            (r'\*\*å¼•ç”¨å…ƒ\*\*:\s*([^*]+)\*\*\*ãƒªãƒ³ã‚¯\*\*:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒ*ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£
            (r'\*å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒ*ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£ï¼ˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            (r'\*å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^\s]+)ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^:]+):\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2')
        ]
        
        # åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»
        cleaned_body = body
        for pattern in ad_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        for pattern in unwanted_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã®è¡¨ç¤ºã‚’ä¿®æ­£
        for pattern, replacement in fix_patterns:
            cleaned_body = re.sub(pattern, replacement, cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«
        cleaned_body = re.sub(r'\n{3,}', '\n\n', cleaned_body)
        
        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºè¡Œã‚’é™¤å»
        cleaned_body = cleaned_body.strip()
        
        return cleaned_body
    
    def remove_duplicate_articles(self, body: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–"""
        import re
        
        # ### ã§å§‹ã¾ã‚‹è¨˜äº‹ã‚’åˆ†å‰²
        articles = re.split(r'\n### ', body)
        
        # æœ€åˆã®è¦ç´ ã¯ç©ºã¾ãŸã¯å¤©æ°—æƒ…å ±ãªã®ã§ãã®ã¾ã¾ä¿æŒ
        if not articles:
            return body
        
        result = [articles[0]]
        seen_titles = set()
        duplicate_count = 0
        
        # å„è¨˜äº‹ã‚’ãƒã‚§ãƒƒã‚¯
        for article in articles[1:]:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
            lines = article.split('\n', 1)
            if len(lines) > 0:
                title = lines[0].strip()
                
                # ã‚¿ã‚¤ãƒˆãƒ«ã®æ­£è¦åŒ–ï¼ˆã‚ˆã‚Šå³å¯†ãªé‡è¤‡ã®ã¿é™¤å¤–ï¼‰
                normalized_title = re.sub(r'[^\w\s]', '', title.lower())
                # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯é‡è¤‡ãƒã‚§ãƒƒã‚¯å¯¾è±¡å¤–
                if len(normalized_title) < 10:
                    result.append(article)
                    continue
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨ä¸€è‡´ã®ã¿ï¼‰
                if normalized_title not in seen_titles:
                    seen_titles.add(normalized_title)
                    result.append(article)
                else:
                    duplicate_count += 1
        
        if duplicate_count > 0:
            print(f"ğŸ”„ é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–: {duplicate_count}ä»¶")
        
        # å†çµåˆï¼ˆè¦‹å‡ºã—ã®å‰ã«ç©ºè¡Œã‚’å…¥ã‚Œã‚‹ï¼‰
        if len(result) > 1:
            return result[0] + '\n\n### ' + '\n\n### '.join(result[1:])
        else:
            return result[0]
    
    def save_article(self, article: Dict, weather_data: Dict = None, output_path: str = None) -> str:
        """ç”Ÿæˆã—ãŸè¨˜äº‹ã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        if output_path is None:
            timestamp = datetime.now(HKT).strftime('%Y-%m-%d')
            output_path = f"daily-articles/hongkong-news_{timestamp}.md"
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨é‡è¤‡ã‚’é™¤å¤–
        article['body'] = self.remove_advertisement_content(article['body'])
        article['body'] = self.remove_duplicate_articles(article['body'])
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åŒºåˆ‡ã‚Šç·šã‚’å‰Šé™¤ã—ã€è¦‹å‡ºã—å‰ã«ç©ºè¡Œã‚’è¿½åŠ 
        import re
        article['body'] = re.sub(r'\n+---\n+', '\n', article['body'])
        article['body'] = re.sub(r'\n{3,}', '\n\n', article['body'])
        # è¦‹å‡ºã—ã®å‰ã«å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
        article['body'] = re.sub(r'([^\n])\n(###)', r'\1\n\n\2', article['body'])
        
        # å¤©æ°—æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        weather_section = self.format_weather_info(weather_data) if weather_data else ""
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’çµ„ã¿ç«‹ã¦ï¼ˆç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ”¹è¡Œã‚’æŒŸã¾ãªã„ï¼‰
        content_parts = []
        if weather_section:
            content_parts.append(weather_section)
        if article['lead']:
            content_parts.append(article['lead'])
        content_parts.append(article['body'])
        
        # Markdownç”Ÿæˆ
        content_str = '\n\n'.join(content_parts)
        
        # åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’è¿½åŠ 
        cantonese_section = self._generate_cantonese_section()
        
        # bodyã®æœ€åˆã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ï¼ˆ1è¡Œç›®ãŒç©ºè¡Œã«ãªã‚Šã€ã“ã“ã«ç›®æ¬¡ã‚’æŒ¿å…¥ï¼‰
        markdown = f"""# {article['title']}

{content_str}

{cantonese_section}
----
**ã‚¿ã‚°**: {article['tags']}
**ç”Ÿæˆæ—¥æ™‚**: {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"ğŸ’¾ è¨˜äº‹ã‚’ä¿å­˜: {output_path}")
        return output_path

def preprocess_news(news_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ"""
    import re
    import os
    from collections import defaultdict
    from datetime import datetime, timedelta
    
    # 0. éå»ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å‡ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    past_urls = set()
    past_titles = []
    
    # éå»3æ—¥åˆ†ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    for days_ago in range(1, 4):
        past_date = datetime.now(HKT) - timedelta(days=days_ago)
        past_file = f"daily-articles/hongkong-news_{past_date.strftime('%Y-%m-%d')}.md"
        
        if os.path.exists(past_file):
            print(f"ğŸ“‚ éå»è¨˜äº‹ãƒã‚§ãƒƒã‚¯: {past_file}")
            try:
                with open(past_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # URLã‚’æŠ½å‡ºï¼ˆ**ãƒªãƒ³ã‚¯**: ã®å¾Œã®URLï¼‰
                    url_matches = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://[^\s]+)', content)
                    past_urls.update(url_matches)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆ### ã®å¾Œã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                    title_matches = re.findall(r'^### (.+)$', content, re.MULTILINE)
                    # å¤©æ°—äºˆå ±ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    past_titles.extend([t for t in title_matches if 'å¤©æ°—' not in t and 'weather' not in t.lower()])
                    
                print(f"  âœ“ æ—¢å‡ºURL: {len(url_matches)}ä»¶ã€æ—¢å‡ºã‚¿ã‚¤ãƒˆãƒ«: {len([t for t in title_matches if 'å¤©æ°—' not in t])}ä»¶")
            except Exception as e:
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    if past_urls:
        print(f"ğŸ” éå»è¨˜äº‹ã‹ã‚‰åˆè¨ˆ {len(past_urls)} ä»¶ã®URLã¨ {len(past_titles)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º")
    
    # éå»è¨˜äº‹ã¨ã®é‡è¤‡ã‚’é™¤å¤–
    filtered_news = []
    duplicate_count = 0
    
    for news in news_list:
        url = news.get('url', '')
        title = news.get('title', '')
        description = news.get('description', '')
        
        # å¤©æ°—é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
        weather_keywords = ['æ°—æ¸©', 'å¤©æ°—', 'å¤©æ–‡å°', 'æ°—è±¡', 'å¤©å€™', 'temperature', 'weather', 'observatory', 'forecast', 'â„ƒ', 'åº¦']
        if any(keyword in title.lower() or keyword in title for keyword in weather_keywords):
            duplicate_count += 1
            continue
        
        # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in past_urls:
            duplicate_count += 1
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ­£è¦åŒ–ï¼‰
        normalized_title = re.sub(r'[^\w\s]', '', title.lower())
        if any(re.sub(r'[^\w\s]', '', past_title.lower()) == normalized_title for past_title in past_titles):
            duplicate_count += 1
            continue
        
        filtered_news.append(news)
    
    if duplicate_count > 0:
        print(f"ğŸš« éå»è¨˜äº‹ã¨ã®é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶")
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(news_list)} â†’ {len(filtered_news)}ä»¶")
    
    # 1. åŒæ—¥å†…é‡è¤‡é™¤å¤–
    seen_titles = set()
    unique_news = []
    same_day_duplicates = 0
    
    for news in filtered_news:
        title = news.get('title', '')
        normalized_title = re.sub(r'[^\w\s]', '', title.lower())
        
        if normalized_title not in seen_titles:
            seen_titles.add(normalized_title)
            unique_news.append(news)
        else:
            same_day_duplicates += 1
    
    if same_day_duplicates > 0:
        print(f"ğŸ“Š åŒæ—¥å†…é‡è¤‡é™¤å¤–: {len(filtered_news)} â†’ {len(unique_news)}ä»¶")
    
    # 2. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    categorized = defaultdict(list)
    
    for news in unique_news:
        title = news.get('title', '').lower()
        description = news.get('description', '').lower()
        content = f"{title} {description}"
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®š
        if any(keyword in content for keyword in ['ãƒ“ã‚¸ãƒã‚¹', 'çµŒæ¸ˆ', 'é‡‘è', 'æ ªå¼', 'æŠ•è³‡', 'business', 'economy', 'finance', 'stock', 'investment', 'ipo', 'ä¸Šå ´', 'å–å¼•æ‰€', 'éŠ€è¡Œ', 'ä¿é™º']):
            category = 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ'
        elif any(keyword in content for keyword in ['ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'ai', 'äººå·¥çŸ¥èƒ½', 'ãƒ­ãƒœãƒƒãƒˆ', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢', 'technology', 'digital', 'app', 'software', 'hardware', 'ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼']):
            category = 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif any(keyword in content for keyword in ['åŒ»ç™‚', 'å¥åº·', 'ç—…é™¢', 'åŒ»å¸«', 'è–¬', 'æ²»ç™‚', 'medical', 'health', 'hospital', 'doctor', 'medicine', 'treatment', 'covid', 'ã‚³ãƒ­ãƒŠ', 'ãƒ¯ã‚¯ãƒãƒ³']):
            category = 'åŒ»ç™‚ãƒ»å¥åº·'
        elif any(keyword in content for keyword in ['æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'å­¦ç”Ÿ', 'æ•™å¸«', 'education', 'school', 'university', 'student', 'teacher', 'å­¦ç¿’', 'ç ”ç©¶']):
            category = 'æ•™è‚²'
        elif any(keyword in content for keyword in ['ä¸å‹•ç”£', 'ä½å®…', 'ãƒãƒ³ã‚·ãƒ§ãƒ³', 'åœŸåœ°', 'è³ƒè²¸', 'real estate', 'property', 'housing', 'apartment', 'rent', 'åœŸåœ°', 'å»ºç‰©']):
            category = 'ä¸å‹•ç”£'
        elif any(keyword in content for keyword in ['äº¤é€š', 'é›»è»Š', 'ãƒã‚¹', 'ã‚¿ã‚¯ã‚·ãƒ¼', 'ç©ºæ¸¯', 'transport', 'train', 'bus', 'taxi', 'airport', 'mtr', 'åœ°ä¸‹é‰„', 'è·¯ç·š']):
            category = 'äº¤é€š'
        elif any(keyword in content for keyword in ['çŠ¯ç½ª', 'é€®æ•', 'è­¦å¯Ÿ', 'è£åˆ¤', 'åˆ‘å‹™æ‰€', 'crime', 'arrest', 'police', 'court', 'prison', 'é•æ³•', 'äº‹ä»¶', 'æœæŸ»']):
            category = 'æ²»å®‰ãƒ»çŠ¯ç½ª'
        elif any(keyword in content for keyword in ['äº‹æ•…', 'ç½å®³', 'ç«äº‹', 'åœ°éœ‡', 'å°é¢¨', 'accident', 'disaster', 'fire', 'earthquake', 'typhoon', 'ç·Šæ€¥', 'æ•‘åŠ©']):
            category = 'äº‹æ•…ãƒ»ç½å®³'
        elif any(keyword in content for keyword in ['æ”¿æ²»', 'æ”¿åºœ', 'è­°å“¡', 'é¸æŒ™', 'æ”¿ç­–', 'politics', 'government', 'minister', 'election', 'policy', 'è¡Œæ”¿', 'è­°ä¼š']):
            category = 'æ”¿æ²»ãƒ»è¡Œæ”¿'
        elif any(keyword in content for keyword in ['æ–‡åŒ–', 'èŠ¸èƒ½', 'ã‚¹ãƒãƒ¼ãƒ„', 'æ˜ ç”»', 'éŸ³æ¥½', 'ã‚¢ãƒ¼ãƒˆ', 'culture', 'entertainment', 'sports', 'movie', 'music', 'art', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ç¥­ã‚Š', 'ä¼çµ±']):
            category = 'ã‚«ãƒ«ãƒãƒ£ãƒ¼'
        else:
            category = 'ç¤¾ä¼šãƒ»ãã®ä»–'
        
        categorized[category].append(news)
    
    print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ä»¶æ•°:")
    for cat, items in sorted(categorized.items(), key=lambda x: -len(x[1])):
        print(f"  {cat}: {len(items)}ä»¶")
    
    # 3. ãƒãƒ©ãƒ³ã‚¹é¸æŠï¼ˆå„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦15-20ä»¶é¸æŠï¼‰
    selected = []
    target_count = 18  # 15-20ä»¶ã«èª¿æ•´ï¼ˆAPIåˆ¶é™ã‚’è€ƒæ…®ï¼‰
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã”ã¨ã®å„ªå…ˆé †ä½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šé †ï¼‰
    priority_cats = [
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ',      # 1ä½: 46ä»¶
        'ç¤¾ä¼šãƒ»ãã®ä»–',        # 2ä½: 19ä»¶  
        'ã‚«ãƒ«ãƒãƒ£ãƒ¼',          # 3ä½: 15ä»¶
        'ä¸å‹•ç”£',             # 4ä½: 13ä»¶
        'æ”¿æ²»ãƒ»è¡Œæ”¿',          # 5ä½: 8ä»¶
        'åŒ»ç™‚ãƒ»å¥åº·',          # 6ä½: 3ä»¶
        'æ²»å®‰ãƒ»çŠ¯ç½ª',          # 7ä½: 6ä»¶
        'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼',        # 8ä½: 76ä»¶
        'äº‹æ•…ãƒ»ç½å®³',          # 9ä½: 1ä»¶
        'äº¤é€š'                # 10ä½: 1ä»¶
    ]
    
    # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰å„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦é¸æŠ
    for cat in priority_cats:
        if cat in categorized and categorized[cat]:
            # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æœ€å¤§ä½•ä»¶å–ã‚‹ã‹ã‚’è¨ˆç®—ï¼ˆAPIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦èª¿æ•´ï¼‰
            if cat == 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ':
                max_count = min(4, len(categorized[cat]))  # 1ä½: 4ä»¶
            elif cat == 'ç¤¾ä¼šãƒ»ãã®ä»–':
                max_count = min(3, len(categorized[cat]))  # 2ä½: 3ä»¶
            elif cat == 'ã‚«ãƒ«ãƒãƒ£ãƒ¼':
                max_count = min(3, len(categorized[cat]))  # 3ä½: 3ä»¶
            elif cat == 'ä¸å‹•ç”£':
                max_count = min(2, len(categorized[cat]))  # 4ä½: 2ä»¶
            elif cat == 'æ”¿æ²»ãƒ»è¡Œæ”¿':
                max_count = min(2, len(categorized[cat]))  # 5ä½: 2ä»¶
            elif cat == 'åŒ»ç™‚ãƒ»å¥åº·':
                max_count = min(2, len(categorized[cat]))  # 6ä½: 2ä»¶
            elif cat == 'æ²»å®‰ãƒ»çŠ¯ç½ª':
                max_count = min(1, len(categorized[cat]))  # 7ä½: 1ä»¶
            elif cat == 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼':
                max_count = min(1, len(categorized[cat]))  # 8ä½: 1ä»¶
            else:
                max_count = min(1, len(categorized[cat]))  # 9-10ä½: 1ä»¶
            
            # é¸æŠ
            for i in range(max_count):
                if categorized[cat] and len(selected) < target_count:
                    selected.append(categorized[cat].pop(0))
            
            if len(selected) >= target_count:
                break
    
    # ã¾ã è¶³ã‚Šãªã„å ´åˆã¯æ®‹ã‚Šã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰è¿½åŠ 
    if len(selected) < target_count:
        for cat in priority_cats:
            if cat in categorized and categorized[cat]:
                while categorized[cat] and len(selected) < target_count:
                    selected.append(categorized[cat].pop(0))
                if len(selected) >= target_count:
                    break
    
    print(f"\nâœ… é¸æŠå®Œäº†: {len(selected)}ä»¶ï¼ˆå„ªå…ˆé †ä½èª¿æ•´æ¸ˆã¿ï¼‰")
    
    # é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³ã‚’è¡¨ç¤º
    selected_categories = defaultdict(int)
    for news in selected:
        category = news.get('category', 'æœªåˆ†é¡')
        selected_categories[category] += 1
    
    print("ğŸ“Š é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³:")
    for cat in priority_cats:
        if cat in selected_categories:
            print(f"  {cat}: {selected_categories[cat]}ä»¶")
    
    return selected

if __name__ == "__main__":
    import sys
    import os
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_article.py <raw_news.json>")
        sys.exit(1)
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’HKTã«è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°HKTï¼‰
    os.environ['TZ'] = os.environ.get('TZ', 'Asia/Hong_Kong')
    
    news_file = sys.argv[1]
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’è¡¨ç¤º
    today = datetime.now(HKT).strftime('%Y-%m-%d')
    print(f"\nğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (HKT): {today}")
    print(f"ğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (æ—¥æœ¬èª): {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    print("=" * 60)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹äº‹å‰å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ
    news_data = preprocess_news(data['news'])
    
    print("=" * 60)
    
    generator = GrokArticleGenerator()
    article = generator.generate_article(news_data)
    
    if article:
        # å¤©æ°—æƒ…å ±ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        weather_data = data.get('weather', None)
        saved_path = generator.save_article(article, weather_data)
        
        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥ä»˜ã‚’ç¢ºèª
        expected_date = datetime.now(HKT).strftime('%Y-%m-%d')
        file_date = saved_path.split('_')[-1].replace('.md', '')
        
        print(f"\nâœ… è¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_path}")
        print(f"ğŸ“… ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜: {file_date}")
        print(f"ğŸ“… æœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜: {expected_date}")
        
        if file_date != expected_date:
            print(f"âš ï¸  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜ãŒæœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜ã¨ä¸€è‡´ã—ã¾ã›ã‚“ï¼")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {file_date}, æœŸå¾…: {expected_date}")
        
        print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
        if weather_data:
            print(f"ğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
