#!/usr/bin/env python3
"""
Grok APIã‚’ä½¿ç”¨ã—ã¦é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ
â€» è¦ç´„ã‚„çŸ­ç¸®ã¯ä¸€åˆ‡è¡Œã‚ãšã€å…ƒã®æƒ…å ±ã‚’ãã®ã¾ã¾ç¿»è¨³ã™ã‚‹
"""

import requests
import json
from datetime import datetime, timedelta, timezone
from typing import List, Dict

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

class GrokArticleGenerator:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # OpenAI GPT-4ä½¿ç”¨ï¼ˆå„ªå…ˆï¼‰
        if 'openai_api' in self.config:
            self.api_key = self.config['openai_api']['api_key']
            self.api_url = self.config['openai_api']['api_url']
            self.use_openai = True
        # Claude APIï¼ˆæ¬¡å€™è£œï¼‰
        elif 'claude_api' in self.config:
            self.api_key = self.config['claude_api']['api_key']
            self.api_url = self.config['claude_api']['api_url']
            self.use_openai = False
        # Grok APIï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        else:
            self.api_key = self.config['grok_api']['api_key']
            self.api_url = self.config['grok_api']['api_url']
            self.use_openai = None
        
    def generate_article(self, news_data: List[Dict]) -> Dict:
        """OpenAI GPT-4/Claude/Grok APIã§æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ"""
        if self.use_openai is True:
            api_name = "OpenAI GPT-4"
        elif self.use_openai is False:
            api_name = "Claude API"
        else:
            api_name = "Grok API"
        print(f"\nğŸ¤– {api_name}ã§è¨˜äº‹ç”Ÿæˆä¸­...")
        print("=" * 60)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        news_text = self._format_news_for_prompt(news_data)
        
        # Grok APIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯é¦™æ¸¯ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ•´å½¢ã™ã‚‹ç¿»è¨³è€…ã§ã™ã€‚

ã€æœ€é‡è¦ã€‘è¦ç´„ã‚„çŸ­ç¸®ã¯çµ¶å¯¾ç¦æ­¢ã€‚å…ƒã®ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹(Full Content)ã‚’ãã®ã¾ã¾å…¨æ–‡æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€è¨˜äº‹æ§‹æˆã€‘
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ä»¥ä¸‹ã®Markdownå½¢å¼ã§è¨˜è¼‰ï¼ˆç•ªå·ãªã—ï¼‰:

### [ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«]

[Full Contentã‚’å…¨æ–‡ç¿»è¨³ã€æ®µè½ã¯ç©ºç™½è¡Œã§åŒºåˆ‡ã‚‹]

**å¼•ç”¨å…ƒ**: [sourceå], [æ—¥ä»˜]  
**ãƒªãƒ³ã‚¯**: [å®Œå…¨ãªURL]  
**å‚™è€ƒ**: [é‡è¦æ€§ã‚„æ—¥æœ¬ã¨ã®é–¢é€£æ€§ã‚’1æ–‡ã§]

---

### [æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«]

[Full Contentã‚’å…¨æ–‡ç¿»è¨³]

**å¼•ç”¨å…ƒ**: [sourceå], [æ—¥ä»˜]  
**ãƒªãƒ³ã‚¯**: [å®Œå…¨ãªURL]  
**å‚™è€ƒ**: [é‡è¦æ€§ã‚„æ—¥æœ¬ã¨ã®é–¢é€£æ€§ã‚’1æ–‡ã§]

ã€å‡ºåŠ›ä¾‹ã€‘
### é¦™æ¸¯ç«‹æ³•ä¼šã€ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢è¦åˆ¶æ³•æ¡ˆå¯æ±º

é¦™æ¸¯ã®ç«‹æ³•ä¼šã¯ã€ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢ã‚µãƒ¼ãƒ“ã‚¹ã‚’è¦åˆ¶ã™ã‚‹æ³•æ¡ˆã‚’å¯æ±ºã—ã¾ã—ãŸã€‚

é‹è¼¸å±€ã®é™³ç¾å®å±€é•·ã¯ã€ã“ã®æ³•æ¡ˆãŒäº¤é€šã‚µãƒ¼ãƒ“ã‚¹ã‚’è¿‘ä»£åŒ–ã™ã‚‹ã¨è¿°ã¹ã¾ã—ãŸã€‚

ãƒ©ã‚¤ãƒ‰ã‚·ã‚§ã‚¢äº‹æ¥­è€…ã¯ãƒ©ã‚¤ã‚»ãƒ³ã‚¹å–å¾—ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

**å¼•ç”¨å…ƒ**: SCMP, 2025å¹´10æœˆ16æ—¥  
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/...  
**å‚™è€ƒ**: äº¤é€šæ”¿ç­–ã®è»¢æ©Ÿã§å³æ™‚æ€§ãŒé«˜ã„ã€‚

---

### å°æ¹¾ã§é¦™æ¸¯äººè¦³å…‰å®¢ãŒå¼·å§¦ã•ã‚Œã‚‹äº‹ä»¶å¾Œã€å…¬è¡†å®‰å…¨ã®æ”¹å–„ã‚’æ±‚ã‚ã‚‹ã‚¢ãƒ‰ãƒœã‚«ã‚·ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—

é¦™æ¸¯ã®è¦³å…‰å®¢ãŒæ˜¼é–“ã€å°åŒ—é§…ã§çŸ¥äººã§ã‚ã‚Šé€ƒäº¡ä¸­ã®ç”·ã«ã‚ˆã£ã¦å¼·å§¦ã•ã‚ŒãŸã¨ã•ã‚Œã‚‹äº‹ä»¶å¾Œã€ã‚¢ãƒ‰ãƒœã‚«ã‚·ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ã¯å…¬è¡†å®‰å…¨ã¨å‚è¦³è€…ã®ä»‹å…¥ã®æ”¹å–„ã‚’æ±‚ã‚ã¦ã„ã¾ã™ã€‚

**å¼•ç”¨å…ƒ**: SCMP, 2025å¹´10æœˆ16æ—¥  
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/...  
**å‚™è€ƒ**: é¦™æ¸¯äººè¦³å…‰å®¢ã®å®‰å…¨ã«é–¢ã‚ã‚‹äº‹ä»¶ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã‚‹ã€‚

ã€é‡è¦ã€‘
- ç•ªå·ï¼ˆ1. 2. ãªã©ï¼‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œ### ã‚¿ã‚¤ãƒˆãƒ«åã€å½¢å¼ã§å°è¦‹å‡ºã—ã«ã™ã‚‹
- ãƒ‹ãƒ¥ãƒ¼ã‚¹é–“ã¯ã€Œ---ã€ã§åŒºåˆ‡ã‚‹
- å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã¯ã€Œ**é …ç›®å**:ã€å½¢å¼ã§å¤ªå­—ã«ã™ã‚‹
- Google Newsã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãƒªãƒ³ã‚¯ï¼ˆnews.google.com/rss/articles/...ï¼‰ã¯ä½¿ç”¨ã—ãªã„ã€‚å…ƒã®ã‚½ãƒ¼ã‚¹ï¼ˆHK01ã€Yahooç­‰ï¼‰ã®å®Ÿãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨

ã€ç¿»è¨³ãƒ«ãƒ¼ãƒ«ã€‘
- åœ°å: å¤©æ°´åœ(ãƒ†ã‚£ãƒ³ãƒ»ã‚·ãƒ¥ã‚¤ãƒ»ãƒ¯ã‚¤)ã€èª¿æ™¯å¶º(ãƒ†ã‚£ã‚¦ãƒ»ã‚±ãƒ³ãƒ»ãƒ¬ãƒ³)ã®ã‚ˆã†ã«æ¼¢å­—+èª­ã¿ä»®å
- äººå: æè‹±è¯(Li Ying-wah)ã®ã‚ˆã†ã«æ¼¢å­—+è‹±èªèª­ã¿
- çµ„ç¹”å: å»‰æ”¿å…¬ç½²(ICAC)ã®ã‚ˆã†ã«æ¼¢å­—+ç•¥ç§°
- æ•°å­—ãƒ»é‡‘é¡: ãã®ã¾ã¾ç¿»è¨³
- å›ºæœ‰åè©: åŸèªã‚’å°Šé‡

ã€çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ã€‘
- Full Contentã®çŸ­ç¸®ãƒ»è¦ç´„ã¯çµ¶å¯¾ç¦æ­¢
- å…ƒã®æƒ…å ±ã‚’ã™ã¹ã¦å«ã‚ã‚‹
- æ–‡å­—æ•°åˆ¶é™ãªã—
- é¦™æ¸¯é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿é¸æŠï¼ˆæœ€å¤§20ä»¶ï¼‰
- é‡è¦åº¦ã®é«˜ã„é †ã«ä¸¦ã¹ã‚‹
- åºƒå‘Šè¨˜äº‹ï¼ˆpresented, sponsoredå«ã‚€URLï¼‰ã¯é™¤å¤–
- **æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é¸ã°ãªã„**ï¼ˆ47äººäº‹ä»¶ã€åˆ‘æœŸæº€äº†ã€æ°‘ä¸»æ´¾ã€ç«‹æ³•ä¼šé¸æŒ™ã€å›½å®¶å®‰å…¨å…¬ç½²ã€å›½å®‰æ³•ãªã©ã¯é™¤å¤–ï¼‰

ã€å‡ºåŠ›å½¢å¼ã€‘
å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚JSONä»¥å¤–ã®æ–‡å­—ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ï¼š
{{
  "title": "æ¯æ—¥AIãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')})",
  "lead": "",
  "body": "ä¸Šè¨˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®Markdownè¨˜äº‹",
  "tags": "é¦™æ¸¯,ãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
}}

ã€é‡è¦ã€‘JSONå½¢å¼ã®ã¿ã§å›ç­”ã—ã€ä»–ã®èª¬æ˜æ–‡ã‚„è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

        user_prompt = f"""ä»¥ä¸‹ã¯{datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')}ã®é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’å…ƒã«ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªè¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€æœ€é‡è¦ã€‘Full Contentã®å†…å®¹ã¯çµ¶å¯¾ã«çŸ­ç¸®ãƒ»è¦ç´„ã›ãšã€å…¨æ–‡ãã®ã¾ã¾æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³å¾¹åº•æŒ‡ç¤ºã€‘
- ä¸­å›½èªãƒ»åºƒæ±èªã®å›ºæœ‰åè©ã‚„åœ°åã¯å¿…ãšã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ã«ç¿»è¨³ã™ã‚‹
- ã€ŒåŸºå­”è‚¯é›…ç†±ã€â†’ã€Œãƒã‚¯ãƒ³ã‚°ãƒ‹ãƒ¤ç†±ã€
- ã€Œé³³å¾³é‚¨ã€â†’ã€Œé³³å¾³é‚¨ã€
- ã€Œè¡›ç”Ÿé˜²è­·ã‚»ãƒ³ã‚¿ãƒ¼ã€â†’ã€Œè¡›ç”Ÿé˜²è­·ã‚»ãƒ³ã‚¿ãƒ¼ã€
- å¼•ç”¨ã‚„ä¼šè©±éƒ¨åˆ†ã®ä¸­å›½èªã‚‚ã™ã¹ã¦æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹
- ã€ã€‘å†…ã®ä¸­å›½èªã‚‚ã™ã¹ã¦ç¿»è¨³ã™ã‚‹
- 1ã¤ã®è¨˜äº‹ã®å†…å®¹ã¯æœ€ä½500æ–‡å­—ä»¥ä¸Šã«ã™ã‚‹
- å…ƒã®æƒ…å ±ã‚’ã™ã¹ã¦å«ã‚ã‚‹
- å…·ä½“çš„ãªå›ºæœ‰åè©ã€æ•°å­—ã€æ—¥ä»˜ã‚’ã™ã¹ã¦å«ã‚ã‚‹

{news_text}

ä¸Šè¨˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é¦™æ¸¯é–¢é€£ã®ã‚‚ã®ã‚’20ä»¶é¸ã³ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚20ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å¿…ãšä½œæˆã—ã¦ãã ã•ã„ã€‚
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã€Œå†…å®¹ã€ã¯å…ƒã®Full Contentã‚’å…¨æ–‡ç¿»è¨³ã—ã¦ãã ã•ã„ï¼ˆçŸ­ç¸®ç¦æ­¢ï¼‰ã€‚

ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šé‡è¤‡ã®æ’é™¤ã€‘
1. **åŒã˜äº‹ä»¶ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã«ã¤ã„ã¦ã€è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ï¼ˆSCMPã€RTHKã€Yahooç­‰ï¼‰ã‹ã‚‰å ±é“ã•ã‚Œã¦ã„ã‚‹å ´åˆã€æœ€ã‚‚è©³ç´°ãª1ã¤ã ã‘ã‚’é¸æŠã—ã¦ãã ã•ã„**
2. **ä¾‹ï¼šã€Œè¯æ‡‹ã‚¿ãƒ¯ãƒ¼ç«ç½ã€ãªã©ã€åŒã˜ç«ç½äº‹ä»¶ã¯1ã¤ã ã‘**
3. å„è¨˜äº‹ã¯ç•°ãªã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ»äº‹ä»¶ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

ã€é‡è¦ã€‘åˆ©ç”¨å¯èƒ½ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰**ç•°ãªã‚‹äº‹ä»¶ãƒ»ãƒˆãƒ”ãƒƒã‚¯**ã‚’å¿…ãš20ä»¶é¸ã³ã€è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚20ä»¶ã‚’ç›®æ¨™ã«ã€ã§ãã‚‹ã ã‘å¤šãã®è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€é™¤å¤–ã™ã¹ãæ”¿æ²»é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘
ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é¸ã°ãªã„ã§ãã ã•ã„ï¼š
- 47äººäº‹ä»¶ã€47 persons caseã€democracy trial
- åˆ‘æœŸæº€äº†ã€prison releaseã€sentence completion
- æ°‘ä¸»æ´¾ã€democratic partyã€pro-democracy
- ç«‹æ³•ä¼šé¸æŒ™ã€legislative council electionã€legco election
- å›½å®¶å®‰å…¨å…¬ç½²ã€national security officeã€å›½å®‰æ³•
- jailed for conspiracy (æ”¿æ²»çš„äº‹ä»¶ã§ã®é€®æ•ãƒ»åˆ¤æ±º)
- 2019 protest related news

ã€å„ªå…ˆçš„ã«é¸ã¶ã¹ããƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘
æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å„ªå…ˆçš„ã«é¸ã‚“ã§ãã ã•ã„ï¼š
- **èŠ¸èƒ½ãƒ»ã‚«ãƒ«ãƒãƒ£ãƒ¼ï¼ˆæœ€å„ªå…ˆï¼‰**: ä¿³å„ªã€æ­Œæ‰‹ã€ãƒ†ãƒ¬ãƒ“ç•ªçµ„ã€ç•ªçµ„çµ‚äº†ã€TVBã€ç”Ÿæ—¥ã€å©šç´—ã€åœæ’­ãªã©
- ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ: ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€IPOã€å¸‚å ´å‹•å‘
- ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼: AIã€ãƒ‡ã‚¸ã‚¿ãƒ«ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
- å¥åº·ãƒ»åŒ»ç™‚: æ„ŸæŸ“ç—‡ã€åŒ»ç™‚æŠ€è¡“
- ã‚¹ãƒãƒ¼ãƒ„
- ä¸å‹•ç”£
- æ•™è‚²
- ç¤¾ä¼šã‚¤ãƒ™ãƒ³ãƒˆ

**é‡è¦ãªé¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:
1. æä¾›ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‹ã‚‰ã€**èŠ¸èƒ½ãƒ»ã‚«ãƒ«ãƒãƒ£ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æœ€ä½3-5ä»¶ã¯å¿…ãšé¸ã‚“ã§ãã ã•ã„**
2. ä¿³å„ªåã€ç•ªçµ„åã€TVBé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ç©æ¥µçš„ã«é¸æŠã—ã¦ãã ã•ã„
3. æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯å®Œå…¨ã«ç„¡è¦–ã—ã¦ãã ã•ã„
4. ãƒ“ã‚¸ãƒã‚¹ã€å¥åº·ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚‚å¤šãé¸ã‚“ã§ãã ã•ã„

ã“ã‚Œã‚‰ã®æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ä¸€åˆ‡å–ã‚Šæ‰±ã‚ãšã€ç‰¹ã«èŠ¸èƒ½ãƒ»ã‚«ãƒ«ãƒãƒ£ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æœ€å„ªå…ˆã«ã€ãƒ“ã‚¸ãƒã‚¹ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã€å¥åº·ã€æ–‡åŒ–ã€ã‚¹ãƒãƒ¼ãƒ„ã€ä¸å‹•ç”£ã€æ•™è‚²ãªã©ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"""

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        if self.use_openai is True:
            # OpenAI GPT-4
            payload = {
                "model": "gpt-4o",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 16000
            }
            print("ğŸ“¤ OpenAI GPT-4ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        elif self.use_openai is False:
            # Claude API
            headers["anthropic-version"] = "2023-06-01"
            payload = {
                "model": "claude-3-5-sonnet-20241022",
                "max_tokens": 16000,
                "system": system_prompt,
                "messages": [
                    {"role": "user", "content": user_prompt}
                ]
            }
            print("ğŸ“¤ Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        else:
            # Grok API
            payload = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "model": "grok-2-latest",
                "stream": False,
                "temperature": 0.1
            }
            print("ğŸ“¤ Grok APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        
        try:
            response = requests.post(
                self.api_url,
                headers=headers,
                json=payload,
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                if self.use_openai is True:
                    content = result['choices'][0]['message']['content']
                elif self.use_openai is False:
                    content = result['content'][0]['text']
                else:
                    content = result['choices'][0]['message']['content']
                print("âœ… è¨˜äº‹ç”Ÿæˆå®Œäº†")
                
                # JSONãƒ‘ãƒ¼ã‚¹
                try:
                    import re
                    
                    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                    if '```json' in content:
                        content = content.split('```json')[1].split('```')[0].strip()
                    elif '```' in content:
                        content = content.split('```')[1].split('```')[0].strip()
                    
                    # JSONã¨ã—ã¦æœ€åˆã® { ã‹ã‚‰æœ€å¾Œã® } ã‚’æŠ½å‡º
                    if '{' in content and '}' in content:
                        start = content.find('{')
                        end = content.rfind('}') + 1
                        json_str = content[start:end]
                        
                        # åˆ¶å¾¡æ–‡å­—ã‚’å®Œå…¨ã«é™¤å»
                        json_str = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\t')
                        
                        article = json.loads(json_str)
                        return article
                    else:
                        raise json.JSONDecodeError("No JSON object found", content, 0)
                        
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                    print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {content[:500]}...")
                    print("   ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡ºä¸­...")
                    
                    # JSONå½¢å¼ã‚’è«¦ã‚ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
                    lines = content.split('\n')
                    title_line = [l for l in lines if 'title' in l.lower() and ':' in l]
                    if title_line:
                        title = title_line[0].split(':', 1)[1].strip().strip('"').strip(',').strip('"')
                    else:
                        title = f"æ¯æ—¥AIã‚¹ãƒ©ãƒ³ã‚°ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')})"
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡º
                    lines = content.split('\n')
                    title_line = [l for l in lines if 'title' in l.lower() and ':' in l]
                    if title_line:
                        title = title_line[0].split(':', 1)[1].strip().strip('"').strip(',').strip('"')
                    else:
                        title = f"æ¯æ—¥AIã‚¹ãƒ©ãƒ³ã‚°ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')})"
                    
                    # bodyã‹ã‚‰Markdownè¨˜äº‹ã‚’æŠ½å‡º
                    body_start = content.find('"body":')
                    if body_start != -1:
                        body_start = content.find('"', body_start + 7) + 1
                        body_end = content.rfind('"')
                        if body_end > body_start:
                            body = content[body_start:body_end]
                            # JSONã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è§£é™¤
                            body = body.replace('\\n', '\n').replace('\\"', '"').replace('\\/', '/')
                        else:
                            body = content
                    else:
                        body = content
                    
                    return {
                        "title": title,
                        "lead": "",
                        "body": body,
                        "tags": "é¦™æ¸¯,ãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
                    }
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                print(f"   è©³ç´°: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")
            return None
    
    def _format_news_for_prompt(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢"""
        formatted = []
        for i, news in enumerate(news_data, 1):  # å…¨ä»¶ä½¿ç”¨
            # full_contentãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°description
            content = news.get('full_content', news.get('description', 'N/A'))
            formatted.append(f"""
ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹{i}ã€‘
Title: {news.get('title', 'N/A')}
Full Content: {content}
Source: {news.get('source', 'N/A')}
URL: {news.get('url', 'N/A')}
Published: {news.get('published_at', 'N/A')}
""")
        return "\n".join(formatted)
    
    def format_weather_info(self, weather_data: Dict) -> str:
        """å¤©æ°—æƒ…å ±ã‚’Markdownå½¢å¼ã«æ•´å½¢"""
        if not weather_data:
            return ""
        
        import re
        
        def clean_weather_text(text: str) -> str:
            """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            if not text:
                return ""
            # HTMLã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            text = re.sub(r'<br\s*/?>', '\n', text)
            # ä»–ã®HTMLã‚¿ã‚°ã‚’é™¤å»
            text = re.sub(r'<[^>]+>', '', text)
            # å„è¡Œã”ã¨ã«å‡¦ç†
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                # è¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                line = re.sub(r'\s+', ' ', line).strip()
                if line:  # ç©ºè¡Œã¯é™¤å¤–
                    cleaned_lines.append(line)
            # é©åˆ‡ãªæ”¹è¡Œã§çµåˆ
            return ' '.join(cleaned_lines)
        
        weather_section = "## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—"
        
        # å¤©æ°—è­¦å ±
        if 'weather_warning' in weather_data:
            warning = weather_data['weather_warning']
            title = warning.get('title', 'N/A')
            desc = clean_weather_text(warning.get('description', ''))
            
            # å¤©æ°—è­¦å ±ã‚’å®Œå…¨ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆã€é…·ç†±å¤©æ°£è­¦å‘Šãªã©ï¼‰
            if title and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in title and "é…·ç†±å¤©æ°£è­¦å‘Š" not in title and "ç™¼å‡º" not in title:
                weather_section += f"\n### å¤©æ°—è­¦å ±{title}"
                if desc and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in desc and "é…·ç†±å¤©æ°£è­¦å‘Š" not in desc:
                    weather_section += f"{desc}"
        
        # åœ°åŸŸå¤©æ°—äºˆå ±ã®ã¿è¡¨ç¤º
        if 'weather_forecast' in weather_data:
            forecast = weather_data['weather_forecast']
            title = forecast.get('title', 'N/A')
            desc = clean_weather_text(forecast.get('description', ''))
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            translated_title = self._translate_weather_text(title)
            
            # æœ¬æ–‡ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            if desc and len(desc) < 1000:  # é•·ã™ãã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                translated_desc = self._translate_weather_text(desc)
                
                # æ”¹è¡Œãªã—ã§ä¸€è¡Œã«ã¾ã¨ã‚ã‚‹
                weather_section += f"\n### å¤©æ°—äºˆå ±\n{translated_title} {translated_desc}\n\n**å¼•ç”¨å…ƒ**: é¦™æ¸¯å¤©æ–‡å°"
            else:
                weather_section += f"\n### å¤©æ°—äºˆå ±\n{translated_title}\n\n**å¼•ç”¨å…ƒ**: é¦™æ¸¯å¤©æ–‡å°"
        
        return weather_section
    
    def _translate_weather_text(self, text: str) -> str:
        """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸­å›½èªãƒ»åºƒæ±èªã‹ã‚‰æ—¥æœ¬èªã«ç¿»è¨³"""
        if not text or len(text.strip()) == 0:
            return text
        
        try:
            import requests
            import json
            
            # Google Translate APIã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³
            url = "https://translate.googleapis.com/translate_a/single"
            params = {
                'client': 'gtx',
                'sl': 'zh',  # ä¸­å›½èªã‹ã‚‰
                'tl': 'ja',  # æ—¥æœ¬èªã¸
                'dt': 't',
                'q': text
            }
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result and len(result) > 0 and len(result[0]) > 0:
                    translated = ''.join([item[0] for item in result[0] if item[0]])
                    return translated.strip()
            
            return text  # ç¿»è¨³ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
            
        except Exception as e:
            print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            return text  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    
    def remove_duplicate_articles(self, body: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–"""
        import re
        
        # ### ã§å§‹ã¾ã‚‹è¨˜äº‹ã‚’åˆ†å‰²
        articles = re.split(r'\n### ', body)
        
        # æœ€åˆã®è¦ç´ ã¯ç©ºã¾ãŸã¯å¤©æ°—æƒ…å ±ãªã®ã§ãã®ã¾ã¾ä¿æŒ
        if articles:
            result = [articles[0]]
            seen_titles = set()
            duplicate_count = 0
            
            for article in articles[1:]:
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
                lines = article.split('\n', 1)
                if len(lines) > 0:
                    title = lines[0].strip()
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã®æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ã€è¨˜å·é™¤å»ï¼‰
                    normalized_title = re.sub(r'[^\w\s]', '', title.lower())
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if normalized_title not in seen_titles and normalized_title:
                        seen_titles.add(normalized_title)
                        result.append(article)
                    else:
                        duplicate_count += 1
            
            if duplicate_count > 0:
                print(f"ğŸ”„ é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–: {duplicate_count}ä»¶")
            
            # å†çµåˆï¼ˆè¦‹å‡ºã—ã®å‰ã«ç©ºè¡Œã‚’å…¥ã‚Œã‚‹ï¼‰
            if len(result) > 1:
                return result[0] + '\n\n### ' + '\n\n### '.join(result[1:])
            else:
                return result[0]
        
        return body
    
    def save_article(self, article: Dict, weather_data: Dict = None, output_path: str = None) -> str:
        """ç”Ÿæˆã—ãŸè¨˜äº‹ã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        if output_path is None:
            timestamp = datetime.now(HKT).strftime('%Y-%m-%d')
            output_path = f"daily-articles/hongkong-news_{timestamp}.md"
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡ã‚’é™¤å¤–
        article['body'] = self.remove_duplicate_articles(article['body'])
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åŒºåˆ‡ã‚Šç·šã‚’å‰Šé™¤ã—ã€è¦‹å‡ºã—å‰ã«ç©ºè¡Œã‚’è¿½åŠ 
        import re
        article['body'] = re.sub(r'\n+---\n+', '\n', article['body'])
        article['body'] = re.sub(r'\n{3,}', '\n\n', article['body'])
        # è¦‹å‡ºã—ã®å‰ã«å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
        article['body'] = re.sub(r'([^\n])\n(###)', r'\1\n\n\2', article['body'])
        
        # å¤©æ°—æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        weather_section = self.format_weather_info(weather_data) if weather_data else ""
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’çµ„ã¿ç«‹ã¦ï¼ˆç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ”¹è¡Œã‚’æŒŸã¾ãªã„ï¼‰
        content_parts = []
        if weather_section:
            content_parts.append(weather_section)
        if article['lead']:
            content_parts.append(article['lead'])
        content_parts.append(article['body'])
        
        # Markdownç”Ÿæˆ
        content_str = '\n\n'.join(content_parts)
        # bodyã®æœ€åˆã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ï¼ˆ1è¡Œç›®ãŒç©ºè¡Œã«ãªã‚Šã€ã“ã“ã«ç›®æ¬¡ã‚’æŒ¿å…¥ï¼‰
        markdown = f"""# {article['title']}

{content_str}
---
**ã‚¿ã‚°**: {article['tags']}
**ç”Ÿæˆæ—¥æ™‚**: {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"ğŸ’¾ è¨˜äº‹ã‚’ä¿å­˜: {output_path}")
        return output_path

def preprocess_news(news_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ"""
    import re
    import os
    from collections import defaultdict
    from datetime import datetime, timedelta
    
    # 0. éå»ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å‡ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    past_urls = set()
    past_titles = []
    
    # éå»3æ—¥åˆ†ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    for days_ago in range(1, 4):
        past_date = datetime.now(HKT) - timedelta(days=days_ago)
        past_file = f"daily-articles/hongkong-news_{past_date.strftime('%Y-%m-%d')}.md"
        
        if os.path.exists(past_file):
            print(f"ğŸ“‚ éå»è¨˜äº‹ãƒã‚§ãƒƒã‚¯: {past_file}")
            try:
                with open(past_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # URLã‚’æŠ½å‡ºï¼ˆ**ãƒªãƒ³ã‚¯**: ã®å¾Œã®URLï¼‰
                    url_matches = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://[^\s]+)', content)
                    past_urls.update(url_matches)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆ### ã®å¾Œã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                    title_matches = re.findall(r'^### (.+)$', content, re.MULTILINE)
                    # å¤©æ°—äºˆå ±ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    past_titles.extend([t for t in title_matches if 'å¤©æ°—' not in t and 'weather' not in t.lower()])
                    
                print(f"  âœ“ æ—¢å‡ºURL: {len(url_matches)}ä»¶ã€æ—¢å‡ºã‚¿ã‚¤ãƒˆãƒ«: {len([t for t in title_matches if 'å¤©æ°—' not in t])}ä»¶")
            except Exception as e:
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    if past_urls:
        print(f"ğŸ” éå»è¨˜äº‹ã‹ã‚‰åˆè¨ˆ {len(past_urls)} ä»¶ã®URLã¨ {len(past_titles)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º")
    
    # éå»è¨˜äº‹ã¨ã®é‡è¤‡ã‚’é™¤å¤–
    filtered_news = []
    duplicate_count = 0
    
    for news in news_list:
        url = news.get('url', '')
        title = news.get('title', '')
        description = news.get('description', '')
        
        # å¤©æ°—é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
        weather_keywords = ['æ°—æ¸©', 'å¤©æ°—', 'å¤©æ–‡å°', 'æ°—è±¡', 'å¤©å€™', 'temperature', 'weather', 'observatory', 'forecast', 'â„ƒ', 'åº¦']
        if any(keyword in title.lower() or keyword in title for keyword in weather_keywords):
            duplicate_count += 1
            continue
        
        # æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
        political_keywords = [
            '47äºº', '47 persons', '47 activists', 'democracy trial',
            'åˆ‘æœŸæº€äº†', 'prison term', 'sentence completion', 'prison release',
            'æ°‘ä¸»æ´¾', 'democratic', 'democrats', 'pro-democracy',
            'ç«‹æ³•ä¼šé¸æŒ™', 'legislative council election', 'legco election',
            'å›½å®¶å®‰å…¨å…¬ç½²', 'national security office', 'nsa', 'nsf', 'national security law',
            'å›½å®‰æ³•', 'å›½å®¶å®‰å…¨æ³•', 'å›½å®‰å…¬ç½²'
        ]
        text_to_check = (title + ' ' + description + ' ' + news.get('full_content', '')).lower()
        if any(keyword.lower() in text_to_check for keyword in political_keywords):
            duplicate_count += 1
            continue
        
        # URLã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in past_urls:
            duplicate_count += 1
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
        is_similar = False
        for past_title in past_titles:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
            def normalize_title(t):
                t = t.lower()
                t = re.sub(r'[^\w\s]', '', t)
                return set(t.split())
            
            title_words = normalize_title(title)
            past_title_words = normalize_title(past_title)
            
            # å…±é€šå˜èªã‚’ãƒã‚§ãƒƒã‚¯
            common_words = title_words & past_title_words
            
            # 3å˜èªä»¥ä¸Šå…±é€š ã‹ã¤ å…±é€šç‡ãŒ70%ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã¿ãªã™
            if len(common_words) >= 3:
                similarity = len(common_words) / max(len(title_words), len(past_title_words), 1)
                if similarity >= 0.7:
                    is_similar = True
                    break
        
        if is_similar:
            duplicate_count += 1
            continue
        
        filtered_news.append(news)
    
    if duplicate_count > 0:
        print(f"ğŸš« éå»è¨˜äº‹ã¨ã®é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶")
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(news_list)} â†’ {len(filtered_news)}ä»¶")
    
    # 1. ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã«ã‚ˆã‚‹é‡è¤‡é™¤å¤–ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    unique_news = []
    seen_titles = []
    
    for news in filtered_news:
        title = news['title']
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å˜èªã«åˆ†å‰²ã—ã¦æ­£è¦åŒ–
        def extract_keywords(text):
            # è¨˜å·ã‚’é™¤å»ã—ã¦å˜èªã‚’æŠ½å‡º
            words = re.sub(r'[^\w\s]', ' ', text).split()
            # 2æ–‡å­—ä»¥ä¸Šã®å˜èªã®ã¿ï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤ãï¼‰
            stop_words = {'ã®', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã¨', 'ã§', 'ã‚„', 'ã‚‚', 'ã‹ã‚‰', 'ã¾ã§', 'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or'}
            keywords = [w.lower() for w in words if len(w) >= 2 and w.lower() not in stop_words]
            return set(keywords)
        
        current_keywords = extract_keywords(title)
        
        # æ—¢å­˜ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¯”è¼ƒ
        is_duplicate = False
        for seen_title in seen_titles:
            seen_keywords = extract_keywords(seen_title)
            
            # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰²åˆã‚’è¨ˆç®—
            if len(current_keywords) > 0 and len(seen_keywords) > 0:
                common = current_keywords & seen_keywords
                
                # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰ã‚’é‡è¦–
                current_major = {k for k in current_keywords if len(k) >= 3}
                seen_major = {k for k in seen_keywords if len(k) >= 3}
                common_major = current_major & seen_major
                
                # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ2ã¤ä»¥ä¸Šä¸€è‡´ã€ã‹ã¤å…¨ä½“ã®é¡ä¼¼åº¦ãŒ70%ä»¥ä¸Šãªã‚‰é‡è¤‡
                if len(common_major) >= 2:
                    similarity = len(common) / min(len(current_keywords), len(seen_keywords))
                    if similarity >= 0.7:
                        is_duplicate = True
                        break
        
        if not is_duplicate:
            seen_titles.append(title)
            unique_news.append(news)
    
    print(f"ğŸ“Š åŒæ—¥å†…é‡è¤‡é™¤å¤–: {len(filtered_news)} â†’ {len(unique_news)}ä»¶")
    
    # 2. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    categorized = defaultdict(list)
    
    for news in unique_news:
        title = news['title'].lower()
        desc = news.get('description', '').lower()
        text = title + ' ' + desc
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®š
        if any(k in text for k in ['business', 'economy', 'finance', 'market', 'stock', 'trade', 'yuan', 'dollar', 'ç¶“æ¿Ÿ', 'è²¡ç¶“', 'è‚¡', 'è²¿æ˜“']):
            cat = 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ'
        elif any(k in text for k in ['property', 'housing', 'home', 'flat', 'homebuyer', 'æ¨“', 'æˆ¿å±‹', 'ç‰©æ¥­']):
            cat = 'ä¸å‹•ç”£'
        elif any(k in text for k in ['tech', 'technology', 'ai', 'digital', 'robot', 'ç§‘æŠ€', 'æ©Ÿæ¢°äºº']):
            cat = 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif any(k in text for k in ['health', 'medical', 'hospital', 'doctor', 'é†«ç™‚', 'å¥åº·', 'é†«é™¢']):
            cat = 'åŒ»ç™‚ãƒ»å¥åº·'
        elif any(k in text for k in ['education', 'university', 'school', 'student', 'hostel', 'æ•™è‚²', 'å¤§å­¸', 'å­¸ç”Ÿ']):
            cat = 'æ•™è‚²'
        elif any(k in text for k in ['art', 'culture', 'entertainment', 'exhibition', 'drama', 'æ–‡åŒ–', 'è—è¡“', 'å±•è¦½', 'æ¼”å‡º', 
                                      'tvb', 'ç”Ÿæ—¥', 'å©šç´—', 'åœæ’­', 'ç¯€ç›®', 'é›»è¦–', 'æ˜æ˜Ÿ', 'æ¼”å“¡', 'ç”Ÿæ—¥', 'å°ç›¤']):
            cat = 'ã‚«ãƒ«ãƒãƒ£ãƒ¼'
        elif any(k in text for k in ['weather', 'storm', 'typhoon', 'å¤©æ°£', 'é¢¨æš´', 'é¢±é¢¨']):
            cat = 'å¤©æ°—'
        elif any(k in text for k in ['traffic', 'transport', 'car', 'road', 'äº¤é€š', 'é“è·¯', 'è»Š']):
            cat = 'äº¤é€š'
        elif any(k in text for k in ['police', 'arrest', 'crime', 'vice', 'è­¦', 'æ‹˜æ•', 'ç½ªæ¡ˆ']):
            cat = 'æ²»å®‰ãƒ»çŠ¯ç½ª'
        elif any(k in text for k in ['fire', 'dead', 'accident', 'ç«ç½', 'æ­»äº¡', 'æ„å¤–']):
            cat = 'äº‹æ•…ãƒ»ç½å®³'
        elif any(k in text for k in ['government', 'policy', 'election', 'official', 'æ”¿åºœ', 'æ”¿ç­–', 'é¸èˆ‰', 'å®˜å“¡']):
            cat = 'æ”¿æ²»ãƒ»è¡Œæ”¿'
        else:
            cat = 'ç¤¾ä¼šãƒ»ãã®ä»–'
        
        categorized[cat].append(news)
    
    print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ä»¶æ•°:")
    for cat, items in sorted(categorized.items(), key=lambda x: -len(x[1])):
        print(f"  {cat}: {len(items)}ä»¶")
    
    # 3. ãƒãƒ©ãƒ³ã‚¹é¸æŠï¼ˆå„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰å‡ç­‰ã«é¸ã¶ï¼‰
    selected = []
    target_count = 30  # 30ä»¶ã«çµã‚‹ï¼ˆAPIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆé˜²æ­¢ï¼‰
    
    # ã‚«ãƒ«ãƒãƒ£ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å„ªå…ˆçš„ã«5ä»¶é¸ã¶
    if 'ã‚«ãƒ«ãƒãƒ£ãƒ¼' in categorized and len(categorized['ã‚«ãƒ«ãƒãƒ£ãƒ¼']) > 0:
        culture_count = min(5, len(categorized['ã‚«ãƒ«ãƒãƒ£ãƒ¼']))
        for i in range(culture_count):
            selected.append(categorized['ã‚«ãƒ«ãƒãƒ£ãƒ¼'].pop(0))
        print(f"ğŸ­ ã‚«ãƒ«ãƒãƒ£ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ {culture_count}ä»¶ã‚’å„ªå…ˆé¸æŠ")
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã”ã¨ã®å„ªå…ˆé †ä½
    priority_cats = [
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'åŒ»ç™‚ãƒ»å¥åº·', 
        'æ•™è‚²', 'ä¸å‹•ç”£', 'äº¤é€š', 'æ²»å®‰ãƒ»çŠ¯ç½ª', 'äº‹æ•…ãƒ»ç½å®³',
        'ç¤¾ä¼šãƒ»ãã®ä»–', 'å¤©æ°—', 'æ”¿æ²»ãƒ»è¡Œæ”¿'
    ]
    
    # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰é¸æŠ
    while len(selected) < target_count:
        added = False
        for cat in priority_cats:
            if cat in categorized and categorized[cat]:
                selected.append(categorized[cat].pop(0))
                added = True
                if len(selected) >= target_count:
                    break
        if not added:
            break
    
    print(f"\nâœ… é¸æŠå®Œäº†: {len(selected)}ä»¶ï¼ˆãƒãƒ©ãƒ³ã‚¹èª¿æ•´æ¸ˆã¿ï¼‰")
    
    return selected

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_article.py <raw_news.json>")
        sys.exit(1)
    
    news_file = sys.argv[1]
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹äº‹å‰å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ
    news_data = preprocess_news(data['news'])
    
    print("=" * 60)
    
    generator = GrokArticleGenerator()
    article = generator.generate_article(news_data)
    
    if article:
        # å¤©æ°—æƒ…å ±ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        weather_data = data.get('weather', None)
        saved_path = generator.save_article(article, weather_data)
        print(f"\nâœ… è¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_path}")
        print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
        if weather_data:
            print(f"ğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
