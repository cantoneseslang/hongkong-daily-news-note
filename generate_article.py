#!/usr/bin/env python3
"""
é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆåºƒæ±èªå­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
"""

import json
import os
import requests
import re
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Set
from urllib.parse import urlparse, urlunparse

try:
    from dateutil import parser as dateutil_parser
except ImportError:  # pragma: no cover - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
    dateutil_parser = None

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))


def normalize_title_words(title: str) -> Set[str]:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’å˜èªé›†åˆã«æ­£è¦åŒ–"""
    if not title:
        return set()
    normalized = re.sub(r'[^\w\s]', ' ', title.lower())
    words = {w for w in normalized.split() if len(w) > 1 or w.isdigit()}
    return words


def normalize_title_chars(title: str) -> str:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ–‡å­—å˜ä½ã§æ­£è¦åŒ–ï¼ˆCJKé‡è¤‡æ¤œå‡ºç”¨ï¼‰"""
    if not title:
        return ""
    cleaned = re.sub(r'\s+', '', title.lower())
    cleaned = re.sub(r'[^\w\u4e00-\u9fff]', '', cleaned)
    return cleaned


def title_similarity_chars(a: str, b: str) -> float:
    """æ–‡å­—åˆ—ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦ï¼ˆSequenceMatcherï¼‰"""
    if not a or not b:
        return 0.0
    from difflib import SequenceMatcher
    return SequenceMatcher(None, a, b).ratio()


def titles_are_similar(
    words_a: Set[str],
    words_b: Set[str],
    *,
    min_common: int = 2,
    min_similarity: float = 0.5,
    min_coverage: float = 0.6
) -> bool:
    """2ã¤ã®ã‚¿ã‚¤ãƒˆãƒ«èªé›†åˆãŒååˆ†ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤å®š"""
    if not words_a or not words_b:
        return False

    shortest_len = min(len(words_a), len(words_b))
    dynamic_min_common = min_common
    if shortest_len <= 4:
        dynamic_min_common = max(2, shortest_len)

    common_words = words_a & words_b
    if len(common_words) < dynamic_min_common:
        return False

    all_words = words_a | words_b
    similarity = len(common_words) / len(all_words) if all_words else 0.0
    coverage = len(common_words) / shortest_len if shortest_len else 0.0

    return similarity >= min_similarity and coverage >= min_coverage


def is_similar_title_words(
    words: Set[str],
    existing_word_sets: List[Set[str]],
    *,
    min_common: int = 2,
    min_similarity: float = 0.5,
    min_coverage: float = 0.6
) -> bool:
    """æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«é›†åˆã¨ã®é¡ä¼¼åˆ¤å®š"""
    for existing in existing_word_sets:
        if titles_are_similar(
            words,
            existing,
            min_common=min_common,
            min_similarity=min_similarity,
            min_coverage=min_coverage,
        ):
            return True
    return False


def normalize_url(url: str) -> str:
    """URLã‚’æ­£è¦åŒ–ï¼ˆã‚¹ã‚­ãƒ¼ãƒ /ãƒ›ã‚¹ãƒˆ/ãƒ‘ã‚¹ã®ã¿ï¼‰"""
    if not url:
        return ""
    try:
        parsed = urlparse(url)
        normalized = urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", "", ""))
        return normalized
    except Exception:
        return url


TOPIC_STOPWORDS = {
    'é¦™æ¸¯', 'æ”¿åºœ', 'é¦™æ¸¯æ”¿åºœ', 'é¦™æ¸¯è­¦å¯Ÿ', 'æ”¿åºœæœ¬éƒ¨', 'æœ€æ–°', 'é€Ÿå ±', 'å¤©æ°—', 'å¤©æ°£',
    'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'å ±é“', 'ä¸­å›½', 'ç‰¹åŒºæ”¿åºœ', 'è¡Œæ”¿é•·å®˜', 'ç«‹æ³•ä¼š', 'é¦™æ¸¯ç«‹æ³•ä¼š',
    'ç«ç½', 'ç«äº‹', 'äº‹æ•…', 'é¦™æ¸¯é›»å°', 'é¦™æ¸¯é›»å°ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'é¦™æ¸¯å¤©æ–‡å°'
}


def derive_topic_key(title: str) -> Optional[str]:
    """ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯èªã‚’æŠ½å‡ºï¼ˆCJKå„ªå…ˆï¼‰"""
    if not title:
        return None

    cjk_matches = re.findall(r'[\u4e00-\u9fff]{2,}', title)
    candidates = []
    for match in cjk_matches:
        if len(match) < 2:
            continue
        if any(sw == match for sw in TOPIC_STOPWORDS):
            continue
        if all(sw not in match for sw in TOPIC_STOPWORDS):
            candidates.append(match)
        else:
            # stopwordã‚’é™¤ã„ãŸéƒ¨åˆ†ã‚’å€™è£œã«ã™ã‚‹
            cleaned = match
            for sw in TOPIC_STOPWORDS:
                cleaned = cleaned.replace(sw, '')
            cleaned = cleaned.strip()
            if len(cleaned) >= 2 and cleaned not in TOPIC_STOPWORDS:
                candidates.append(cleaned)

    if candidates:
        # æœ€é•·æ–‡å­—åˆ—ã‚’å„ªå…ˆ
        return max(candidates, key=len)

    # è‹±èªç³»ã®å›ºæœ‰åè©ã‚’æŠ½å‡º
    english_candidates = re.findall(r'[A-Za-z][A-Za-z\s\-\']{3,}', title)
    english_candidates = [
        re.sub(r'\s+', ' ', cand.strip()).lower()
        for cand in english_candidates
        if cand.strip().lower() not in TOPIC_STOPWORDS
    ]
    if english_candidates:
        return max(english_candidates, key=len)

    return None


def get_recent_topics(days: int = 3) -> Dict[str, int]:
    """éå»Næ—¥é–“ã®é »å‡ºãƒˆãƒ”ãƒƒã‚¯ã‚’å–å¾—"""
    try:
        from collections import Counter
        
        topic_counter = Counter()
        today = datetime.now(HKT)
        
        # ãƒˆãƒ”ãƒƒã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        topic_patterns = {
            'å…¨å›½é‹å‹•ä¼š': [r'å…¨å›½é‹å‹•ä¼š|National Games|å…¨é‹æœƒ|NG|national games'],
            'ç«‹æ³•ä¼šé¸æŒ™': [r'ç«‹æ³•ä¼šé¸æŒ™|LegCo election|ç«‹æ³•æœƒé¸èˆ‰|district council'],
            'æ–½æ”¿å ±å‘Š': [r'æ–½æ”¿å ±å‘Š|Policy Address|policy address'],
            'å¤±æ¥­ç‡': [r'å¤±æ¥­ç‡|unemployment rate|jobless'],
            'GDP': [r'GDP|çµŒæ¸ˆæˆé•·|economic growth|gross domestic'],
            'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯': [r'ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯|Olympics|olympic'],
            'å°é¢¨': [r'å°é¢¨|Typhoon|typhoon|tropical storm'],
            'ä¸å‹•ç”£ä¾¡æ ¼': [r'ä¸å‹•ç”£ä¾¡æ ¼|property prices|housing prices|home prices'],
        }
        
        for i in range(1, days + 1):
            target_date = (today - timedelta(days=i)).strftime('%Y-%m-%d')
            article_file = f'daily-articles/hongkong-news_{target_date}.md'
            
            if os.path.exists(article_file):
                try:
                    with open(article_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    for topic, patterns in topic_patterns.items():
                        for pattern in patterns:
                            try:
                                if re.search(pattern, content, re.IGNORECASE):
                                    topic_counter[topic] += 1
                                    break  # 1è¨˜äº‹ã«ã¤ã1å›ã‚«ã‚¦ãƒ³ãƒˆ
                            except Exception:
                                pass
                except Exception:
                    pass
        
        return dict(topic_counter)
    except Exception:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºã®è¾æ›¸ã‚’è¿”ã™
        return {}


def is_overused_topic(title: str, description: str, recent_topics: Dict[str, int], threshold: int = 2) -> bool:
    """ã‚¿ã‚¤ãƒˆãƒ«/èª¬æ˜ãŒéå»Næ—¥é–“ã§é »å‡ºãƒˆãƒ”ãƒƒã‚¯ï¼ˆ2å›ä»¥ä¸Šï¼‰ã«è©²å½“ã™ã‚‹ã‹"""
    try:
        if not recent_topics:
            return False
        
        content = f"{title} {description}".lower()
        
        for topic, count in recent_topics.items():
            if count >= threshold:  # éå»3æ—¥é–“ã§2å›ä»¥ä¸Šå‡ºç¾
                # ãƒˆãƒ”ãƒƒã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                try:
                    if topic == 'å…¨å›½é‹å‹•ä¼š':
                        if re.search(r'å…¨å›½é‹å‹•ä¼š|national games|å…¨é‹æœƒ|ng\s', content, re.IGNORECASE):
                            return True
                    elif topic == 'ç«‹æ³•ä¼šé¸æŒ™':
                        if re.search(r'ç«‹æ³•ä¼šé¸æŒ™|legco election|ç«‹æ³•æœƒé¸èˆ‰|district council', content, re.IGNORECASE):
                            return True
                    elif topic == 'æ–½æ”¿å ±å‘Š':
                        if re.search(r'æ–½æ”¿å ±å‘Š|policy address', content, re.IGNORECASE):
                            return True
                    # ãã®ä»–ã®ãƒˆãƒ”ãƒƒã‚¯ã¯å³ã—ããƒã‚§ãƒƒã‚¯ã—ãªã„ï¼ˆçµŒæ¸ˆæŒ‡æ¨™ç­‰ã¯é‡è¦ï¼‰
                except Exception:
                    pass
        
        return False
    except Exception:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯éå‰°ãƒˆãƒ”ãƒƒã‚¯ã§ã¯ãªã„ã¨åˆ¤å®š
        return False


def parse_published_at(value: Optional[str]) -> Optional[datetime]:
    """å…¬é–‹æ—¥æ™‚æ–‡å­—åˆ—ã‚’HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã®datetimeã«å¤‰æ›"""
    if not value:
        return None

    # æ—¢ã«ISOå½¢å¼ã®å ´åˆã‚’æƒ³å®š
    try:
        if dateutil_parser:
            dt = dateutil_parser.parse(value)
        else:
            dt = datetime.fromisoformat(value)
    except Exception:
        return None

    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)

    try:
        return dt.astimezone(HKT)
    except Exception:
        return dt

class GrokArticleGenerator:
    def __init__(self, config_path: str = "config.json"):
        self.config_path = config_path
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # APIé¸æŠï¼ˆGemini â†’ Claude â†’ Grok ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'gemini_api' in self.config and self.config['gemini_api'].get('api_key'):
            self.api_key = self.config['gemini_api']['api_key']
            self.api_url = self.config['gemini_api']['api_url']
            self.use_gemini = True
        elif 'claude_api' in self.config and self.config['claude_api'].get('api_key'):
            self.api_key = self.config['claude_api']['api_key']
            self.api_url = self.config['claude_api']['api_url']
            self.use_gemini = False
        else:
            # Grok APIã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨
            self.api_key = self.config['grok_api']['api_key']
            self.api_url = self.config['grok_api']['api_url']
            self.use_gemini = None
        
        self.grok_model = (
            self.config.get('grok_api', {}).get('model')
            or os.environ.get('GROK_MODEL')
            or 'grok-3'
        )
        
    def generate_article(self, news_data: List[Dict]) -> Dict:
        """Gemini/Claude/Grok APIã§æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ"""
        if self.use_gemini is True:
            api_name = "Google Gemini"
        elif self.use_gemini is False:
            api_name = "Claude API"
        else:
            api_name = "Grok API"
        print(f"\nğŸ¤– {api_name}ã§è¨˜äº‹ç”Ÿæˆä¸­...")
        print("=" * 60)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        news_text = self._format_news_for_prompt(news_data)
        
        # ä»Šæ—¥ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆHKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼‰
        today_date = datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')
        today_date_iso = datetime.now(HKT).strftime('%Y-%m-%d')
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ã‚ãªãŸã¯é¦™æ¸¯ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ç¿»è¨³ãƒ«ãƒ¼ãƒ«ï¼š
- ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³
- é¦™æ¸¯ã®åœ°åã€äººåã€çµ„ç¹”åã¯é©åˆ‡ã«ç¿»è¨³
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ã‚’æ­£ç¢ºã«ä¼ãˆã‚‹
- èª­ã¿ã‚„ã™ã„è¨˜äº‹å½¢å¼ã§æ§‹æˆ

è¨˜äº‹æ§‹æˆï¼š
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’### è¦‹å‡ºã—ã§åŒºåˆ‡ã‚‹
- å†…å®¹ã‚’è©³ã—ãç¿»è¨³
- å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã‚’é©åˆ‡ã«é…ç½®
- åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–

å¼•ç”¨æƒ…å ±ã®å½¢å¼ï¼ˆé‡è¦ï¼‰ï¼š
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ€å¾Œã«å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼š

**å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹åï¼ˆä¾‹ï¼šSCMPã€RTHKã€é¦™æ¸¯01ç­‰ï¼‰
**ãƒªãƒ³ã‚¯**: å®Œå…¨ãªURL

ä¾‹ï¼š
**å¼•ç”¨å…ƒ**: SCMP
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/news/hong-kong/law-and-crime/article/3330816/hong-kongs-scameter-app-gets-upgrade-ai-tools-tackle-social-media-scams

é‡è¦ï¼šJSONå½¢å¼ã§ã¯ãªãã€Markdownå½¢å¼ã§è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã¯å¿…ãšåˆ¥ã€…ã®è¡Œã«è¨˜è¼‰ã—ã€**ã§å›²ã‚“ã§ãã ã•ã„ã€‚"""

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        user_prompt = f"""ä»¥ä¸‹ã®é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€è¨˜äº‹ã¨ã—ã¦æ§‹æˆã—ã¦ãã ã•ã„ï¼š

ã€é‡è¦ã€‘ä»Šæ—¥ã®æ—¥ä»˜ã¯ {today_date}ï¼ˆ{today_date_iso}ï¼‰ã§ã™ã€‚ã‚¿ã‚¤ãƒˆãƒ«ã«ã¯å¿…ãšã€Œæ¯æ—¥AIé¦™æ¸¯ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({today_date})ã€ã¨ã„ã†å½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

{news_text}

è¨˜äº‹ã®è¦ä»¶ï¼š
1. ã‚¿ã‚¤ãƒˆãƒ«ã¯å¿…ãšã€Œ# æ¯æ—¥AIé¦™æ¸¯ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({today_date})ã€ã¨ã„ã†å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„
2. å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’### è¦‹å‡ºã—ã§åŒºåˆ‡ã‚‹
3. å†…å®¹ã‚’è©³ã—ãç¿»è¨³
4. å¼•ç”¨å…ƒã€ãƒªãƒ³ã‚¯ã€å‚™è€ƒã‚’é©åˆ‡ã«é…ç½®
5. åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–
6. Markdownå½¢å¼ã§å‡ºåŠ›

å¼•ç”¨æƒ…å ±ã®å½¢å¼ï¼ˆé‡è¦ï¼‰ï¼š
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æœ€å¾Œã«å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§è¨˜è¼‰ã—ã¦ãã ã•ã„ï¼š

**å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹åï¼ˆä¾‹ï¼šSCMPã€RTHKã€é¦™æ¸¯01ç­‰ï¼‰
**ãƒªãƒ³ã‚¯**: å®Œå…¨ãªURL

ä¾‹ï¼š
**å¼•ç”¨å…ƒ**: SCMP
**ãƒªãƒ³ã‚¯**: https://www.scmp.com/news/hong-kong/law-and-crime/article/3330816/hong-kongs-scameter-app-gets-upgrade-ai-tools-tackle-social-media-scams

é‡è¦ï¼šå¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã¯å¿…ãšåˆ¥ã€…ã®è¡Œã«è¨˜è¼‰ã—ã€**ã§å›²ã‚“ã§ãã ã•ã„ã€‚

è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š"""

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆGemini/Claude/Grokå¯¾å¿œï¼‰
        if self.use_gemini is True:
            # Gemini API
            headers = {
                "Content-Type": "application/json"
            }
            # APIã‚­ãƒ¼ã‚’URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
            api_url_with_key = f"{self.api_url}?key={self.api_key}"
            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"{system_prompt}\n\n{user_prompt}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 50000
                }
            }
        else:
            # Claude/Grok API
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            if self.use_gemini is False:  # Claude API
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "messages": [
                        {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50000
                }
            else:  # Grok API
                payload = {
                    "model": self.grok_model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 50000
                }
        
        if self.use_gemini is True:
            print("ğŸ“¤ Google Geminiã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        elif self.use_gemini is False:
            print("ğŸ“¤ Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        else:
            print("ğŸ“¤ Grok APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        
        try:
            # Gemini APIã®å ´åˆã¯URLã«APIã‚­ãƒ¼ã‚’è¿½åŠ 
            url = api_url_with_key if self.use_gemini is True else self.api_url
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if self.use_gemini is True:
                    # Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    content = result['candidates'][0]['content']['parts'][0]['text']
                else:
                    # Claude/Grok APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    if self.use_gemini is False:  # Claude API
                        content = result['content'][0]['text']
                    else:  # Grok API
                        content = result['choices'][0]['message']['content']
                
                print("âœ… è¨˜äº‹ç”Ÿæˆå®Œäº†")
                
                # è¨˜äº‹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–
                article = self._parse_article_content(content)
                article["body"] = self._ensure_section_count(article["body"], news_data)
                return article
                
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                print(f"   è©³ç´°: {response.text}")
                
                # Gemini APIãŒåœ°åŸŸåˆ¶é™ã®å ´åˆã¯Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if (response.status_code == 403 or response.status_code == 400) and self.use_gemini is True:
                    print("ğŸ”„ Gemini APIåœ°åŸŸåˆ¶é™ã®ãŸã‚Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_grok(news_data)
                
                # Grok APIãŒã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®å ´åˆã¯Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if response.status_code == 429 and self.use_gemini is None:
                    print("ğŸ”„ Grok APIã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®ãŸã‚Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_claude(news_data)
                
                return None
                
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")
            return None
    
    def _fallback_to_grok(self, news_data: List[Dict]) -> Dict:
        """Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Grok APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Grok APIã®è¨­å®š
        self.api_key = self.config['grok_api']['api_key']
        self.api_url = self.config['grok_api']['api_url']
        self.use_gemini = None
        self.grok_model = (
            self.config.get('grok_api', {}).get('model')
            or os.environ.get('GROK_MODEL')
            or 'grok-3'
        )
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _fallback_to_claude(self, news_data: List[Dict]) -> Dict:
        """Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Claude APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Claude APIã®è¨­å®š
        self.api_key = self.config['claude_api']['api_key']
        self.api_url = self.config['claude_api']['api_url']
        self.use_gemini = False
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _parse_article_content(self, content: str) -> Dict:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
        lines = content.split('\n')
        title = lines[0].replace('#', '').strip() if lines else "é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        
        # æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒˆãƒ«è¡Œã‚’é™¤ãï¼‰
        if lines and lines[0].startswith('#'):
            body = '\n'.join(lines[1:])
        else:
            body = content
        
        return {
            "title": title,
            "lead": "",
            "body": body,
            "tags": "é¦™æ¸¯,åºƒæ±èª,å»£æ±è©±,ä¸­å›½èªãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
        }
    
    def _format_news_for_prompt(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢"""
        formatted = []
        for i, news in enumerate(news_data, 1):
            title = news.get('title', '')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', '')
            published = news.get('published', '')
            
            formatted.append(f"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹ {i}:
ã‚¿ã‚¤ãƒˆãƒ«: {title}
å†…å®¹: {description}
URL: {url}
ã‚½ãƒ¼ã‚¹: {source}
å…¬é–‹æ—¥æ™‚: {published}
""")
        
        return '\n'.join(formatted)
    
    def format_weather_info(self, weather_data: Dict) -> str:
        """å¤©æ°—æƒ…å ±ã‚’Markdownå½¢å¼ã«æ•´å½¢"""
        if weather_data is None:
            return ""
        
        import re
        
        def clean_weather_text(text: str) -> str:
            """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            if not text:
                return ""
            # HTMLã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            text = re.sub(r'<br\s*/?>', '\n', text)
            # ä»–ã®HTMLã‚¿ã‚°ã‚’é™¤å»
            text = re.sub(r'<[^>]+>', '', text)
            # å„è¡Œã”ã¨ã«å‡¦ç†
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                # è¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                line = re.sub(r'\s+', ' ', line).strip()
                if line:
                    cleaned_lines.append(line)
            return '\n'.join(cleaned_lines)
        
        weather_section = "## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—\n"
        has_content = False
        
        # å¤©æ°—è­¦å ±
        if weather_data.get('weather_warning'):
            warning = weather_data['weather_warning']
            title = warning.get('title', 'N/A')
            desc = clean_weather_text(warning.get('description', ''))
            
            if title and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in title and "é…·ç†±å¤©æ°£è­¦å‘Š" not in title and "ç™¼å‡º" not in title:
                weather_section += f"\n### å¤©æ°—è­¦å ±\n{title}\n"
                if desc and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in desc and "é…·ç†±å¤©æ°—è­¦å‘Š" not in desc:
                    weather_section += f"{desc}\n"
                has_content = True
        
        # åœ°åŸŸå¤©æ°—äºˆå ±ã®ã¿è¡¨ç¤º
        if weather_data.get('weather_forecast'):
            forecast = weather_data['weather_forecast']
            title = forecast.get('title', 'N/A')
            desc = clean_weather_text(forecast.get('description', ''))
            
            # å¤©æ°—æƒ…å ±ã¯LLMã§ä¸€æ‹¬æ—¥æœ¬èªç¿»è¨³ï¼ˆè¾æ›¸ç½®æ›ã¯ä½¿ã‚ãªã„ï¼‰
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’åˆ†ã‘ã¦ç¿»è¨³
            if title and title != 'N/A':
                translated_title = self._llm_translate_text(title)
            else:
                translated_title = ""
            
            if desc:
                translated_desc = self._llm_translate_text(desc)
            else:
                translated_desc = ""
            
            # å¤©æ°—äºˆå ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰
            weather_section += "\n### å¤©æ°—äºˆå ±\n"
            if translated_title:
                weather_section += f"{translated_title}\n\n"
            if translated_desc:
                weather_section += f"{translated_desc}\n"
            weather_section += "\n**å¼•ç”¨å…ƒ**: é¦™æ¸¯å¤©æ–‡å°"
            has_content = True

        if not has_content:
            weather_section += "\nç¾åœ¨ã€å¤©æ°—æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å¾Œã»ã©æ›´æ–°äºˆå®šã§ã™ã€‚\n"
        
        return weather_section
    
    def _translate_weather_text(self, text: str) -> str:
        """ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›ï¼ˆæœªä½¿ç”¨ï¼‰ã€‚LLMãƒ™ãƒ¼ã‚¹ç¿»è¨³ã«åˆ‡æ›¿æ¸ˆã¿ã€‚"""
        return self._llm_translate_text(text)

    def _llm_translate_text(self, text: str) -> str:
        """LLMã§åºƒæ±èª/ä¸­æ–‡ã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ä¸€ç™ºç¿»è¨³ï¼ˆæ—¥æœ¬èªä»¥å¤–æ··åœ¨ç¦æ­¢ï¼‰"""
        if not text:
            return ""
        
        # ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²ã—ã¦ç¿»è¨³
        max_chunk_length = 1000
        if len(text) > max_chunk_length:
            # ã¾ãšæ”¹è¡Œã§åˆ†å‰²ã‚’è©¦ã¿ã€ã•ã‚‰ã«é•·ã™ãã‚‹è¡Œã¯å›ºå®šé•·ã§åˆ†å‰²ã™ã‚‹
            def split_into_chunks(raw: str) -> List[str]:
                chunks: List[str] = []
                buffer = ""
                
                for line in raw.split('\n'):
                    # è¡Œè‡ªä½“ãŒé•·ã™ãã‚‹å ´åˆã¯å›ºå®šé•·ã§åˆ†å‰²
                    while len(line) > max_chunk_length:
                        chunks.append(line[:max_chunk_length])
                        line = line[max_chunk_length:]
                    
                    tentative = (buffer + "\n" + line) if buffer else line
                    if len(tentative) > max_chunk_length and buffer:
                        chunks.append(buffer)
                        buffer = line
                    else:
                        buffer = tentative
                
                if buffer:
                    chunks.append(buffer)
                
                # å¿µã®ãŸã‚å›ºå®šé•·ã§ã®å†åˆ†å‰²ï¼ˆæ”¹è¡ŒãŒå…¨ãç„¡ã„ãƒ†ã‚­ã‚¹ãƒˆã«ã‚‚å¯¾å¿œï¼‰
                normalized: List[str] = []
                for chunk in chunks:
                    if len(chunk) <= max_chunk_length:
                        normalized.append(chunk)
                    else:
                        for i in range(0, len(chunk), max_chunk_length):
                            normalized.append(chunk[i:i + max_chunk_length])
                return normalized
            
            translated_chunks = [
                self._llm_translate_text(chunk)
                for chunk in split_into_chunks(text)
            ]
            return "\n".join(translated_chunks)
        
        prompt = (
            "ä»¥ä¸‹ã®åºƒæ±èª/ä¸­æ–‡/è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’å®Œå…¨ã«è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚\n\n"
            "ã€é‡è¦ãªç¿»è¨³ãƒ«ãƒ¼ãƒ«ã€‘\n"
            "1. ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹ã“ã¨ï¼ˆè‹±èªã€åºƒæ±èªã€ä¸­å›½èªãŒæ®‹ã‚‰ãªã„ã‚ˆã†ã«ï¼‰\n"
            "2. æ•°å€¤ã€è¨˜å·ã€æ—¥ä»˜ã€æ™‚åˆ»ã¯ãã®ã¾ã¾ä¿æŒã™ã‚‹ã“ã¨\n"
            "3. åœ°åã€äººåã€çµ„ç¹”åã¯é©åˆ‡ã«æ—¥æœ¬èªè¡¨è¨˜ã™ã‚‹ã“ã¨\n"
            "4. å°‚é–€ç”¨èªã¯é©åˆ‡ãªæ—¥æœ¬èªè¨³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨\n"
            "5. æ–‡ç« ãŒé€”ä¸­ã§åˆ‡ã‚Œãªã„ã‚ˆã†ã€å®Œå…¨ãªæ–‡ç« ã¨ã—ã¦ç¿»è¨³ã™ã‚‹ã“ã¨\n"
            "6. ç¿»è¨³çµæœã«ã¯æ—¥æœ¬èªã®ã¿ã‚’å«ã‚ã€ä»–ã®è¨€èªï¼ˆè‹±èªã€ä¸­å›½èªã€åºƒæ±èªï¼‰ã‚’ä¸€åˆ‡å«ã‚ãªã„ã“ã¨\n\n"
            "åŸæ–‡:\n" + text + "\n\n"
            "æ—¥æœ¬èªè¨³:"
        )

        try:
            if self.use_gemini is True:
                headers = {"Content-Type": "application/json"}
                api_url_with_key = f"{self.api_url}?key={self.api_key}"
                payload = {
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": {"temperature": 0.1, "maxOutputTokens": 4096},
                }
                resp = requests.post(api_url_with_key, headers=headers, json=payload, timeout=120)
                if resp.status_code == 200:
                    txt = resp.json()['candidates'][0]['content']['parts'][0]['text']
                    # "æ—¥æœ¬èªè¨³:" ã§å§‹ã¾ã‚‹å ´åˆã¯é™¤å»
                    if txt.startswith("æ—¥æœ¬èªè¨³:"):
                        txt = txt[6:].strip()
                    return txt.strip()
                else:
                    print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼ (Gemini): HTTP {resp.status_code}")
                    print(f"    ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {resp.text[:200]}")
            else:
                headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
                if self.use_gemini is False:
                    payload = {
                        "model": "claude-3-5-sonnet-20241022",
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.1,
                        "max_tokens": 4096,
                    }
                else:
                    payload = {
                        "model": self.grok_model,
                        "messages": [
                            {"role": "system", "content": "ã‚ãªãŸã¯å„ªç§€ãªç¿»è¨³è€…ã§ã™ã€‚åºƒæ±èªã€ä¸­å›½èªã€è‹±èªã‚’å®Œå…¨ã«è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚åŸæ–‡ã®è¨€èªãŒæ®‹ã‚‰ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚"},
                            {"role": "user", "content": prompt},
                        ],
                        "temperature": 0.1,
                        "max_tokens": 4096,
                    }
                resp = requests.post(self.api_url, headers=headers, json=payload, timeout=120)
                if resp.status_code == 200:
                    if self.use_gemini is False:
                        txt = resp.json()['content'][0]['text']
                    else:
                        txt = resp.json()['choices'][0]['message']['content']
                    # "æ—¥æœ¬èªè¨³:" ã§å§‹ã¾ã‚‹å ´åˆã¯é™¤å»
                    if txt.startswith("æ—¥æœ¬èªè¨³:"):
                        txt = txt[6:].strip()
                    return txt.strip()
                else:
                    print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼ (Claude/Grok): HTTP {resp.status_code}")
                    print(f"    ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {resp.text[:200]}")
        except Exception as e:
            print(f"âš ï¸  å¤©æ°—ç¿»è¨³ã‚¨ãƒ©ãƒ¼ (ä¾‹å¤–): {e}")
            import traceback
            traceback.print_exc()
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸæ–‡ã‚’è¿”å´ï¼ˆå°‘ãªãã¨ã‚‚æ¬ è½ã—ãªã„ï¼‰
        print(f"âš ï¸  å¤©æ°—ç¿»è¨³ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸæ–‡ã‚’è¿”å´")
        return text
    
    def _ensure_section_count(self, body: str, news_data: List[Dict]) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸæœ¬æ–‡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°ã‚’æ¤œè¨¼ã—ã€è¶³ã‚Šãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆ"""
        expected_count = len(news_data)
        if expected_count == 0:
            return body

        section_count = len(re.findall(r'(?m)^###\s', body))
        if section_count >= expected_count:
            return body

        print(f"âš ï¸  è¨˜äº‹æ•°ãŒä¸è¶³: æœŸå¾… {expected_count} ä»¶ã«å¯¾ã— {section_count} ä»¶ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")

        # æ—¢å­˜æœ¬æ–‡ã®å†’é ­ï¼ˆæœ€åˆã®è¦‹å‡ºã—ä»¥å‰ï¼‰ã‚’ä¿æŒ
        first_heading_index = body.find("### ")
        prefix = body[:first_heading_index].strip() if first_heading_index > 0 else ""

        fallback_body = self._build_sections_from_news(news_data)
        fallback_sections = fallback_body.strip()

        combined = []
        if prefix:
            combined.append(prefix)
        combined.append(fallback_sections)

        final_body = "\n\n".join(part for part in combined if part)
        final_count = len(re.findall(r'(?m)^###\s', final_body))
        if final_count < expected_count:
            print(f"âš ï¸  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã§ã‚‚è¨˜äº‹æ•°ãŒä¸è¶³ ({final_count}/{expected_count})ã€‚")
        else:
            print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”Ÿæˆã§ {final_count} ä»¶ã®è¨˜äº‹ã‚’å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
        return final_body

    def _build_sections_from_news(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¢ºå®Ÿã«ä»¶æ•°åˆ†ã®Markdownã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        sections: List[str] = []
        for idx, news in enumerate(news_data, 1):
            source = news.get('_source') or news.get('source') or 'Unknown'
            url = news.get('url', '').strip()

            raw_title = (news.get('title') or f"ãƒ‹ãƒ¥ãƒ¼ã‚¹ {idx}").strip()
            translated_title = self._llm_translate_text(raw_title).strip() or raw_title

            summary_source = (news.get('full_content') or news.get('description') or "").strip()
            if len(summary_source) > 1500:
                summary_source = summary_source[:1500]
            translated_summary = self._llm_translate_text(summary_source).strip() if summary_source else ""

            section_lines = [f"### {translated_title}"]
            if translated_summary:
                section_lines.append(translated_summary)
            if source:
                section_lines.append(f"**å¼•ç”¨å…ƒ**: {source}")
            if url:
                section_lines.append(url)

            sections.append("\n\n".join(section_lines).strip())

        return "\n\n\n".join(sections)
    
    def _generate_cantonese_section(self) -> str:
        """åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’ç”Ÿæˆï¼ˆå›ºå®šå†…å®¹ãƒ»å¤‰æ›´ç¦æ­¢ï¼‰"""
        # ã“ã®å®šå‹æ–‡ã¯è¨˜äº‹ã®æœ€å¾Œã«å¿…ãšè¿½åŠ ã•ã‚Œã‚‹å›ºå®šå†…å®¹ã§ã™
        # å†…å®¹ã‚’å¤‰æ›´ã—ãªã„ã§ãã ã•ã„
        return """## åºƒæ±èªå­¦ç¿’è€…å‘ã‘æƒ…å ±

åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã«LINEãŒè‰¯ã„ã€ä¾¿åˆ©ã¨ã„ã†æ–¹ã‚‚ã„ã‚‹ã§ã—ã‚‡ã†ã‹ã‚‰ã€ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚‚ã‚ã‚Šã¾ã™ã®ã§ã“ã¡ã‚‰ã”ç™»éŒ²ã—ã¦ã‹ã‚‰ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ç§˜æ›¸ã®ãƒªãƒ¼ã•ã‚“ãŒåºƒæ±èªã«ã¤ã„ã¦ãªã‚“ã§ã‚‚å›ç­”ã—ã¦ãã‚Œã¾ã™ã®ã§ãœã²ä½¿ã£ã¦ã¿ã¦ãã ã•ã„

(ä»Šç¾åœ¨400åä»¥ä¸Šã®æ–¹ã«ç™»éŒ²ã—ã¦ã„ãŸã ã„ã¦ãŠã‚Šã¾ã™ï¼‰

[ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”ŸLINEå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ](https://line.me/R/ti/p/@298mwivr)

## åºƒæ±èª| åºƒæ±èªè¶…åŸºç¤ã€€è¶…ç°¡å˜ï¼åˆã‚ã¦ã®åºƒæ±èªã€Œ9å£°6èª¿ã€

https://youtu.be/RAWZAJUrvOU?si=WafOkQixyLiwMhUW"""
    
    def remove_advertisement_content(self, body: str) -> str:
        """è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šãƒ»å®£ä¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»"""
        import re
        
        # åºƒå‘Šãƒ»å®£ä¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        ad_patterns = [
            r'æœ€æ–°ã®å‹•ç”»ç´¹ä»‹ï¼š.*?ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'TOPick.*?ãƒãƒ£ãƒ³ãƒãƒ«.*?ãƒ•ã‚©ãƒ­ãƒ¼.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ç„¡æ–™ã®.*?ä¼šå“¡.*?ä»Šã™ã.*?ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
            r'ä¼šå“¡æ–°è¦å‹Ÿé›†.*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š',
            r'https://whatsapp\.com/channel/.*?',
            r'https://onelink\.to/.*?',
            r'https://event\.hket\.com/.*?',
            r'ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'ç”³ã—è¾¼ã¿å—ä»˜ä¸­',
            r'ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š.*?',
            r'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š.*?',
            r'ğŸ””.*?ãƒ•ã‚©ãƒ­ãƒ¼',
            r'ç„¡æ–™.*?ä¼šå“¡.*?å‚åŠ ã—ã¾ã—ã‚‡ã†',
            r'æ–°è¦ä¼šå“¡ç™»éŒ².*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ',
            # åºƒå‘Šè¨˜äº‹ã®é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
            r'ã“ã®è¨˜äº‹ã¯åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Š.*?ç¿»è¨³ã—ã¾ã›ã‚“ã€‚',
            r'åºƒå‘Šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã«ã‚ˆã£ã¦åˆ¶ä½œã•ã‚ŒãŸ.*?åºƒå‘Šã‚„å®£ä¼æ–‡ã¯é™¤å¤–',
            r'presented.*?news.*?åºƒå‘Š',
            r'ã‚¹ãƒãƒ³ã‚µãƒ¼è¨˜äº‹',
            r'åºƒå‘Šè¨˜äº‹',
            r'PRè¨˜äº‹',
            r'presented.*?content'
        ]
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆAIãŒè‡ªå‹•ç”Ÿæˆã™ã‚‹ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆï¼‰
        unwanted_patterns = [
            r'### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚',
            r'### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹.*?',
            r'### ä»¥ä¸Š.*?',
            r'### çµ‚äº†.*?',
            r'### è¨˜äº‹ã¯ä»¥ä¸Šã§ã™ã€‚',
            r'### ä»¥ä¸ŠãŒ.*?ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚',
            r'### ä»¥ä¸Šã§.*?ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’çµ‚äº†ã—ã¾ã™ã€‚'
        ]
        
        # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã®è¡¨ç¤ºã‚’ä¿®æ­£ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        fix_patterns = [
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒä¸€è¡Œã«ã¾ã¨ã¾ã£ã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£
            (r'\*\*å¼•ç”¨å…ƒ\*\*:\s*([^*]+)\*\*\*ãƒªãƒ³ã‚¯\*\*:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒ*ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£
            (r'\*å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒ*ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã‚’ä¿®æ­£ï¼ˆåˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            (r'\*å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^\s]+)ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^:]+):\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP**ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP*ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP*ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMPãƒªãƒ³ã‚¯: [URL](URL) â†’ å¼•ç”¨å…ƒè¡Œ + URLç‹¬ç«‹è¡Œ
            (r'å¼•ç”¨å…ƒ:\s*([^\s]+)ãƒªãƒ³ã‚¯:\s*\[((?:https?|ftp)://[^\]]+)\]\(([^\)]+)\)', r'**å¼•ç”¨å…ƒ**: \1\n\n\3'),
            # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ãŒåŒä¸€è¡Œï¼ˆ[]()ä»˜ããƒ»å¤ªå­—ã§ãªã„ï¼‰â†’ å¼•ç”¨å…ƒè¡Œ + URLç‹¬ç«‹è¡Œ
            (r'å¼•ç”¨å…ƒ:\s*([^\n]+?)\s*ãƒªãƒ³ã‚¯:\s*\[([^\]]+)\]\(([^\)]+)\)', r'**å¼•ç”¨å…ƒ**: \1\n\n\3'),
            # å¼•ç”¨å…ƒ: SCMP**ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # å¼•ç”¨å…ƒ: SCMP*ãƒªãƒ³ã‚¯: URL ã®å½¢å¼ã‚’ä¿®æ­£ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰
            (r'å¼•ç”¨å…ƒ:\s*([^*]+)\*ãƒªãƒ³ã‚¯:\s*([^\s]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # HTMLæ®µè½ã§å‡ºåŠ›ã•ã‚ŒãŸå¼•ç”¨æƒ…å ±ã‚’Markdown2è¡Œã«æ­£è¦åŒ–
            (r'<p[^>]*>\s*<strong>å¼•ç”¨å…ƒ</strong>:\s*([^<]+)<br\s*/?>\s*<strong>ãƒªãƒ³ã‚¯</strong>:\s*(https?://[^\s<]+)\s*</p>', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2'),
            # strongã‚¿ã‚°æ··åœ¨ã®å˜è¡Œè¡¨è¨˜ã‚’æ­£è¦åŒ–
            (r'<strong>å¼•ç”¨å…ƒ</strong>:\s*([^<]+)\s*<strong>ãƒªãƒ³ã‚¯</strong>:\s*(https?://[^\s<]+)', r'**å¼•ç”¨å…ƒ**: \1\n**ãƒªãƒ³ã‚¯**: \2')
        ]

        # HTMLæ®‹éª¸ã®å‰Šé™¤ï¼ˆæ±ç”¨ï¼‰
        html_cleanup_patterns = [
            r'<p[^>]*>\s*</p>',                 # ç©ºã®p
            r'</?br\s*/?>',                    # brã‚¿ã‚°
            # [![...]](...) ã‚’åŒ…ã‚€ p/span ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’å‰¥ãŒã™
            r'<p[^>]*>\s*<span[^>]*>(\[!\[.*?\]\(.*?\)\]\(.*?\))\s*</span>\s*</p>'
        ]
        
        # åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»
        cleaned_body = body
        for pattern in ad_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å»
        for pattern in unwanted_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # å¼•ç”¨å…ƒã¨ãƒªãƒ³ã‚¯ã®è¡¨ç¤ºã‚’ä¿®æ­£
        for pattern, replacement in fix_patterns:
            cleaned_body = re.sub(pattern, replacement, cleaned_body, flags=re.DOTALL | re.IGNORECASE)

        # æ±ç”¨HTMLã‚¿ã‚°ã®æƒé™¤ï¼ˆå¿…è¦æœ€å°é™ï¼‰
        for pattern in html_cleanup_patterns:
            # ãƒ©ãƒƒãƒ‘ãƒ¼é™¤å»ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¯ç½®æ›å¯¾è±¡ã‚’æ®‹ã™
            if '(' in pattern and '\\[!\\[' in pattern:
                cleaned_body = re.sub(pattern, r'\1', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
            else:
                cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)

        # noteå´ã®è‡ªå‹•ãƒªãƒ³ã‚¯åŒ–ã«ä»»ã›ã‚‹ãŸã‚ã€URLã¯ãƒ—ãƒ¬ãƒ¼ãƒ³ã§ç‹¬ç«‹è¡Œã«ã™ã‚‹
        # **ãƒªãƒ³ã‚¯**: [text](url) â†’ ç©ºè¡Œ + url
        cleaned_body = re.sub(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*\[[^\]]+\]\((https?://[^\)]+)\)', r'\n\n\1', cleaned_body)
        # **ãƒªãƒ³ã‚¯**: url â†’ ç©ºè¡Œ + url
        cleaned_body = re.sub(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://\S+)', r'\n\n\1', cleaned_body)
        # ãƒªãƒ³ã‚¯: url â†’ ç©ºè¡Œ + url
        cleaned_body = re.sub(r'(?m)^ãƒªãƒ³ã‚¯:\s*(https?://\S+)', r'\n\n\1', cleaned_body)
        
        # è¡Œæœ«ã®ä½™åˆ†ãªã‚¹ãƒšãƒ¼ã‚¹ã‚’é™¤å»ï¼ˆæ”¹è¡Œå‰ã®2ã‚¹ãƒšãƒ¼ã‚¹ãªã©ï¼‰
        cleaned_body = re.sub(r'[ \t]+$', '', cleaned_body, flags=re.MULTILINE)

        # é€£ç¶šé‡è¤‡ã™ã‚‹å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯ã‚’1ã¤ã«åœ§ç¸®ï¼ˆURLç‹¬ç«‹è¡Œå¯¾å¿œï¼‰
        cleaned_body = re.sub(r'(\*\*å¼•ç”¨å…ƒ\*\*: .*?\n+https?://\S+)\n+\1', r'\1', cleaned_body, flags=re.DOTALL)

        # åºƒæ±èªå­¦ç¿’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®é‡è¤‡è¡Œã‚’1å›ã«åœ§ç¸®ï¼ˆç”»åƒãƒªãƒ³ã‚¯2ç¨®ï¼‰
        cantonese_img1 = re.escape('[![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-img1.jpg)](https://line.me/R/ti/p/@298mwivr)')
        cantonese_img2 = re.escape('[![LINEã§ãŠå•åˆã›](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-qr.png)](https://line.me/R/ti/p/@298mwivr)')
        cleaned_body = re.sub(fr'(?:{cantonese_img1})\n+(?:{cantonese_img1})', r'\g<0>'.replace('\\g<0>','\1'), cleaned_body)
        cleaned_body = re.sub(fr'(?:{cantonese_img2})\n+(?:{cantonese_img2})', r'\g<0>'.replace('\\g<0>','\1'), cleaned_body)

        # ä¸Šè¨˜ã®ãƒãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ç”ŸæˆãŒé›£ã—ã„ãŸã‚æ˜ç¤ºç½®æ›ï¼ˆ2å›ä»¥ä¸Šã®é€£ç¶šã‚’1å›ã¸ï¼‰
        cleaned_body = re.sub(fr'(?:{cantonese_img1})(?:\n+{cantonese_img1})+', r'[![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-img1.jpg)](https://line.me/R/ti/p/@298mwivr)', cleaned_body)
        cleaned_body = re.sub(fr'(?:{cantonese_img2})(?:\n+{cantonese_img2})+', r'[![LINEã§ãŠå•åˆã›](https://raw.githubusercontent.com/cantoneseslang/hongkong-daily-news-note/main/shared/line-qr.png)](https://line.me/R/ti/p/@298mwivr)', cleaned_body)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«
        cleaned_body = re.sub(r'\n{3,}', '\n\n', cleaned_body)
        
        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºè¡Œã‚’é™¤å»
        cleaned_body = cleaned_body.strip()
        
        return cleaned_body
    
    def remove_duplicate_articles(self, body: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–"""
        import re
        
        # ### ã§å§‹ã¾ã‚‹è¨˜äº‹ã‚’åˆ†å‰²
        articles = re.split(r'\n### ', body)
        
        # æœ€åˆã®è¦ç´ ã¯ç©ºã¾ãŸã¯å¤©æ°—æƒ…å ±ãªã®ã§ãã®ã¾ã¾ä¿æŒ
        if not articles:
            return body
        
        result = [articles[0]]
        seen_title_words: List[Set[str]] = []
        duplicate_count = 0
        
        # å„è¨˜äº‹ã‚’ãƒã‚§ãƒƒã‚¯
        for article in articles[1:]:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
            lines = article.split('\n', 1)
            if len(lines) > 0:
                title = lines[0].strip()
                
                title_words = normalize_title_words(title)
                # èªæ•°ãŒæ¥µç«¯ã«å°‘ãªã„å ´åˆã¯é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’ç·©ã‚ã‚‹
                if len(title_words) < 2:
                    result.append(article)
                    continue
                
                if not is_similar_title_words(title_words, seen_title_words):
                    seen_title_words.append(title_words)
                    result.append(article)
                else:
                    duplicate_count += 1
        
        if duplicate_count > 0:
            print(f"ğŸ”„ é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–: {duplicate_count}ä»¶")
        
        # å†çµåˆï¼ˆè¦‹å‡ºã—ã®å‰ã«ç©ºè¡Œã‚’å…¥ã‚Œã‚‹ï¼‰
        if len(result) > 1:
            return result[0] + '\n\n### ' + '\n\n### '.join(result[1:])
        else:
            return result[0]
    
    def save_article(self, article: Dict, weather_data: Dict = None, output_path: str = None) -> str:
        """ç”Ÿæˆã—ãŸè¨˜äº‹ã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        if output_path is None:
            timestamp = datetime.now(HKT).strftime('%Y-%m-%d')
            output_path = f"daily-articles/hongkong-news_{timestamp}.md"
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨é‡è¤‡ã‚’é™¤å¤–
        article['body'] = self.remove_advertisement_content(article['body'])
        article['body'] = self.remove_duplicate_articles(article['body'])
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åŒºåˆ‡ã‚Šç·šã‚’å‰Šé™¤ã—ã€è¦‹å‡ºã—å‰ã«ç©ºè¡Œã‚’è¿½åŠ 
        import re
        article['body'] = re.sub(r'\n+---\n+', '\n', article['body'])
        article['body'] = re.sub(r'\n{3,}', '\n\n', article['body'])
        # è¦‹å‡ºã—ã®å‰ã«å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
        article['body'] = re.sub(r'([^\n])\n(###)', r'\1\n\n\2', article['body'])
        
        # å¤©æ°—æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        weather_section = self.format_weather_info(weather_data) if weather_data is not None else ""
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’çµ„ã¿ç«‹ã¦ï¼ˆç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ”¹è¡Œã‚’æŒŸã¾ãªã„ï¼‰
        content_parts = []
        if weather_section:
            content_parts.append(weather_section)
        if article['lead']:
            content_parts.append(article['lead'])
        content_parts.append(article['body'])
        
        # Markdownç”Ÿæˆ
        content_str = '\n\n'.join(content_parts)
        
        # åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’è¿½åŠ 
        cantonese_section = self._generate_cantonese_section()
        
        # è¦‹å‡ºã—ç”»åƒã‚’ç”Ÿæˆï¼ˆè¨˜äº‹æœ¬æ–‡ã‚’æ¸¡ã—ã¦æœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼‰
        thumbnail_path = self._generate_thumbnail_image(article_body=article['body'])
        
        # front matterã«thumbnailã‚’è¿½åŠ 
        front_matter = ""
        if thumbnail_path:
            front_matter = f"""---
title: {article['title']}
thumbnail: {thumbnail_path}
---

"""
        
        # bodyã®æœ€åˆã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ï¼ˆ1è¡Œç›®ãŒç©ºè¡Œã«ãªã‚Šã€ã“ã“ã«ç›®æ¬¡ã‚’æŒ¿å…¥ï¼‰
        markdown = f"""{front_matter}# {article['title']}

{content_str}

{cantonese_section}
----
**ã‚¿ã‚°**: {article['tags']}
**ç”Ÿæˆæ—¥æ™‚**: {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"ğŸ’¾ è¨˜äº‹ã‚’ä¿å­˜: {output_path}")
        if thumbnail_path:
            print(f"ğŸ–¼ï¸  è¦‹å‡ºã—ç”»åƒ: {thumbnail_path}")
        return output_path
    
    def _extract_first_news_title(self, article_body: str) -> str:
        """è¨˜äº‹æœ¬æ–‡ã‹ã‚‰æœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆå¤©æ°—äºˆå ±ã®æ¬¡ï¼‰ã‚’æŠ½å‡º"""
        import re
        
        # å¤©æ°—ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—ã‹ã‚‰æ¬¡ã®##ã¾ãŸã¯###ã¾ã§ï¼‰
        # å¤©æ°—ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã€Œ### å¤©æ°—äºˆå ±ã€ã‚‚å«ã‚ã¦ã‚¹ã‚­ãƒƒãƒ—
        weather_pattern = r'##\s*æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—.*?(?=\n###\s+(?!å¤©æ°—äºˆå ±)|\n##|$)'
        body_after_weather = re.sub(weather_pattern, '', article_body, flags=re.DOTALL)
        
        # ãƒªãƒ¼ãƒ‰æ–‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
        # æœ€åˆã® ### ã§å§‹ã¾ã‚‹è¦‹å‡ºã—ã‚’æ¢ã™ï¼ˆã€Œå¤©æ°—äºˆå ±ã€ä»¥å¤–ï¼‰
        lines = body_after_weather.split('\n')
        for line in lines:
            line = line.strip()
            # ### ã§å§‹ã¾ã‚Šã€ã€Œå¤©æ°—äºˆå ±ã€ã§ãªã„è¦‹å‡ºã—ã‚’æ¢ã™
            if line.startswith('###') and 'å¤©æ°—äºˆå ±' not in line:
                title = line.replace('###', '').strip()
                # è¦‹å‡ºã—ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚„ä½™åˆ†ãªæ–‡å­—ã‚’é™¤å»
                title = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', title)  # Markdownãƒªãƒ³ã‚¯ã‚’é™¤å»
                title = title.strip()
                if title:
                    return title
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®è¦‹å‡ºã—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        return "é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"
    
    def _get_outfit_pattern(self) -> str:
        """å‰æ—¥ã¨ç•°ãªã‚‹æœè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ"""
        import random
        import json
        from pathlib import Path
        
        # 10ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœè£…
        outfit_patterns = [
            "wearing a dark brown suit, light brown shirt, and dark navy tie; the woman on the right has shoulder-length brown hair with pony tail wearing the glasses, wearing a light orange blouse and brown skirt.",
            "wearing a dark brown suit, pale green shirt, and light brown tie; the woman on the right has shoulder-length brown hair with pony tail wearing the glasses, wearing a light sky blue blouse and light brown skirt.",
            "wearing a navy blue suit, blue shirt, and light yellow tie; the woman on the right has shoulder-length brown hair with pony tail, wearing a milly yellow blouse and light gray skirt.",
            "wearing a dark brown suit, light gray shirt, and brown tie; the woman on the right has shoulder-length brown hair with pony tail wearing the glasses, wearing a light blue blouse and brown skirt.",
            "wearing a dark blue suit, milky white shirt, and deep blue tie; the woman on the right has shoulder-length brown hair with pony tail, wearing a mily white blouse and light gray skirt.",
            "wearing a milky brown suit, light blue shirt, and light orange tie; the woman on the right has shoulder-length brown hair with pony tail wearing the glasses, wearing a light yellow blouse and sky blue skirt.",
            "wearing a dark blue suit, light sky blue shirt, and light yellow tie; the woman on the right has shoulder-length brown hair with pony tail wearing the glasses, wearing a light blue blouse and light white skirt.",
            "wearing a dark navy blue suit, blue shirt, and orange tie; the woman on the right has shoulder-length brown hair with pony tail wearing the glasses, wearing a pink blouse and light white skirt.",
            "wearing a dark navy blue suit, white shirt, and glay tie; the woman on the right has shoulder-length brown hair with bangs, wearing a white blouse and light glay skirt.",
            "wearing a dark navy blue suit, white shirt, and blue tie; the woman on the right has shoulder-length brown hair with bangs, wearing a white blouse and dark skirt.",
        ]
        
        # å‰æ—¥ã®æœè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª­ã¿è¾¼ã‚€
        history_file = Path("thumbnail_outfit_history.json")
        last_outfit_index = None
        
        if history_file.exists():
            try:
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
                    last_outfit_index = history.get('last_outfit_index')
            except Exception as e:
                print(f"âš ï¸  æœè£…å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # å‰æ—¥ã¨ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ
        if last_outfit_index is not None:
            available_indices = [i for i in range(len(outfit_patterns)) if i != last_outfit_index]
            selected_index = random.choice(available_indices)
        else:
            selected_index = random.randint(0, len(outfit_patterns) - 1)
        
        # é¸æŠã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿å­˜
        try:
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump({'last_outfit_index': selected_index}, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  æœè£…å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        return outfit_patterns[selected_index]
    
    def _generate_thumbnail_image(self, article_body: str = None) -> str:
        """è¦‹å‡ºã—ç”»åƒã‚’ç”Ÿæˆã—ã¦ä¸€æ™‚ä¿å­˜"""
        try:
            # generate_thumbnail.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            import sys
            import os
            sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
            
            from generate_thumbnail import generate_thumbnail_for_article
            
            # æ—¥ä»˜ã‚’å–å¾—ï¼ˆä¾‹: "2025 11 23"ï¼‰
            today = datetime.now(HKT)
            # æœˆã¨æ—¥ã‹ã‚‰å…ˆé ­ã®0ã‚’é™¤å»
            month = str(today.month)
            day = str(today.day)
            date_str = f"{today.year} {month} {day}"
            
            # æœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
            if article_body:
                first_news_title = self._extract_first_news_title(article_body)
            else:
                first_news_title = "é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹"
            
            # æœè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
            outfit_pattern = self._get_outfit_pattern()
            
            print(f"ğŸ¨ è¦‹å‡ºã—ç”»åƒç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            print(f"   æ—¥ä»˜: {date_str}")
            print(f"   æœ€åˆã®ãƒ‹ãƒ¥ãƒ¼ã‚¹: {first_news_title[:50]}...")
            print(f"   æœè£…ãƒ‘ã‚¿ãƒ¼ãƒ³: {outfit_pattern[:50]}...")
            
            # ç”»åƒã‚’ç”Ÿæˆ
            thumbnail_path = generate_thumbnail_for_article(
                config_path=self.config_path,
                output_dir="images",
                date_str=date_str,
                first_news_title=first_news_title,
                outfit_pattern=outfit_pattern
            )
            
            return thumbnail_path if thumbnail_path else ""
            
        except Exception as e:
            print(f"âš ï¸  è¦‹å‡ºã—ç”»åƒç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return ""

def preprocess_news(news_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ"""
    import re
    import os
    from collections import defaultdict
    from datetime import datetime, timedelta
    
    # 0. éå»ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å‡ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    past_urls = set()
    past_title_words: List[Set[str]] = []
    past_title_chars: List[str] = []
    
    for days_ago in range(1, 4):
        past_date = datetime.now(HKT) - timedelta(days=days_ago)
        past_file = f"daily-articles/hongkong-news_{past_date.strftime('%Y-%m-%d')}.md"
        
        if os.path.exists(past_file):
            print(f"ğŸ“‚ éå»è¨˜äº‹ãƒã‚§ãƒƒã‚¯: {past_file}")
            try:
                with open(past_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    url_matches = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://[^\s]+)', content)
                    normalized_urls = {normalize_url(url) for url in url_matches if url}
                    past_urls.update({u for u in normalized_urls if u})
                    
                    title_matches = re.findall(r'^### (.+)$', content, re.MULTILINE)
                    filtered_titles = [t for t in title_matches if 'å¤©æ°—' not in t and 'weather' not in t.lower()]
                    for t in filtered_titles:
                        words = normalize_title_words(t)
                        if words:
                            past_title_words.append(words)
                        chars = normalize_title_chars(t)
                        if chars:
                            past_title_chars.append(chars)
                    
                print(f"  âœ“ æ—¢å‡ºURL: {len(normalized_urls)}ä»¶ã€æ—¢å‡ºã‚¿ã‚¤ãƒˆãƒ«: {len(filtered_titles)}ä»¶")
            except Exception as e:
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    if past_urls or past_title_words:
        print(f"ğŸ” éå»è¨˜äº‹ã‹ã‚‰åˆè¨ˆ {len(past_urls)} ä»¶ã®URLã¨ {len(past_title_words)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º")
    
    # 1. åˆæœŸãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡ãƒ»å¤©æ°—è¨˜äº‹ãƒ»NGãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒˆãƒ”ãƒƒã‚¯éå‰°ãƒ»é¦™æ¸¯ç„¡é–¢ä¿‚è¨˜äº‹ã‚’é™¤å¤–ï¼‰
    recent_topics = get_recent_topics(days=3)
    
    # é »å‡ºãƒˆãƒ”ãƒƒã‚¯è¡¨ç¤º
    if recent_topics:
        print(f"\nğŸ“Š éå»3æ—¥é–“ã®é »å‡ºãƒˆãƒ”ãƒƒã‚¯:")
        for topic, count in sorted(recent_topics.items(), key=lambda x: -x[1]):
            status = "âš ï¸ éå‰°" if count >= 2 else "âœ… æ­£å¸¸"
            print(f"  {status} {topic}: {count}å›")
    
    filtered_news = []
    duplicate_count = 0
    ng_word_count = 0
    non_hk_count = 0
    overused_topic_count = 0
    
    for news in news_list:
        url = news.get('url', '')
        title = news.get('title', '')
        description = news.get('description', '')
        published_at = news.get('published_at') or news.get('published') or news.get('publishedAt')
        
        normalized_url = normalize_url(url)
        title_words = news.get('_title_words') or normalize_title_words(title)
        news['_title_words'] = title_words
        title_chars = news.get('_title_chars') or normalize_title_chars(title)
        news['_title_chars'] = title_chars
        if '_topic_key' not in news:
            topic_key = derive_topic_key(title)
            if topic_key:
                news['_topic_key'] = topic_key
        news['_normalized_url'] = normalized_url
        news['_source'] = (news.get('source') or 'Unknown').strip() or 'Unknown'
        news['_published_dt'] = parse_published_at(published_at)
        
        # å¤©æ°—è¨˜äº‹é™¤å¤–
        weather_keywords = ['æ°—æ¸©', 'å¤©æ°—', 'å¤©æ–‡å°', 'æ°—è±¡', 'å¤©å€™', 'temperature', 'weather', 'observatory', 'forecast', 'â„ƒ', 'åº¦']
        if any(keyword in title.lower() or keyword in title for keyword in weather_keywords):
            duplicate_count += 1
            continue
        
        # NGãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼ˆå…¨å›½é‹å‹•ä¼šãªã©ï¼‰
        ng_keywords = [
            'å…¨å›½é‹å‹•ä¼š', 'national games', 'å…¨é‹ä¼š', 'å…¨å›½é‹å‹•',
            'å®ç¦è‹‘', 'å®ç¦è‹‘ç«ç½', 'å®ç¦è‹‘ç«ç½ç¾å ´', 'é¦™æ¸¯èµ¤åå­—ä¼š', 'å¤§åŸ”å®ç¦è‹‘ç«ç½',
        ]
        content_lower = f"{title} {description}".lower()
        if any(keyword.lower() in content_lower for keyword in ng_keywords):
            ng_word_count += 1
            continue
        
        # éå‰°ãƒˆãƒ”ãƒƒã‚¯é™¤å¤–
        if is_overused_topic(title, description, recent_topics, threshold=2):
            overused_topic_count += 1
            continue
        
        # é¦™æ¸¯é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        # 1. æ˜ã‚‰ã‹ã«é¦™æ¸¯ç„¡é–¢ä¿‚ã®åœ°åŸŸãƒ»å›½ã‚’ãƒ–ãƒ­ãƒƒã‚¯
        non_hk_regions = [
            'ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠ', 'ukraine', 'ã‚¼ãƒ¬ãƒ³ã‚¹ã‚­ãƒ¼', 'zelensky',
            'ãƒ­ã‚·ã‚¢', 'russia', 'ãƒ—ãƒ¼ãƒãƒ³', 'putin',
            'ã‚ªãƒ©ãƒ³ãƒ€', 'netherlands', 'dutch',
            'å¤§åˆ†', 'ç†Šæœ¬', 'ç¦å²¡', 'æ²–ç¸„', 'åŒ—æµ·é“',  # æ—¥æœ¬ã®åœ°æ–¹éƒ½å¸‚ï¼ˆæ±äº¬ä»¥å¤–ï¼‰
            'ãƒˆãƒ«ã‚³', 'turkey', 'ã‚¨ãƒ«ãƒ‰ã‚¢ãƒ³', 'erdogan',
            'ã‚¤ã‚¹ãƒ©ã‚¨ãƒ«', 'israel', 'ãƒ‘ãƒ¬ã‚¹ãƒãƒŠ', 'palestine',
        ]
        if any(region in content_lower or region in title.lower() for region in non_hk_regions):
            # ãŸã ã—ã€é¦™æ¸¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯è¨±å¯
            if not any(keyword in content_lower for keyword in ['é¦™æ¸¯', 'hong kong', 'hongkong']):
                non_hk_count += 1
                continue
        
        # 2. é¦™æ¸¯é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        hk_keywords = [
            'é¦™æ¸¯', 'hong kong', 'hk ', ' hk', 'hongkong',
            'ç«‹æ³•ä¼š', 'legco', 'è¡Œæ”¿é•·å®˜', 'ç‰¹åŒºæ”¿åºœ',
            'mtr', 'ä¹é¾', 'kowloon', 'æ–°ç•Œ', 'new territories',
            'é¦™æ¸¯å³¶', 'hong kong island', 'ä¸­ç’°', 'central',
            'å¤§åŸ”', 'tai po', 'å±¯é–€', 'tuen mun', 'è¦³å¡˜', 'kwun tong',
            'æ—ºè§’', 'mong kok', 'å°–æ²™å’€', 'tsim sha tsui',
            'ç£ä»”', 'wan chai', 'wanchai', 'éŠ…é‘¼ç£', 'causeway bay',
            'scmp', 'rthk', 'hkfp',  # é¦™æ¸¯ãƒ¡ãƒ‡ã‚£ã‚¢
            'bn(o)', 'bno',  # é¦™æ¸¯äººå‘ã‘åˆ¶åº¦
        ]
        is_hk_related = any(keyword in content_lower for keyword in hk_keywords)
        if not is_hk_related:
            non_hk_count += 1
            continue
        
        # é‡è¤‡URLé™¤å¤–
        if normalized_url and normalized_url in past_urls:
            duplicate_count += 1
            continue
        
        # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«é™¤å¤–
        title_chars = news.get('_title_chars')
        if title_words and is_similar_title_words(title_words, past_title_words):
            duplicate_count += 1
            continue
        if title_chars and any(
            title_similarity_chars(title_chars, past_chars) >= 0.88
            for past_chars in past_title_chars
        ):
            duplicate_count += 1
            continue
        
        filtered_news.append(news)
    
    if duplicate_count > 0:
        print(f"ğŸš« éå»è¨˜äº‹ã¨ã®é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶")
    if ng_word_count > 0:
        print(f"ğŸš« NGãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼ˆå…¨å›½é‹å‹•ä¼šç­‰ï¼‰: {ng_word_count}ä»¶")
    if overused_topic_count > 0:
        print(f"ğŸš« éå‰°ãƒˆãƒ”ãƒƒã‚¯é™¤å¤–: {overused_topic_count}ä»¶")
    if non_hk_count > 0:
        print(f"ğŸš« é¦™æ¸¯ç„¡é–¢ä¿‚è¨˜äº‹é™¤å¤–: {non_hk_count}ä»¶")
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(news_list)} â†’ {len(filtered_news)}ä»¶")
    
    # 2. åŒæ—¥å†…é‡è¤‡é™¤å¤–
    existing_title_words: List[Set[str]] = []
    existing_title_chars: List[str] = []
    unique_news = []
    same_day_duplicates = 0
    
    for news in filtered_news:
        title_words = news.get('_title_words') or normalize_title_words(news.get('title', ''))
        news['_title_words'] = title_words
        title_chars = news.get('_title_chars') or normalize_title_chars(news.get('title', ''))
        news['_title_chars'] = title_chars
        
        if title_words and is_similar_title_words(title_words, existing_title_words):
            same_day_duplicates += 1
            continue
        if title_chars and any(
            title_similarity_chars(title_chars, existing_chars) >= 0.9
            for existing_chars in existing_title_chars
        ):
            same_day_duplicates += 1
            continue
        
        if title_words:
            existing_title_words.append(title_words)
        if title_chars:
            existing_title_chars.append(title_chars)
        unique_news.append(news)
    
    if same_day_duplicates > 0:
        print(f"ğŸ“Š åŒæ—¥å†…é‡è¤‡é™¤å¤–: {len(filtered_news)} â†’ {len(unique_news)}ä»¶")
    
    # 3. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    categorized = defaultdict(list)
    
    for news in unique_news:
        title_text = news.get('title', '').lower()
        description_text = news.get('description', '').lower()
        content = f"{title_text} {description_text}"
        
        if any(keyword in content for keyword in ['ãƒ“ã‚¸ãƒã‚¹', 'çµŒæ¸ˆ', 'é‡‘è', 'æ ªå¼', 'æŠ•è³‡', 'business', 'economy', 'finance', 'stock', 'investment', 'ipo', 'ä¸Šå ´', 'å–å¼•æ‰€', 'éŠ€è¡Œ', 'ä¿é™º']):
            category = 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ'
        elif any(keyword in content for keyword in ['ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼', 'ai', 'äººå·¥çŸ¥èƒ½', 'ãƒ­ãƒœãƒƒãƒˆ', 'ãƒ‡ã‚¸ã‚¿ãƒ«', 'ã‚¢ãƒ—ãƒª', 'ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢', 'ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢', 'technology', 'digital', 'app', 'software', 'hardware', 'ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³', 'ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼']):
            category = 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif any(keyword in content for keyword in ['åŒ»ç™‚', 'å¥åº·', 'ç—…é™¢', 'åŒ»å¸«', 'è–¬', 'æ²»ç™‚', 'medical', 'health', 'hospital', 'doctor', 'medicine', 'treatment', 'covid', 'ã‚³ãƒ­ãƒŠ', 'ãƒ¯ã‚¯ãƒãƒ³']):
            category = 'åŒ»ç™‚ãƒ»å¥åº·'
        elif any(keyword in content for keyword in ['æ•™è‚²', 'å­¦æ ¡', 'å¤§å­¦', 'å­¦ç”Ÿ', 'æ•™å¸«', 'education', 'school', 'university', 'student', 'teacher', 'å­¦ç¿’', 'ç ”ç©¶']):
            category = 'æ•™è‚²'
        elif any(keyword in content for keyword in ['ä¸å‹•ç”£', 'ä½å®…', 'ãƒãƒ³ã‚·ãƒ§ãƒ³', 'åœŸåœ°', 'è³ƒè²¸', 'real estate', 'property', 'housing', 'apartment', 'rent', 'åœŸåœ°', 'å»ºç‰©']):
            category = 'ä¸å‹•ç”£'
        elif any(keyword in content for keyword in ['äº¤é€š', 'é›»è»Š', 'ãƒã‚¹', 'ã‚¿ã‚¯ã‚·ãƒ¼', 'ç©ºæ¸¯', 'transport', 'train', 'bus', 'taxi', 'airport', 'mtr', 'åœ°ä¸‹é‰„', 'è·¯ç·š']):
            category = 'äº¤é€š'
        elif any(keyword in content for keyword in ['çŠ¯ç½ª', 'é€®æ•', 'è­¦å¯Ÿ', 'è£åˆ¤', 'åˆ‘å‹™æ‰€', 'crime', 'arrest', 'police', 'court', 'prison', 'é•æ³•', 'äº‹ä»¶', 'æœæŸ»']):
            category = 'æ²»å®‰ãƒ»çŠ¯ç½ª'
        elif any(keyword in content for keyword in ['äº‹æ•…', 'ç½å®³', 'ç«äº‹', 'åœ°éœ‡', 'å°é¢¨', 'accident', 'disaster', 'fire', 'earthquake', 'typhoon', 'ç·Šæ€¥', 'æ•‘åŠ©']):
            category = 'äº‹æ•…ãƒ»ç½å®³'
        elif any(keyword in content for keyword in ['æ”¿æ²»', 'æ”¿åºœ', 'è­°å“¡', 'é¸æŒ™', 'æ”¿ç­–', 'politics', 'government', 'minister', 'election', 'policy', 'è¡Œæ”¿', 'è­°ä¼š']):
            category = 'æ”¿æ²»ãƒ»è¡Œæ”¿'
        elif any(keyword in content for keyword in ['ã‚¹ãƒãƒ¼ãƒ„', 'sports', 'è©¦åˆ', 'é¸æ‰‹', 'ãƒ¡ãƒ€ãƒ«', 'ç«¶æŠ€', 'ãƒ•ã‚§ãƒ³ã‚·ãƒ³ã‚°', 'è‡ªè»¢è»Š', 'ã‚µãƒƒã‚«ãƒ¼', 'ãƒã‚¹ã‚±', 'ãƒ†ãƒ‹ã‚¹', 'æ°´æ³³', 'match', 'athlete', 'medal', 'game']):
            category = 'ã‚¹ãƒãƒ¼ãƒ„'
        elif any(keyword in content for keyword in ['æ–‡åŒ–', 'èŠ¸èƒ½', 'æ˜ ç”»', 'éŸ³æ¥½', 'ã‚¢ãƒ¼ãƒˆ', 'culture', 'entertainment', 'movie', 'music', 'art', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'ç¥­ã‚Š', 'ä¼çµ±', 'ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ', 'ã‚¿ãƒ¬ãƒ³ãƒˆ']):
            category = 'ã‚«ãƒ«ãƒãƒ£ãƒ¼'
        else:
            category = 'ç¤¾ä¼šãƒ»ãã®ä»–'
        
        news['category'] = category
        categorized[category].append(news)
    
    print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ä»¶æ•°:")
    for cat, items in sorted(categorized.items(), key=lambda x: -len(x[1])):
        print(f"  {cat}: {len(items)}ä»¶")
    
    # å…¬é–‹æ—¥æ™‚ã§å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚½ãƒ¼ãƒˆ
    for cat, items in categorized.items():
        for item in items:
            if '_title_words' not in item:
                item['_title_words'] = normalize_title_words(item.get('title', ''))
            if '_published_dt' not in item:
                published_at = item.get('published_at') or item.get('published') or item.get('publishedAt')
                item['_published_dt'] = parse_published_at(published_at)
            if '_source' not in item:
                item['_source'] = (item.get('source') or 'Unknown').strip() or 'Unknown'
        categorized[cat] = sorted(
            items,
            key=lambda n: (n.get('_published_dt') is not None, n.get('_published_dt')),
            reverse=True
        )
    
    # 4. ãƒãƒ©ãƒ³ã‚¹é¸æŠï¼ˆå„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦15-20ä»¶é¸æŠï¼‰
    selected = []
    selected_ids = set()
    selected_title_words: List[Set[str]] = []
    category_counts = defaultdict(int)
    source_usage = defaultdict(int)
    topic_usage = defaultdict(int)
    topic_exceeded = defaultdict(int)
    
    target_count = 100
    max_per_source_initial = 4
    fallback_category_limit = 10
    category_limits = {
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ': 20,
        'ç¤¾ä¼šãƒ»ãã®ä»–': 15,
        'ã‚«ãƒ«ãƒãƒ£ãƒ¼': 12,
        'æ”¿æ²»ãƒ»è¡Œæ”¿': 12,
        'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼': 10,
        'äº¤é€š': 8,
        'ä¸å‹•ç”£': 8,
        'äº‹æ•…ãƒ»ç½å®³': 5,
        'æ²»å®‰ãƒ»çŠ¯ç½ª': 5,
        'åŒ»ç™‚ãƒ»å¥åº·': 5,
        'æ•™è‚²': 3,
        'ã‚¹ãƒãƒ¼ãƒ„': 3,  # å…¨é‹ä¼šé–¢é€£ã¯é™¤å¤–æ¸ˆã¿
    }
    
    priority_cats = [
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ',
        'æ”¿æ²»ãƒ»è¡Œæ”¿',
        'ç¤¾ä¼šãƒ»ãã®ä»–',
        'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼',
        'äº¤é€š',
        'ä¸å‹•ç”£',
        'ã‚«ãƒ«ãƒãƒ£ãƒ¼',
        'äº‹æ•…ãƒ»ç½å®³',
        'æ²»å®‰ãƒ»çŠ¯ç½ª',
        'åŒ»ç™‚ãƒ»å¥åº·',
        'æ•™è‚²'
    ]
    
    ordered_categories = priority_cats + [
        cat for cat in sorted(categorized.keys())
        if cat not in priority_cats
    ]
    
    def select_news(limit_source: bool, enforce_category_limit: bool, categories: List[str]) -> None:
        nonlocal selected
        for cat in categories:
            items = categorized.get(cat, [])
            if not items:
                continue
            for news in items:
                if len(selected) >= target_count:
                    return
                if id(news) in selected_ids:
                    continue
                
                if enforce_category_limit:
                    limit = category_limits.get(cat, fallback_category_limit)
                    if category_counts[cat] >= limit:
                        continue
                
                source = news.get('_source', 'Unknown')
                if limit_source and source_usage[source] >= max_per_source_initial:
                    continue
                
                title_words = news.get('_title_words') or normalize_title_words(news.get('title', ''))
                if title_words and is_similar_title_words(title_words, selected_title_words):
                    continue
                
                topic_key = news.get('_topic_key')
                if topic_key:
                    topic_limit = 4
                    if topic_usage[topic_key] >= topic_limit:
                        topic_exceeded[topic_key] += 1
                        continue
                
                selected.append(news)
                selected_ids.add(id(news))
                if title_words:
                    selected_title_words.append(title_words)
                category_counts[cat] += 1
                source_usage[source] += 1
                if topic_key:
                    topic_usage[topic_key] += 1
                if len(selected) >= target_count:
                    return
    
    # 1st pass: respect category limits and per-source cap
    select_news(limit_source=True, enforce_category_limit=True, categories=ordered_categories)
    
    # 2nd pass: relax source cap but keep category limits
    if len(selected) < target_count:
        select_news(limit_source=False, enforce_category_limit=True, categories=ordered_categories)
    
    # Final pass: fill remaining slots without category limits
    if len(selected) < target_count:
        select_news(limit_source=False, enforce_category_limit=False, categories=ordered_categories)
    
    print(f"\nâœ… é¸æŠå®Œäº†: {len(selected)}ä»¶ï¼ˆç›®æ¨™: {target_count}ä»¶ï¼‰")
    
    selected_categories = defaultdict(int)
    for news in selected:
        category = news.get('category', 'æœªåˆ†é¡')
        selected_categories[category] += 1
    
    print("ğŸ“Š é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³:")
    for cat, count in sorted(selected_categories.items(), key=lambda x: (-x[1], x[0])):
        limit = category_limits.get(cat)
        if limit is not None:
            print(f"  {cat}: {count}ä»¶ï¼ˆä¸Šé™: {limit}ä»¶ï¼‰")
        else:
            print(f"  {cat}: {count}ä»¶")
    
    if topic_exceeded:
        print("\nâš ï¸  ãƒˆãƒ”ãƒƒã‚¯ä¸Šé™ã«ã‚ˆã‚Šé™¤å¤–ã•ã‚ŒãŸä»¶æ•°:")
        for topic, count in sorted(topic_exceeded.items(), key=lambda x: -x[1]):
            print(f"  {topic}: {count}ä»¶")
    
    return selected

if __name__ == "__main__":
    import sys
    import os
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_article.py <raw_news.json>")
        sys.exit(1)
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’HKTã«è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°HKTï¼‰
    os.environ['TZ'] = os.environ.get('TZ', 'Asia/Hong_Kong')
    
    news_file = sys.argv[1]
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’è¡¨ç¤º
    today = datetime.now(HKT).strftime('%Y-%m-%d')
    print(f"\nğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (HKT): {today}")
    print(f"ğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (æ—¥æœ¬èª): {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    print("=" * 60)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹äº‹å‰å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ
    news_data = preprocess_news(data['news'])
    
    print("=" * 60)
    
    # ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ‘ã‚¹ã®æ±ºå®šï¼ˆå„ªå…ˆé †ä½: ç’°å¢ƒå¤‰æ•°CONFIG_PATH > config.local.json > config.jsonï¼‰
    config_path = os.environ.get('CONFIG_PATH')
    if not config_path:
        if os.path.exists('config.local.json'):
            config_path = 'config.local.json'
        else:
            config_path = 'config.json'

    generator = GrokArticleGenerator(config_path)
    article = generator.generate_article(news_data)
    
    if article:
        # å¤©æ°—æƒ…å ±ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        weather_data = data.get('weather', None)
        saved_path = generator.save_article(article, weather_data)
        
        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥ä»˜ã‚’ç¢ºèª
        expected_date = datetime.now(HKT).strftime('%Y-%m-%d')
        file_date = saved_path.split('_')[-1].replace('.md', '')
        
        print(f"\nâœ… è¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_path}")
        print(f"ğŸ“… ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜: {file_date}")
        print(f"ğŸ“… æœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜: {expected_date}")
        
        if file_date != expected_date:
            print(f"âš ï¸  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜ãŒæœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜ã¨ä¸€è‡´ã—ã¾ã›ã‚“ï¼")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {file_date}, æœŸå¾…: {expected_date}")
        
        print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
        if weather_data:
            print(f"ğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
