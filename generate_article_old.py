#!/usr/bin/env python3
"""
Grok APIã‚’ä½¿ç”¨ã—ã¦é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ
â€» è¦ç´„ã‚„çŸ­ç¸®ã¯ä¸€åˆ‡è¡Œã‚ãšã€å…ƒã®æƒ…å ±ã‚’ãã®ã¾ã¾ç¿»è¨³ã™ã‚‹
"""

import requests
import json
from datetime import datetime, timedelta, timezone
from typing import List, Dict

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

class GrokArticleGenerator:
    def __init__(self, config_path: str = "config.json"):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # APIé¸æŠï¼ˆGemini â†’ Claude â†’ Grok ã®é †ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if 'gemini_api' in self.config and self.config['gemini_api'].get('api_key'):
            self.api_key = self.config['gemini_api']['api_key']
            self.api_url = self.config['gemini_api']['api_url']
            self.use_gemini = True
        elif 'claude_api' in self.config and self.config['claude_api'].get('api_key'):
            self.api_key = self.config['claude_api']['api_key']
            self.api_url = self.config['claude_api']['api_url']
            self.use_gemini = False
        else:
            # Grok APIã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨
            self.api_key = self.config['grok_api']['api_key']
            self.api_url = self.config['grok_api']['api_url']
            self.use_gemini = None
        
    def generate_article(self, news_data: List[Dict]) -> Dict:
        """Gemini/Claude/Grok APIã§æ—¥æœ¬èªè¨˜äº‹ã‚’ç”Ÿæˆ"""
        if self.use_gemini is True:
            api_name = "Google Gemini"
        elif self.use_gemini is False:
            api_name = "Claude API"
        else:
            api_name = "Grok API"
        print(f"\nğŸ¤– {api_name}ã§è¨˜äº‹ç”Ÿæˆä¸­...")
        print("=" * 60)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        news_text = self._format_news_for_prompt(news_data)
        
        # GPT-4 APIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt = """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚

ç¿»è¨³ãƒ«ãƒ¼ãƒ«ï¼š
- ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³
- è¦ç´„ã—ãªã„

å‡ºåŠ›å½¢å¼ï¼š
{
  "title": "æ¯æ—¥AIãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹(2025å¹´10æœˆ28æ—¥)",
  "lead": "",
  "body": "### ã‚¿ã‚¤ãƒˆãƒ«\\n\\næœ¬æ–‡\\n\\n**å¼•ç”¨å…ƒ**: ã‚½ãƒ¼ã‚¹, æ—¥ä»˜\\n**ãƒªãƒ³ã‚¯**: URL\\n**å‚™è€ƒ**: èª¬æ˜\\n\\n---\\n\\n### æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹...",
  "tags": "é¦™æ¸¯,åºƒæ±èª,å»£æ±è©±,ä¸­å›½èªãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
}"""

        user_prompt = f"""ä»¥ä¸‹ã¯{datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')}ã®é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’å…ƒã«ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªè¨˜äº‹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€æœ€é‡è¦ã€‘Full Contentã®å†…å®¹ã¯çµ¶å¯¾ã«çŸ­ç¸®ãƒ»è¦ç´„ã›ãšã€å…¨æ–‡ãã®ã¾ã¾æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã€ç¿»è¨³å¾¹åº•æŒ‡ç¤ºã€‘
- ä¸­å›½èªãƒ»åºƒæ±èªã®å›ºæœ‰åè©ã‚„åœ°åã¯å¿…ãšé©åˆ‡ãªæ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹
- æ„ŸæŸ“ç—‡åã¯æ­£ç¢ºãªæ—¥æœ¬èªåŒ»å­¦ç”¨èªã‚’ä½¿ç”¨ï¼š
  * ã€ŒåŸºå­”è‚¯é›…ç†±ã€â†’ã€Œãƒã‚¯ãƒ³ã‚°ãƒ‹ã‚¢ç†±ã€ï¼ˆæ³¨ï¼šã“ã‚Œã‚‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯é™¤å¤–ã•ã‚Œã‚‹ï¼‰
  * ã€Œç™»é©ç†±ã€â†’ã€Œãƒ‡ãƒ³ã‚°ç†±ã€
- åœ°åã¯æ¨™æº–çš„ãªã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ï¼š
  * ã€Œé³³å¾³é‚¨ã€â†’ã€Œãƒ•ã‚§ãƒ³ã‚¿ã‚¯æ‘ã€ï¼ˆåºƒæ±èªèª­ã¿ï¼‰
  * ã€Œè¡›ç”Ÿé˜²è­·ã‚»ãƒ³ã‚¿ãƒ¼ã€â†’ã€Œè¡›ç”Ÿé˜²è­·ã‚»ãƒ³ã‚¿ãƒ¼ã€ï¼ˆè·èƒ½åï¼‰
- å°‚é–€ç”¨èªã¯æ­£ç¢ºãªæ—¥æœ¬èªè¡¨è¨˜ã‚’ä½¿ç”¨ã—ã€åŒ»å­¦ãƒ»å…¬è¡†è¡›ç”Ÿç”¨èªã¯ç‰¹ã«æ³¨æ„
- å¼•ç”¨ã‚„ä¼šè©±éƒ¨åˆ†ã®ä¸­å›½èªã‚‚ã™ã¹ã¦æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹
- ã€ã€‘å†…ã®ä¸­å›½èªã‚‚ã™ã¹ã¦ç¿»è¨³ã™ã‚‹
- 1ã¤ã®è¨˜äº‹ã®å†…å®¹ã¯æœ€ä½500æ–‡å­—ä»¥ä¸Šã«ã™ã‚‹
- å…ƒã®æƒ…å ±ã‚’ã™ã¹ã¦å«ã‚ã‚‹
- å…·ä½“çš„ãªå›ºæœ‰åè©ã€æ•°å­—ã€æ—¥ä»˜ã‚’ã™ã¹ã¦å«ã‚ã‚‹

{news_text}

ä¸Šè¨˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é¦™æ¸¯é–¢é€£ã®ã‚‚ã®ã‚’20ä»¶é¸ã³ã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚20ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å¿…ãšä½œæˆã—ã¦ãã ã•ã„ã€‚
å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã€Œå†…å®¹ã€ã¯å…ƒã®Full Contentã‚’å…¨æ–‡ç¿»è¨³ã—ã¦ãã ã•ã„ï¼ˆçŸ­ç¸®ç¦æ­¢ï¼‰ã€‚

ã€æœ€é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šé‡è¤‡ã®æ’é™¤ã€‘
1. **åŒã˜äº‹ä»¶ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã«ã¤ã„ã¦ã€è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ï¼ˆSCMPã€RTHKã€Yahooç­‰ï¼‰ã‹ã‚‰å ±é“ã•ã‚Œã¦ã„ã‚‹å ´åˆã€æœ€ã‚‚è©³ç´°ãª1ã¤ã ã‘ã‚’é¸æŠã—ã¦ãã ã•ã„**
2. **ä¾‹ï¼šã€Œè¯æ‡‹ã‚¿ãƒ¯ãƒ¼ç«ç½ã€ãªã©ã€åŒã˜ç«ç½äº‹ä»¶ã¯1ã¤ã ã‘**
3. å„è¨˜äº‹ã¯ç•°ãªã‚‹ãƒˆãƒ”ãƒƒã‚¯ãƒ»äº‹ä»¶ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

ã€é‡è¦ã€‘åˆ©ç”¨å¯èƒ½ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰**ç•°ãªã‚‹äº‹ä»¶ãƒ»ãƒˆãƒ”ãƒƒã‚¯**ã‚’å¿…ãš20ä»¶é¸ã³ã€è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚20ä»¶ã‚’ç›®æ¨™ã«ã€ã§ãã‚‹ã ã‘å¤šãã®è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€é™¤å¤–ã™ã¹ãæ”¿æ²»é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘
ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯çµ¶å¯¾ã«é¸ã°ãªã„ã§ãã ã•ã„ï¼š
- 47äººäº‹ä»¶ã€47 persons caseã€democracy trial
- åˆ‘æœŸæº€äº†ã€prison releaseã€sentence completion
- æ°‘ä¸»æ´¾ã€democratic partyã€pro-democracy
- ç«‹æ³•ä¼šé¸æŒ™ã€legislative council electionã€legco election
- å›½å®¶å®‰å…¨å…¬ç½²ã€national security officeã€å›½å®‰æ³•
- jailed for conspiracy (æ”¿æ²»çš„äº‹ä»¶ã§ã®é€®æ•ãƒ»åˆ¤æ±º)
- 2019 protest related news

ã€å„ªå…ˆçš„ã«é¸ã¶ã¹ããƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘
æ”¿æ²»ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å„ªå…ˆçš„ã«é¸ã‚“ã§ãã ã•ã„ï¼š
- **èŠ¸èƒ½ãƒ»ã‚«ãƒ«ãƒãƒ£ãƒ¼ï¼ˆæœ€å„ªå…ˆï¼‰**: ä¿³å„ªã€æ­Œæ‰‹ã€ãƒ†ãƒ¬ãƒ“ç•ªçµ„ã€ç•ªçµ„çµ‚äº†ã€TVBã€ç”Ÿæ—¥ã€å©šç´—ã€åœæ’­ãªã©
- ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ: ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€IPOã€å¸‚å ´å‹•å‘
- ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼: AIã€ãƒ‡ã‚¸ã‚¿ãƒ«ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
- å¥åº·ãƒ»åŒ»ç™‚: æ„ŸæŸ“ç—‡ã€åŒ»ç™‚æŠ€è¡“
- ã‚¹ãƒãƒ¼ãƒ„
- ä¸å‹•ç”£
- æ•™è‚²
- ç¤¾ä¼šã‚¤ãƒ™ãƒ³ãƒˆ

**é‡è¦ãªé¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**:
1. æä¾›ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªã‚¹ãƒˆã‹ã‚‰ã€**èŠ¸èƒ½ãƒ»ã‚«ãƒ«ãƒãƒ£ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æœ€ä½3-5ä»¶ã¯å¿…ãšé¸ã‚“ã§ãã ã•ã„**
2. ä¿³å„ªåã€ç•ªçµ„åã€TVBé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ç©æ¥µçš„ã«é¸æŠã—ã¦ãã ã•ã„
3. æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯å®Œå…¨ã«ç„¡è¦–ã—ã¦ãã ã•ã„
4. ãƒ“ã‚¸ãƒã‚¹ã€å¥åº·ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚‚å¤šãé¸ã‚“ã§ãã ã•ã„

ã“ã‚Œã‚‰ã®æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ä¸€åˆ‡å–ã‚Šæ‰±ã‚ãšã€ç‰¹ã«èŠ¸èƒ½ãƒ»ã‚«ãƒ«ãƒãƒ£ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æœ€å„ªå…ˆã«ã€ãƒ“ã‚¸ãƒã‚¹ã€ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã€å¥åº·ã€æ–‡åŒ–ã€ã‚¹ãƒãƒ¼ãƒ„ã€ä¸å‹•ç”£ã€æ•™è‚²ãªã©ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"""

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆGemini/Claude/Grokå¯¾å¿œï¼‰
        if self.use_gemini is True:
            # Gemini API
            headers = {
                "Content-Type": "application/json"
            }
            # APIã‚­ãƒ¼ã‚’URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«è¿½åŠ 
            api_url_with_key = f"{self.api_url}?key={self.api_key}"
            payload = {
                "contents": [{
                    "parts": [{
                        "text": f"{system_prompt}\n\n{user_prompt}"
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.1,
                    "maxOutputTokens": 32000
                }
            }
        else:
            # Claude/Grok API
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            if self.use_gemini is False:  # Claude API
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "messages": [
                        {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 32000
                }
            else:  # Grok API
                payload = {
                    "model": "grok-beta",
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 32000
                }
        
        if self.use_gemini is True:
            print("ğŸ“¤ Google Geminiã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        elif self.use_gemini is False:
            print("ğŸ“¤ Claude APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        else:
            print("ğŸ“¤ Grok APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        
        try:
            # Gemini APIã®å ´åˆã¯URLã«APIã‚­ãƒ¼ã‚’è¿½åŠ 
            url = api_url_with_key if self.use_gemini is True else self.api_url
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if self.use_gemini is True:
                    # Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    content = result['candidates'][0]['content']['parts'][0]['text']
                else:
                    # Claude/Grok APIãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    if self.use_gemini is False:  # Claude API
                        content = result['content'][0]['text']
                    else:  # Grok API
                        content = result['choices'][0]['message']['content']
                
                print("âœ… è¨˜äº‹ç”Ÿæˆå®Œäº†")
                
                # JSONãƒ‘ãƒ¼ã‚¹
                try:
                    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
                    if content.strip().startswith('```'):
                        lines = content.split('\n')
                        # æœ€åˆã®```è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã€æœ€å¾Œã®```è¡Œã‚‚ã‚¹ã‚­ãƒƒãƒ—
                        start_line = 1 if len(lines) > 1 else 0
                        end_line = len(lines) - 1 if len(lines) > 1 and lines[-1].strip() == '```' else len(lines)
                        content = '\n'.join(lines[start_line:end_line]).strip()
                    
                    # JSONã¨ã—ã¦æœ€åˆã® { ã‹ã‚‰æœ€å¾Œã® } ã‚’æŠ½å‡º
                    if '{' in content and '}' in content:
                        start = content.find('{')
                        end = content.rfind('}') + 1
                        json_str = content[start:end]
                        
                        # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
                        json_str = ''.join(char for char in json_str if ord(char) >= 32 or char in '\n\t\r')
                        
                        article = json.loads(json_str)
                        return article
                    
                    raise json.JSONDecodeError("No JSON object found", content, 0)
                        
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                    print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {content[:500]}...")
                    print("   ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è¨˜äº‹ã‚’æŠ½å‡ºä¸­...")
                    
                    # JSONå½¢å¼ã‚’è«¦ã‚ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
                    # æœ€åˆã«ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ã—ãŸ content ã‚’ä½¿ç”¨
                    lines = content.split('\n')
                    title_line = [l for l in lines if 'title' in l.lower() and ':' in l]
                    if title_line:
                        title = title_line[0].split(':', 1)[1].strip().strip('"').strip(',').strip('"')
                    else:
                        title = f"æ¯æ—¥AIãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹({datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')})"
                    
                    # bodyã‹ã‚‰Markdownè¨˜äº‹ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰
                    body_start = content.find('"body":')
                    if body_start != -1:
                        # "body": ã®å¾Œã®æœ€åˆã® " ã‚’æ¢ã™
                        quote_start = content.find('"', body_start + 7)
                        if quote_start != -1:
                            # æ¬¡ã® " ã‚’æ¢ã™ï¼ˆã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚ŒãŸ " ã¯è€ƒæ…®ï¼‰
                            body_end = quote_start + 1
                            while body_end < len(content):
                                if content[body_end] == '"' and content[body_end - 1] != '\\':
                                    break
                                body_end += 1
                            
                            if body_end > quote_start + 1:
                                body = content[quote_start + 1:body_end]
                                # JSONã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è§£é™¤
                                body = body.replace('\\n', '\n').replace('\\"', '"').replace('\\/', '/')
                            else:
                                body = ""
                        else:
                            body = ""
                    else:
                        body = ""
                    
                    return {
                        "title": title,
                        "lead": "",
                        "body": body,
                        "tags": "é¦™æ¸¯,åºƒæ±èª,å»£æ±è©±,ä¸­å›½èªãƒ‹ãƒ¥ãƒ¼ã‚¹,æœ€æ–°,æƒ…å ±,ã‚¢ã‚¸ã‚¢"
                    }
            else:
                print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                print(f"   è©³ç´°: {response.text}")
                
                # Gemini APIãŒåœ°åŸŸåˆ¶é™ã®å ´åˆã¯Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if (response.status_code == 403 or response.status_code == 400) and self.use_gemini is True:
                    print("ğŸ”„ Gemini APIåœ°åŸŸåˆ¶é™ã®ãŸã‚Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_grok(news_data)
                
                # Grok APIãŒã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®å ´åˆã¯Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if response.status_code == 429 and self.use_gemini is None:
                    print("ğŸ”„ Grok APIã‚¯ãƒ¬ã‚¸ãƒƒãƒˆåˆ‡ã‚Œã®ãŸã‚Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯...")
                    return self._fallback_to_claude(news_data)
                
                return None
                
        except Exception as e:
            print(f"âŒ ä¾‹å¤–ç™ºç”Ÿ: {e}")
            return None
    
    def _fallback_to_grok(self, news_data: List[Dict]) -> Dict:
        """Grok APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Grok APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Grok APIã®è¨­å®š
        self.api_key = self.config['grok_api']['api_key']
        self.api_url = self.config['grok_api']['api_url']
        self.use_gemini = None
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _fallback_to_claude(self, news_data: List[Dict]) -> Dict:
        """Claude APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        print("ğŸ”„ Claude APIã§è¨˜äº‹ç”Ÿæˆä¸­...")
        
        # Claude APIã®è¨­å®š
        self.api_key = self.config['claude_api']['api_key']
        self.api_url = self.config['claude_api']['api_url']
        self.use_gemini = False
        
        # å…ƒã®generate_articleãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†å¸°å‘¼ã³å‡ºã—
        return self.generate_article(news_data)
    
    def _format_news_for_prompt(self, news_data: List[Dict]) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«æ•´å½¢"""
        formatted = []
        for i, news in enumerate(news_data, 1):  # å…¨ä»¶ä½¿ç”¨
            # full_contentãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°description
            content = news.get('full_content', news.get('description', 'N/A'))
            formatted.append(f"""
ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹{i}ã€‘
Title: {news.get('title', 'N/A')}
Full Content: {content}
Source: {news.get('source', 'N/A')}
URL: {news.get('url', 'N/A')}
Published: {news.get('published_at', 'N/A')}
""")
        return "\n".join(formatted)
    
    def format_weather_info(self, weather_data: Dict) -> str:
        """å¤©æ°—æƒ…å ±ã‚’Markdownå½¢å¼ã«æ•´å½¢"""
        if not weather_data:
            return ""
        
        import re
        
        def clean_weather_text(text: str) -> str:
            """å¤©æ°—æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
            if not text:
                return ""
            # HTMLã‚¿ã‚°ã‚’æ”¹è¡Œã«å¤‰æ›
            text = re.sub(r'<br\s*/?>', '\n', text)
            # ä»–ã®HTMLã‚¿ã‚°ã‚’é™¤å»
            text = re.sub(r'<[^>]+>', '', text)
            # å„è¡Œã”ã¨ã«å‡¦ç†
            lines = text.split('\n')
            cleaned_lines = []
            for line in lines:
                # è¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
                line = re.sub(r'\s+', ' ', line).strip()
                if line:  # ç©ºè¡Œã¯é™¤å¤–
                    cleaned_lines.append(line)
            # é©åˆ‡ãªæ”¹è¡Œã§çµåˆ
            return ' '.join(cleaned_lines)
        
        weather_section = "## æœ¬æ—¥ã®é¦™æ¸¯ã®å¤©æ°—"
        
        # å¤©æ°—è­¦å ±
        if 'weather_warning' in weather_data:
            warning = weather_data['weather_warning']
            title = warning.get('title', 'N/A')
            desc = clean_weather_text(warning.get('description', ''))
            
            # å¤©æ°—è­¦å ±ã‚’å®Œå…¨ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆã€é…·ç†±å¤©æ°£è­¦å‘Šãªã©ï¼‰
            if title and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in title and "é…·ç†±å¤©æ°£è­¦å‘Š" not in title and "ç™¼å‡º" not in title:
                weather_section += f"\n### å¤©æ°—è­¦å ±{title}"
                if desc and "ç¾æ™‚ä¸¦ç„¡è­¦å‘Šç”Ÿæ•ˆ" not in desc and "é…·ç†±å¤©æ°£è­¦å‘Š" not in desc:
                    weather_section += f"{desc}"
        
        # åœ°åŸŸå¤©æ°—äºˆå ±ã®ã¿è¡¨ç¤º
        if 'weather_forecast' in weather_data:
            forecast = weather_data['weather_forecast']
            title = forecast.get('title', 'N/A')
            desc = clean_weather_text(forecast.get('description', ''))
            
            # å¤©æ°—æƒ…å ±ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            translated_title = self._translate_weather_text(title)
            translated_desc = self._translate_weather_text(desc)
            weather_section += f"\n### å¤©æ°—äºˆå ±\n{translated_title}\n{translated_desc}\n\n**å¼•ç”¨å…ƒ**: é¦™æ¸¯å¤©æ–‡å°"
        
        return weather_section
    
    def _translate_weather_text(self, text: str) -> str:
        """å¤©æ°—æƒ…å ±ã®åºƒæ±èªã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
        if not text:
            return ""
        
        # åºƒæ±èªã®å¤©æ°—æƒ…å ±ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã™ã‚‹è¾æ›¸
        weather_translations = {
            "é¦™æ¸¯å¤©æ–‡å°æ–¼": "é¦™æ¸¯å¤©æ–‡å°ãŒ",
            "ç™¼å‡ºä¹‹å¤©æ°£å ±å‘Š": "ç™ºè¡¨ã—ãŸå¤©æ°—å ±å‘Š",
            "ä¸€è‚¡æ¸…å‹çš„åæ±æ°£æµæ­£å½±éŸ¿å»£æ±æ²¿å²¸": "æ¸…æ¶¼ãªæ±é¢¨ãŒåºƒæ±æ²¿å²¸ã«å½±éŸ¿ã—ã¦ã„ã¾ã™",
            "æ­¤å¤–": "ã¾ãŸ",
            "ä¸€é“é›²å¸¶æ­£è¦†è“‹è¯å—æ²¿å²¸": "é›²ãŒè¯å—æ²¿å²¸ã‚’è¦†ã£ã¦ã„ã¾ã™",
            "æœ¬æ¸¯åœ°å€ä»Šæ—¥å¤©æ°£é æ¸¬": "é¦™æ¸¯åœ°åŒºã®ä»Šæ—¥ã®å¤©æ°—äºˆå ±",
            "å¤§è‡´å¤šé›²": "æ¦‚ã­æ›‡ã‚Š",
            "æœ‰ä¸€å…©é™£å¾®é›¨": "æ™‚ã€…å°é›¨",
            "æ—¥é–“çŸ­æš«æ™‚é–“æœ‰é™½å…‰": "æ—¥ä¸­ã¯çŸ­æ™‚é–“æ™´ã‚Œé–“",
            "å¹å’Œç·©è‡³æ¸…å‹æ±è‡³æ±åŒ—é¢¨": "æ±ã‹ã‚‰åŒ—æ±ã®é¢¨ãŒã‚„ã‚„å¼·ãå¹ã",
            "å±•æœ›": "ä»Šå¾Œã®è¦‹é€šã—",
            "æ˜æ—¥æ—¥é–“ç‚ç†±": "æ˜æ—¥ã®æ—¥ä¸­ã¯æš‘ã„",
            "é€±æœ«æœŸé–“æ°£æº«ç¨ç‚ºä¸‹é™": "é€±æœ«ã¯æ°—æ¸©ãŒã‚„ã‚„ä¸‹ãŒã‚Š",
            "å¤©æ°£ä¹¾ç‡¥": "å¤©æ°—ã¯ä¹¾ç‡¥",
            "ä¸‹é€±åˆé¢¨å‹¢é —å¤§": "æ¥é€±åˆã‚ã¯é¢¨ãŒå¼·ã„"
        }
        
        # ç¿»è¨³ã‚’é©ç”¨
        translated_text = text
        for chinese, japanese in weather_translations.items():
            translated_text = translated_text.replace(chinese, japanese)
        
        return translated_text
    
    def _generate_cantonese_section(self) -> str:
        """åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã®å®šå‹æ–‡ã‚’ç”Ÿæˆ"""
        return """## åºƒæ±èªå­¦ç¿’è€…å‘ã‘æƒ…å ±

åºƒæ±èªå­¦ç¿’è€…å‘ã‘ã«LINEãŒè‰¯ã„ã€ä¾¿åˆ©ã¨ã„ã†æ–¹ã‚‚ã„ã‚‹ã§ã—ã‚‡ã†ã‹ã‚‰ã€ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚‚ã‚ã‚Šã¾ã™ã®ã§ã“ã¡ã‚‰ã”ç™»éŒ²ã—ã¦ã‹ã‚‰ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ç§˜æ›¸ã®ãƒªãƒ¼ã•ã‚“ãŒåºƒæ±èªã«ã¤ã„ã¦ãªã‚“ã§ã‚‚å›ç­”ã—ã¦ãã‚Œã¾ã™ã®ã§ãœã²ä½¿ã£ã¦ã¿ã¦ãã ã•ã„

(ä»Šç¾åœ¨400åä»¥ä¸Šã®æ–¹ã«ç™»éŒ²ã—ã¦ã„ãŸã ã„ã¦ãŠã‚Šã¾ã™ï¼‰

[ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”ŸLINEå…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ](https://line.me/R/ti/p/@298mwivr)

[![ã‚¹ãƒ©ãƒ³ã‚°å…ˆç”Ÿå…¬å¼LINE](../shared/line-img1.jpg)](https://line.me/R/ti/p/@298mwivr)

[![LINEã§ãŠå•åˆã›](../shared/line-qr.png)](https://line.me/R/ti/p/@298mwivr)

## åºƒæ±èª| åºƒæ±èªè¶…åŸºç¤ã€€è¶…ç°¡å˜ï¼åˆã‚ã¦ã®åºƒæ±èªã€Œ9å£°6èª¿ã€

@https://youtu.be/RAWZAJUrvOU?si=WafOkQixyLiwMhUW"""
    
    def remove_advertisement_content(self, body: str) -> str:
        """è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šãƒ»å®£ä¼ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»"""
        import re
        
        # åºƒå‘Šãƒ»å®£ä¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        ad_patterns = [
            r'æœ€æ–°ã®å‹•ç”»ç´¹ä»‹ï¼š.*?ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'TOPick.*?ãƒãƒ£ãƒ³ãƒãƒ«.*?ãƒ•ã‚©ãƒ­ãƒ¼.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ç„¡æ–™ã®.*?ä¼šå“¡.*?ä»Šã™ã.*?ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰',
            r'ä¼šå“¡æ–°è¦å‹Ÿé›†.*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š',
            r'https://whatsapp\.com/channel/.*?',
            r'https://onelink\.to/.*?',
            r'https://event\.hket\.com/.*?',
            r'ã€è©³ç´°ã¨ç”³ã—è¾¼ã¿ã€‘',
            r'ç”³ã—è¾¼ã¿å—ä»˜ä¸­',
            r'ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦.*?è¦‹é€ƒã•ãªã„ã§ãã ã•ã„',
            r'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼š.*?',
            r'ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ.*?è©³ç´°ï¼š.*?',
            r'ğŸ””.*?ãƒ•ã‚©ãƒ­ãƒ¼',
            r'ç„¡æ–™.*?ä¼šå“¡.*?å‚åŠ ã—ã¾ã—ã‚‡ã†',
            r'æ–°è¦ä¼šå“¡ç™»éŒ².*?ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ'
        ]
        
        # åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é™¤å»
        cleaned_body = body
        for pattern in ad_patterns:
            cleaned_body = re.sub(pattern, '', cleaned_body, flags=re.DOTALL | re.IGNORECASE)
        
        # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«
        cleaned_body = re.sub(r'\n{3,}', '\n\n', cleaned_body)
        
        # å…ˆé ­ãƒ»æœ«å°¾ã®ç©ºè¡Œã‚’é™¤å»
        cleaned_body = cleaned_body.strip()
        
        return cleaned_body
    
    def remove_duplicate_articles(self, body: str) -> str:
        """ç”Ÿæˆã•ã‚ŒãŸè¨˜äº‹æœ¬æ–‡ã‹ã‚‰é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–"""
        import re
        
        # ### ã§å§‹ã¾ã‚‹è¨˜äº‹ã‚’åˆ†å‰²
        articles = re.split(r'\n### ', body)
        
        # æœ€åˆã®è¦ç´ ã¯ç©ºã¾ãŸã¯å¤©æ°—æƒ…å ±ãªã®ã§ãã®ã¾ã¾ä¿æŒ
        if articles:
            result = [articles[0]]
            seen_titles = set()
            duplicate_count = 0
            
            for article in articles[1:]:
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®è¡Œï¼‰
                lines = article.split('\n', 1)
                if len(lines) > 0:
                    title = lines[0].strip()
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã®æ­£è¦åŒ–ï¼ˆã‚ˆã‚Šå³å¯†ãªé‡è¤‡ã®ã¿é™¤å¤–ï¼‰
                    normalized_title = re.sub(r'[^\w\s]', '', title.lower())
                    # çŸ­ã™ãã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã¯é‡è¤‡ãƒã‚§ãƒƒã‚¯å¯¾è±¡å¤–
                    if len(normalized_title) < 10:
                        result.append(article)
                        continue
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨ä¸€è‡´ã®ã¿ï¼‰
                    if normalized_title not in seen_titles:
                        seen_titles.add(normalized_title)
                        result.append(article)
                    else:
                        duplicate_count += 1
            
            if duplicate_count > 0:
                print(f"ğŸ”„ é‡è¤‡è¨˜äº‹ã‚’é™¤å¤–: {duplicate_count}ä»¶")
            
            # å†çµåˆï¼ˆè¦‹å‡ºã—ã®å‰ã«ç©ºè¡Œã‚’å…¥ã‚Œã‚‹ï¼‰
            if len(result) > 1:
                return result[0] + '\n\n### ' + '\n\n### '.join(result[1:])
            else:
                return result[0]
        
        return body
    
    def save_article(self, article: Dict, weather_data: Dict = None, output_path: str = None) -> str:
        """ç”Ÿæˆã—ãŸè¨˜äº‹ã‚’Markdownå½¢å¼ã§ä¿å­˜"""
        if output_path is None:
            timestamp = datetime.now(HKT).strftime('%Y-%m-%d')
            output_path = f"daily-articles/hongkong-news_{timestamp}.md"
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åºƒå‘Šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¨é‡è¤‡ã‚’é™¤å¤–
        article['body'] = self.remove_advertisement_content(article['body'])
        article['body'] = self.remove_duplicate_articles(article['body'])
        
        # è¨˜äº‹æœ¬æ–‡ã‹ã‚‰åŒºåˆ‡ã‚Šç·šã‚’å‰Šé™¤ã—ã€è¦‹å‡ºã—å‰ã«ç©ºè¡Œã‚’è¿½åŠ 
        import re
        article['body'] = re.sub(r'\n+---\n+', '\n', article['body'])
        article['body'] = re.sub(r'\n{3,}', '\n\n', article['body'])
        # è¦‹å‡ºã—ã®å‰ã«å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹
        article['body'] = re.sub(r'([^\n])\n(###)', r'\1\n\n\2', article['body'])
        
        # å¤©æ°—æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ
        weather_section = self.format_weather_info(weather_data) if weather_data else ""
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’çµ„ã¿ç«‹ã¦ï¼ˆç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯æ”¹è¡Œã‚’æŒŸã¾ãªã„ï¼‰
        content_parts = []
        if weather_section:
            content_parts.append(weather_section)
        if article['lead']:
            content_parts.append(article['lead'])
        content_parts.append(article['body'])
        
        # Markdownç”Ÿæˆ
        content_str = '\n\n'.join(content_parts)
        # bodyã®æœ€åˆã«æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ï¼ˆ1è¡Œç›®ãŒç©ºè¡Œã«ãªã‚Šã€ã“ã“ã«ç›®æ¬¡ã‚’æŒ¿å…¥ï¼‰
        markdown = f"""# {article['title']}

{content_str}
---
**ã‚¿ã‚°**: {article['tags']}
**ç”Ÿæˆæ—¥æ™‚**: {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown)
        
        print(f"ğŸ’¾ è¨˜äº‹ã‚’ä¿å­˜: {output_path}")
        return output_path

def preprocess_news(news_list):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ"""
    import re
    import os
    from collections import defaultdict
    from datetime import datetime, timedelta
    
    # 0. éå»ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¢å‡ºãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
    past_urls = set()
    past_titles = []
    
    # éå»3æ—¥åˆ†ã®è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
    for days_ago in range(1, 4):
        past_date = datetime.now(HKT) - timedelta(days=days_ago)
        past_file = f"daily-articles/hongkong-news_{past_date.strftime('%Y-%m-%d')}.md"
        
        if os.path.exists(past_file):
            print(f"ğŸ“‚ éå»è¨˜äº‹ãƒã‚§ãƒƒã‚¯: {past_file}")
            try:
                with open(past_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # URLã‚’æŠ½å‡ºï¼ˆ**ãƒªãƒ³ã‚¯**: ã®å¾Œã®URLï¼‰
                    url_matches = re.findall(r'\*\*ãƒªãƒ³ã‚¯\*\*:\s*(https?://[^\s]+)', content)
                    past_urls.update(url_matches)
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆ### ã®å¾Œã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰
                    title_matches = re.findall(r'^### (.+)$', content, re.MULTILINE)
                    # å¤©æ°—äºˆå ±ã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    past_titles.extend([t for t in title_matches if 'å¤©æ°—' not in t and 'weather' not in t.lower()])
                    
                print(f"  âœ“ æ—¢å‡ºURL: {len(url_matches)}ä»¶ã€æ—¢å‡ºã‚¿ã‚¤ãƒˆãƒ«: {len([t for t in title_matches if 'å¤©æ°—' not in t])}ä»¶")
            except Exception as e:
                print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    if past_urls:
        print(f"ğŸ” éå»è¨˜äº‹ã‹ã‚‰åˆè¨ˆ {len(past_urls)} ä»¶ã®URLã¨ {len(past_titles)} ä»¶ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º")
    
    # éå»è¨˜äº‹ã¨ã®é‡è¤‡ã‚’é™¤å¤–
    filtered_news = []
    duplicate_count = 0
    
    for news in news_list:
        url = news.get('url', '')
        title = news.get('title', '')
        description = news.get('description', '')
        
        # å¤©æ°—é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
        weather_keywords = ['æ°—æ¸©', 'å¤©æ°—', 'å¤©æ–‡å°', 'æ°—è±¡', 'å¤©å€™', 'temperature', 'weather', 'observatory', 'forecast', 'â„ƒ', 'åº¦']
        if any(keyword in title.lower() or keyword in title for keyword in weather_keywords):
            duplicate_count += 1
            continue
        
        # æ”¿æ²»é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–
        political_keywords = [
            '47äºº', '47 persons', '47 activists', 'democracy trial',
            'åˆ‘æœŸæº€äº†', 'prison term', 'sentence completion', 'prison release',
            'æ°‘ä¸»æ´¾', 'democratic', 'democrats', 'pro-democracy',
            'ç«‹æ³•ä¼šé¸æŒ™', 'legislative council election', 'legco election',
            'å›½å®¶å®‰å…¨å…¬ç½²', 'national security office', 'nsa', 'nsf', 'national security law',
            'å›½å®‰æ³•', 'å›½å®¶å®‰å…¨æ³•', 'å›½å®‰å…¬ç½²'
        ]
        text_to_check = (title + ' ' + description + ' ' + news.get('full_content', '')).lower()
        if any(keyword.lower() in text_to_check for keyword in political_keywords):
            duplicate_count += 1
            continue
        
        # URLã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if url in past_urls:
            duplicate_count += 1
            continue
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
        is_similar = False
        for past_title in past_titles:
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦æ¯”è¼ƒ
            def normalize_title(t):
                t = t.lower()
                t = re.sub(r'[^\w\s]', '', t)
                return set(t.split())
            
            title_words = normalize_title(title)
            past_title_words = normalize_title(past_title)
            
            # å…±é€šå˜èªã‚’ãƒã‚§ãƒƒã‚¯
            common_words = title_words & past_title_words
            
            # 3å˜èªä»¥ä¸Šå…±é€š ã‹ã¤ å…±é€šç‡ãŒ70%ä»¥ä¸Šãªã‚‰é‡è¤‡ã¨ã¿ãªã™
            if len(common_words) >= 3:
                similarity = len(common_words) / max(len(title_words), len(past_title_words), 1)
                if similarity >= 0.7:
                    is_similar = True
                    break
        
        if is_similar:
            duplicate_count += 1
            continue
        
        filtered_news.append(news)
    
    if duplicate_count > 0:
        print(f"ğŸš« éå»è¨˜äº‹ã¨ã®é‡è¤‡é™¤å¤–: {duplicate_count}ä»¶")
    
    print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(news_list)} â†’ {len(filtered_news)}ä»¶")
    
    # 1. ã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã«ã‚ˆã‚‹é‡è¤‡é™¤å¤–ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    unique_news = []
    seen_titles = []
    
    for news in filtered_news:
        title = news['title']
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å˜èªã«åˆ†å‰²ã—ã¦æ­£è¦åŒ–
        def extract_keywords(text):
            # è¨˜å·ã‚’é™¤å»ã—ã¦å˜èªã‚’æŠ½å‡º
            words = re.sub(r'[^\w\s]', ' ', text).split()
            # 2æ–‡å­—ä»¥ä¸Šã®å˜èªã®ã¿ï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤ãï¼‰
            stop_words = {'ã®', 'ã«', 'ã‚’', 'ã¯', 'ãŒ', 'ã¨', 'ã§', 'ã‚„', 'ã‚‚', 'ã‹ã‚‰', 'ã¾ã§', 'a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or'}
            keywords = [w.lower() for w in words if len(w) >= 2 and w.lower() not in stop_words]
            return set(keywords)
        
        current_keywords = extract_keywords(title)
        
        # æ—¢å­˜ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¯”è¼ƒ
        is_duplicate = False
        for seen_title in seen_titles:
            seen_keywords = extract_keywords(seen_title)
            
            # å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‰²åˆã‚’è¨ˆç®—
            if len(current_keywords) > 0 and len(seen_keywords) > 0:
                common = current_keywords & seen_keywords
                
                # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ3æ–‡å­—ä»¥ä¸Šï¼‰ã‚’é‡è¦–
                current_major = {k for k in current_keywords if len(k) >= 3}
                seen_major = {k for k in seen_keywords if len(k) >= 3}
                common_major = current_major & seen_major
                
                # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ2ã¤ä»¥ä¸Šä¸€è‡´ã€ã‹ã¤å…¨ä½“ã®é¡ä¼¼åº¦ãŒ70%ä»¥ä¸Šãªã‚‰é‡è¤‡
                if len(common_major) >= 2:
                    similarity = len(common) / min(len(current_keywords), len(seen_keywords))
                    if similarity >= 0.7:
                        is_duplicate = True
                        break
        
        if not is_duplicate:
            seen_titles.append(title)
            unique_news.append(news)
    
    print(f"ğŸ“Š åŒæ—¥å†…é‡è¤‡é™¤å¤–: {len(filtered_news)} â†’ {len(unique_news)}ä»¶")
    
    # 2. ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡
    categorized = defaultdict(list)
    
    for news in unique_news:
        title = news['title'].lower()
        desc = news.get('description', '').lower()
        text = title + ' ' + desc
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¤å®š
        if any(k in text for k in ['business', 'economy', 'finance', 'market', 'stock', 'trade', 'yuan', 'dollar', 'ç¶“æ¿Ÿ', 'è²¡ç¶“', 'è‚¡', 'è²¿æ˜“']):
            cat = 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ'
        elif any(k in text for k in ['property', 'housing', 'home', 'flat', 'homebuyer', 'æ¨“', 'æˆ¿å±‹', 'ç‰©æ¥­']):
            cat = 'ä¸å‹•ç”£'
        elif any(k in text for k in ['tech', 'technology', 'ai', 'digital', 'robot', 'ç§‘æŠ€', 'æ©Ÿæ¢°äºº']):
            cat = 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼'
        elif any(k in text for k in ['health', 'medical', 'hospital', 'doctor', 'é†«ç™‚', 'å¥åº·', 'é†«é™¢']):
            cat = 'åŒ»ç™‚ãƒ»å¥åº·'
        elif any(k in text for k in ['education', 'university', 'school', 'student', 'hostel', 'æ•™è‚²', 'å¤§å­¸', 'å­¸ç”Ÿ']):
            cat = 'æ•™è‚²'
        elif any(k in text for k in ['art', 'culture', 'entertainment', 'exhibition', 'drama', 'æ–‡åŒ–', 'è—è¡“', 'å±•è¦½', 'æ¼”å‡º', 
                                      'tvb', 'ç”Ÿæ—¥', 'å©šç´—', 'åœæ’­', 'ç¯€ç›®', 'é›»è¦–', 'æ˜æ˜Ÿ', 'æ¼”å“¡', 'ç”Ÿæ—¥', 'å°ç›¤']):
            cat = 'ã‚«ãƒ«ãƒãƒ£ãƒ¼'
        elif any(k in text for k in ['weather', 'storm', 'typhoon', 'å¤©æ°£', 'é¢¨æš´', 'é¢±é¢¨']):
            cat = 'å¤©æ°—'
        elif any(k in text for k in ['traffic', 'transport', 'car', 'road', 'äº¤é€š', 'é“è·¯', 'è»Š']):
            cat = 'äº¤é€š'
        elif any(k in text for k in ['police', 'arrest', 'crime', 'vice', 'è­¦', 'æ‹˜æ•', 'ç½ªæ¡ˆ']):
            cat = 'æ²»å®‰ãƒ»çŠ¯ç½ª'
        elif any(k in text for k in ['fire', 'dead', 'accident', 'ç«ç½', 'æ­»äº¡', 'æ„å¤–']):
            cat = 'äº‹æ•…ãƒ»ç½å®³'
        elif any(k in text for k in ['government', 'policy', 'election', 'official', 'æ”¿åºœ', 'æ”¿ç­–', 'é¸èˆ‰', 'å®˜å“¡']):
            cat = 'æ”¿æ²»ãƒ»è¡Œæ”¿'
        else:
            cat = 'ç¤¾ä¼šãƒ»ãã®ä»–'
        
        categorized[cat].append(news)
    
    print(f"\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ä»¶æ•°:")
    for cat, items in sorted(categorized.items(), key=lambda x: -len(x[1])):
        print(f"  {cat}: {len(items)}ä»¶")
    
    # 3. ãƒãƒ©ãƒ³ã‚¹é¸æŠï¼ˆå„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦20-25ä»¶é¸æŠï¼‰
    selected = []
    target_count = 22  # 20-25ä»¶ã®ä¸­å¤®å€¤ï¼ˆAPIåˆ¶é™ã‚’è€ƒæ…®ï¼‰
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼ã”ã¨ã®å„ªå…ˆé †ä½ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šé †ï¼‰
    priority_cats = [
        'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ',      # 1ä½: 46ä»¶
        'ç¤¾ä¼šãƒ»ãã®ä»–',        # 2ä½: 19ä»¶  
        'ã‚«ãƒ«ãƒãƒ£ãƒ¼',          # 3ä½: 15ä»¶
        'ä¸å‹•ç”£',             # 4ä½: 13ä»¶
        'æ”¿æ²»ãƒ»è¡Œæ”¿',          # 5ä½: 8ä»¶
        'åŒ»ç™‚ãƒ»å¥åº·',          # 6ä½: 3ä»¶
        'æ²»å®‰ãƒ»çŠ¯ç½ª',          # 7ä½: 6ä»¶
        'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼',        # 8ä½: 76ä»¶
        'äº‹æ•…ãƒ»ç½å®³',          # 9ä½: 1ä»¶
        'äº¤é€š'                # 10ä½: 1ä»¶
    ]
    
    # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰å„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦é¸æŠ
    for cat in priority_cats:
        if cat in categorized and categorized[cat]:
            # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æœ€å¤§ä½•ä»¶å–ã‚‹ã‹ã‚’è¨ˆç®—ï¼ˆAPIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦èª¿æ•´ï¼‰
            if cat == 'ãƒ“ã‚¸ãƒã‚¹ãƒ»çµŒæ¸ˆ':
                max_count = min(5, len(categorized[cat]))  # 1ä½: 5ä»¶
            elif cat == 'ç¤¾ä¼šãƒ»ãã®ä»–':
                max_count = min(4, len(categorized[cat]))  # 2ä½: 4ä»¶
            elif cat == 'ã‚«ãƒ«ãƒãƒ£ãƒ¼':
                max_count = min(3, len(categorized[cat]))  # 3ä½: 3ä»¶
            elif cat == 'ä¸å‹•ç”£':
                max_count = min(3, len(categorized[cat]))  # 4ä½: 3ä»¶
            elif cat == 'æ”¿æ²»ãƒ»è¡Œæ”¿':
                max_count = min(2, len(categorized[cat]))  # 5ä½: 2ä»¶
            elif cat == 'åŒ»ç™‚ãƒ»å¥åº·':
                max_count = min(2, len(categorized[cat]))  # 6ä½: 2ä»¶
            elif cat == 'æ²»å®‰ãƒ»çŠ¯ç½ª':
                max_count = min(2, len(categorized[cat]))  # 7ä½: 2ä»¶
            elif cat == 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼':
                max_count = min(1, len(categorized[cat]))  # 8ä½: 1ä»¶
            else:
                max_count = min(1, len(categorized[cat]))  # 9-10ä½: 1ä»¶
            
            # é¸æŠ
            for i in range(max_count):
                if categorized[cat] and len(selected) < target_count:
                    selected.append(categorized[cat].pop(0))
            
            if len(selected) >= target_count:
                break
    
    # ã¾ã è¶³ã‚Šãªã„å ´åˆã¯æ®‹ã‚Šã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰è¿½åŠ 
    if len(selected) < target_count:
        for cat in priority_cats:
            if cat in categorized and categorized[cat]:
                while categorized[cat] and len(selected) < target_count:
                    selected.append(categorized[cat].pop(0))
                if len(selected) >= target_count:
                    break
    
    print(f"\nâœ… é¸æŠå®Œäº†: {len(selected)}ä»¶ï¼ˆå„ªå…ˆé †ä½èª¿æ•´æ¸ˆã¿ï¼‰")
    
    # é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³ã‚’è¡¨ç¤º
    selected_categories = defaultdict(int)
    for news in selected:
        category = news.get('category', 'æœªåˆ†é¡')
        selected_categories[category] += 1
    
    print("ğŸ“Š é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³:")
    for cat in priority_cats:
        if cat in selected_categories:
            print(f"  {cat}: {selected_categories[cat]}ä»¶")
    
    return selected

if __name__ == "__main__":
    import sys
    import os
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python generate_article.py <raw_news.json>")
        sys.exit(1)
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’HKTã«è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°HKTï¼‰
    os.environ['TZ'] = os.environ.get('TZ', 'Asia/Hong_Kong')
    
    news_file = sys.argv[1]
    
    # ä»Šæ—¥ã®æ—¥ä»˜ã‚’è¡¨ç¤º
    today = datetime.now(HKT).strftime('%Y-%m-%d')
    print(f"\nğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (HKT): {today}")
    print(f"ğŸ“… ä»Šæ—¥ã®æ—¥ä»˜ (æ—¥æœ¬èª): {datetime.now(HKT).strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    print("=" * 60)
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\nğŸ” ãƒ‹ãƒ¥ãƒ¼ã‚¹äº‹å‰å‡¦ç†é–‹å§‹")
    print("=" * 60)
    
    # äº‹å‰å‡¦ç†ï¼šé‡è¤‡é™¤å¤–ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ†é¡ã€ãƒãƒ©ãƒ³ã‚¹é¸æŠ
    news_data = preprocess_news(data['news'])
    
    print("=" * 60)
    
    generator = GrokArticleGenerator()
    article = generator.generate_article(news_data)
    
    if article:
        # å¤©æ°—æƒ…å ±ã‚‚å–å¾—ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        weather_data = data.get('weather', None)
        saved_path = generator.save_article(article, weather_data)
        
        # ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ—¥ä»˜ã‚’ç¢ºèª
        expected_date = datetime.now(HKT).strftime('%Y-%m-%d')
        file_date = saved_path.split('_')[-1].replace('.md', '')
        
        print(f"\nâœ… è¨˜äº‹ç”Ÿæˆå®Œäº†ï¼")
        print(f"ğŸ“ ä¿å­˜å…ˆ: {saved_path}")
        print(f"ğŸ“… ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜: {file_date}")
        print(f"ğŸ“… æœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜: {expected_date}")
        
        if file_date != expected_date:
            print(f"âš ï¸  è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ«æ—¥ä»˜ãŒæœŸå¾…ã•ã‚Œã‚‹æ—¥ä»˜ã¨ä¸€è‡´ã—ã¾ã›ã‚“ï¼")
            print(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {file_date}, æœŸå¾…: {expected_date}")
        
        print(f"\nğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {article['title']}")
        if weather_data:
            print(f"ğŸŒ¤ï¸  å¤©æ°—æƒ…å ±ã‚‚è¿½åŠ ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)
