#!/usr/bin/env python3
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®å…¨æ–‡ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
"""

import requests
from bs4 import BeautifulSoup
from typing import Dict, Optional
import time

class ArticleScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
    
    def scrape_article(self, url: str) -> Optional[str]:
        """URLã‹ã‚‰è¨˜äº‹å…¨æ–‡ã‚’å–å¾—"""
        try:
            # Hong Kong Free Pressã‚’é™¤å¤–
            if 'hongkongfp.com' in url.lower():
                print(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: Hong Kong Free Press")
                return None
            
            # åºƒå‘Šè¨˜äº‹ã‚’é™¤å¤–
            if '/presented/' in url.lower() or '/sponsored/' in url.lower() or '/advertorial/' in url.lower():
                print(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: åºƒå‘Šè¨˜äº‹")
                return None
            
            print(f"  ğŸ“° è¨˜äº‹å–å¾—ä¸­: {url[:60]}...")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ã‚µã‚¤ãƒˆåˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            if 'thestandard.com.hk' in url:
                content = self._scrape_standard(soup)
            elif 'scmp.com' in url:
                content = self._scrape_scmp(soup)
            else:
                # ä¸€èˆ¬çš„ãªè¨˜äº‹æ§‹é€ ã‚’è©¦ã™
                content = self._scrape_generic(soup)
            
            if content and len(content) > 200:
                print(f"    âœ… {len(content)}æ–‡å­—å–å¾—")
                return content
            else:
                print(f"    âš ï¸  è¨˜äº‹æœ¬æ–‡ãŒçŸ­ã™ãã‚‹")
                return None
                
        except Exception as e:
            print(f"    âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {e}")
            return None
    
    def _scrape_standard(self, soup: BeautifulSoup) -> Optional[str]:
        """The Standardå°‚ç”¨"""
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        selectors = [
            ('div', {'class': 'article-content'}),
            ('div', {'class': 'article-body'}),
            ('div', {'class': 'content'}),
            ('article', {}),
            ('div', {'id': 'article-content'}),
        ]
        
        for tag, attrs in selectors:
            article = soup.find(tag, attrs)
            if article:
                paragraphs = article.find_all('p')
                if paragraphs:
                    content = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                    if len(content) > 100:
                        return content
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ã¦ã®pã‚¿ã‚°ã‚’å–å¾—
        paragraphs = soup.find_all('p')
        if paragraphs:
            content = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip() and len(p.get_text()) > 50])
            return content
        
        return None
    
    def _scrape_scmp(self, soup: BeautifulSoup) -> Optional[str]:
        """South China Morning Postå°‚ç”¨"""
        article = soup.find('article')
        if article:
            paragraphs = article.find_all('p')
            content = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
            return content
        return None
    
    def _scrape_generic(self, soup: BeautifulSoup) -> Optional[str]:
        """ä¸€èˆ¬çš„ãªè¨˜äº‹æ§‹é€ """
        # articleã‚¿ã‚°ã‚’æ¢ã™
        article = soup.find('article')
        if not article:
            # main contentã‚’æ¢ã™
            article = soup.find('div', class_=['content', 'article-content', 'post-content', 'entry-content'])
        
        if article:
            paragraphs = article.find_all('p')
            content = '\n\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
            return content
        return None
    
    def enrich_news_data(self, news_list: list) -> list:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã«å…¨æ–‡ã‚’è¿½åŠ """
        print("\nğŸ“° è¨˜äº‹å…¨æ–‡ã‚’å–å¾—ä¸­...")
        print("=" * 60)
        
        enriched = []
        for i, news in enumerate(news_list, 1):
            print(f"\n[{i}/{len(news_list)}]")
            
            full_content = self.scrape_article(news['url'])
            
            news_enriched = news.copy()
            if full_content:
                news_enriched['full_content'] = full_content
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: descriptionã‚’ä½¿ç”¨
                news_enriched['full_content'] = news.get('description', '')
            
            enriched.append(news_enriched)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(1)
        
        print("\n" + "=" * 60)
        print(f"âœ… å…¨æ–‡å–å¾—å®Œäº†: {len(enriched)}ä»¶\n")
        
        return enriched

if __name__ == "__main__":
    import json
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python scrape_article.py <raw_news.json>")
        sys.exit(1)
    
    news_file = sys.argv[1]
    
    with open(news_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    scraper = ArticleScraper()
    enriched_news = scraper.enrich_news_data(data['news'])
    
    # ä¿å­˜
    output_file = news_file.replace('.json', '_enriched.json')
    data['news'] = enriched_news
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")

