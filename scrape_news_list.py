#!/usr/bin/env python3
"""
é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆPlaywrightä½¿ç”¨ï¼‰
"""

import os
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import time
import re
from urllib.parse import urljoin

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

# Playwrightã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸  Playwright not available, falling back to requests")

class NewsListScraper:
    def __init__(self):
        self.use_playwright = PLAYWRIGHT_AVAILABLE
        self.requests = None
        self.BeautifulSoup = None
        self.session = None
        
        if not self.use_playwright:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: requests + BeautifulSoup
            try:
                import requests
                from bs4 import BeautifulSoup
                self.requests = requests
                self.BeautifulSoup = BeautifulSoup
                self.headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh;q=0.7',
                }
                self.session = requests.Session()
                self.session.headers.update(self.headers)
            except ImportError:
                print("âš ï¸  requests/BeautifulSoup also not available")
                self.session = None
    
    def scrape_hk01(self) -> List[Dict]:
        """HK01ï¼ˆé¦™æ¸¯01ï¼‰ã‹ã‚‰å–å¾— - RSSãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("\nğŸ“° HK01 ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            # HK01ã®ä¸»è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã¨ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ï¼‰
            urls = [
                'https://www.hk01.com/',  # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
                'https://www.hk01.com/zone/1',  # æ¸¯è
                'https://www.hk01.com/channel/310',  # æ”¿æƒ…
                'https://www.hk01.com/channel/4',  # ç¶“æ¿Ÿ
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    context = browser.new_context(
                        user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        viewport={'width': 1920, 'height': 1080}
                    )
                    page = context.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            page.wait_for_timeout(8000)  # JavaScriptã®å®Ÿè¡Œã‚’å¾…ã¤ï¼ˆ8ç§’ã«å¢—åŠ ï¼‰
                            
                            # HK01ã¯Next.jsã®SPAã§ã€è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã¯JSONã«åŸ‹ã‚è¾¼ã¾ã‚Œã¦ã„ã‚‹
                            # ãƒšãƒ¼ã‚¸å†…ã®JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨˜äº‹æƒ…å ±ã‚’æŠ½å‡º
                            try:
                                # __NEXT_DATA__ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°ã‹ã‚‰JSONã‚’å–å¾—
                                json_script = page.query_selector('script#__NEXT_DATA__')
                                if json_script:
                                    json_text = json_script.inner_text()
                                    import json as json_lib
                                    data = json_lib.loads(json_text)
                                    
                                    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                                    articles_data = []
                                    
                                    # props.initialProps.pageProps.sections ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
                                    def extract_articles_from_data(data_obj, articles_list):
                                        """å†å¸°çš„ã«è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
                                        if isinstance(data_obj, dict):
                                            # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³1: data.articleId ã¨ data.canonicalUrl ãŒã‚ã‚‹
                                            if 'articleId' in data_obj and 'canonicalUrl' in data_obj:
                                                article_url = data_obj.get('canonicalUrl', '')
                                                article_title = data_obj.get('title', '')
                                                article_id = data_obj.get('articleId', '')
                                                
                                                if article_url and ('/article/' in article_url or article_id):
                                                    # URLæ­£è¦åŒ–
                                                    if article_url.startswith('/'):
                                                        full_url = urljoin('https://www.hk01.com', article_url)
                                                    elif article_url.startswith('http'):
                                                        full_url = article_url.split('?')[0].split('#')[0]
                                                    else:
                                                        # articleIdã‹ã‚‰URLã‚’æ§‹ç¯‰
                                                        full_url = f"https://www.hk01.com/article/{article_id}"
                                                    
                                                    if article_title and len(article_title) > 5:
                                                        articles_list.append({
                                                            'title': article_title,
                                                            'url': full_url,
                                                            'id': article_id
                                                        })
                                            else:
                                                # å†å¸°çš„ã«æ¢ç´¢
                                                for key, value in data_obj.items():
                                                    extract_articles_from_data(value, articles_list)
                                        elif isinstance(data_obj, list):
                                            for item in data_obj:
                                                extract_articles_from_data(item, articles_list)
                                    
                                    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æ¢ç´¢
                                    if 'props' in data:
                                        extract_articles_from_data(data['props'], articles_data)
                                    
                                    print(f"    ğŸ“° JSONã‹ã‚‰ {len(articles_data)}ä»¶ã®è¨˜äº‹ã‚’æŠ½å‡º")
                                    
                                    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’news_listã«è¿½åŠ 
                                    for article_data in articles_data[:100]:  # æœ€å¤§100ä»¶
                                        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                        if not any(n['url'] == article_data['url'] for n in news_list):
                                            news_list.append({
                                                'title': article_data['title'],
                                                'url': article_data['url'],
                                                'source': 'HK01',
                                                'published_at': datetime.now(HKT).isoformat()
                                            })
                                    
                                    if len(articles_data) > 0:
                                        continue  # JSONã‹ã‚‰å–å¾—ã§ããŸã®ã§ã€æ¬¡ã®URLã¸
                            except Exception as e:
                                print(f"    âš ï¸  JSONæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                            
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—
                            print("    ğŸ” HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’å–å¾—ä¸­...")
                            selectors = [
                                'a[href*="/article/"]',
                                'a[href^="/article/"]',
                            ]
                            
                            articles = []
                            for selector in selectors:
                                found = page.query_selector_all(selector)
                                if found:
                                    articles.extend(found)
                                    if len(articles) >= 50:
                                        break
                            
                            print(f"    ğŸ“° HTMLãƒªãƒ³ã‚¯: {len(articles)}ä»¶")
                            
                            for article in articles[:100]:  # æœ€å¤§100ä»¶ã¾ã§
                                try:
                                    href = article.get_attribute('href')
                                    if not href:
                                        continue
                                    
                                    # articleã‚’å«ã‚€URLã®ã¿ã‚’å¯¾è±¡ï¼ˆåºƒå‘Šã‚„JSãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ï¼‰
                                    if '/article/' not in href and 'article' not in href.lower():
                                        continue
                                    
                                    # JavaScriptãƒ•ã‚¡ã‚¤ãƒ«ã‚„åºƒå‘ŠURLã‚’é™¤å¤–
                                    if '/_next/' in href or 'omgt3.com' in href or 'clk.' in href or '.js' in href:
                                        continue
                                    
                                    # URLæ­£è¦åŒ–
                                    if href.startswith('/'):
                                        full_url = urljoin('https://www.hk01.com', href)
                                    elif href.startswith('http'):
                                        full_url = href
                                    else:
                                        continue
                                    
                                    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–
                                    full_url = full_url.split('?')[0].split('#')[0]
                                    
                                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰- å…ˆã«ãƒã‚§ãƒƒã‚¯
                                    if any(n['url'] == full_url for n in news_list):
                                        continue
                                    
                                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã‚’è©¦ã™ï¼‰
                                    title = None
                                    title_selectors = ['h2', 'h3', 'h4', '.title', '.article-title', '[class*="title"]', 'span', 'div']
                                    
                                    for title_sel in title_selectors:
                                        try:
                                            title_elem = article.query_selector(title_sel)
                                            if title_elem:
                                                title_text = title_elem.inner_text().strip()
                                                if title_text and len(title_text) > 5:
                                                    title = title_text
                                                    break
                                        except:
                                            continue
                                    
                                    # ã‚»ãƒ¬ã‚¯ã‚¿ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
                                    if not title or len(title) <= 5:
                                        try:
                                            title = article.inner_text().strip()
                                        except:
                                            title = ''
                                    
                                    # ã‚¿ã‚¤ãƒˆãƒ«ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿½åŠ 
                                    if title and len(title) > 5 and len(title) < 300:
                                        # åºƒå‘Šã‚„ä¸è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’é™¤å¤–
                                        if any(skip in title.lower() for skip in ['å»£å‘Š', 'advertisement', 'æ¨å»£', 'promotion', 'click here', 'javascript']):
                                            continue
                                        
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'HK01',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                        
                                        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                                        if len([n for n in news_list if n.get('source') == 'HK01']) <= 5:
                                            print(f"      âœ… å–å¾—: {title[:50]}... | {full_url}")
                                except Exception as e:
                                    # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®ã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤º
                                    if len(news_list) == 0:
                                        print(f"      âš ï¸  ã‚¨ãƒ©ãƒ¼: {e}")
                                    continue
                            
                            time.sleep(2)  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’2ç§’ã«
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            import traceback
                            traceback.print_exc()
                            continue
                    
                    browser.close()
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆrequestsã§ã¯å–å¾—å›°é›£ï¼‰
                print("  âš ï¸  HK01ã¯JavaScriptã§å‹•çš„ç”Ÿæˆã•ã‚Œã‚‹ãŸã‚ã€PlaywrightãŒå¿…è¦ã§ã™")
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_mingpao(self) -> List[Dict]:
        """æ˜å ±ï¼ˆMing Paoï¼‰ã‹ã‚‰å–å¾— - RSSãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("\nğŸ“° æ˜å ± ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://news.mingpao.com/pns/æ¸¯è',
                'https://news.mingpao.com/pns/è¦è',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            page.wait_for_timeout(5000)
                            
                            articles = page.query_selector_all('a[href*="/pns/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href or '/article/' not in href:
                                        continue
                                    
                                    full_url = urljoin('https://news.mingpao.com', href)
                                    title = article.inner_text().strip()
                                    
                                    if title and len(title) > 5:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'æ˜å ±',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_am730(self) -> List[Dict]:
        """am730ã‹ã‚‰å–å¾— - RSSãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("\nğŸ“° am730 ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.am730.com.hk/news',
                'https://www.am730.com.hk/news/local',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            page.wait_for_timeout(5000)
                            
                            articles = page.query_selector_all('a[href*="/news/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href or '/news/' not in href:
                                        continue
                                    
                                    full_url = urljoin('https://www.am730.com.hk', href)
                                    title = article.inner_text().strip()
                                    
                                    if title and len(title) > 5:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'am730',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_scmp_hongkong(self) -> List[Dict]:
        """SCMP é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° SCMP Hong Kong ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.scmp.com/news/hong-kong',
                'https://www.scmp.com/news/hong-kong/politics',
                'https://www.scmp.com/news/hong-kong/society',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='networkidle', timeout=30000)
                            page.wait_for_timeout(2000)  # JavaScriptã®å®Ÿè¡Œã‚’å¾…ã¤
                            
                            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
                            articles = page.query_selector_all('a[href*="/article/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href:
                                        continue
                                    
                                    full_url = urljoin('https://www.scmp.com', href)
                                    
                                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                                    title_elem = article.query_selector('h2, h3, h4, span')
                                    if not title_elem:
                                        title = article.inner_text().strip()
                                    else:
                                        title = title_elem.inner_text().strip()
                                    
                                    if title and len(title) > 10:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'SCMP',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception as e:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            elif self.session:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: requests
                for url in urls:
                    try:
                        response = self.session.get(url, timeout=10)
                        response.raise_for_status()
                        soup = self.BeautifulSoup(response.content, 'html.parser')
                        
                        articles = soup.find_all('a', href=re.compile(r'/article/\d+'))
                        
                        for article in articles[:20]:
                            href = article.get('href')
                            if not href:
                                continue
                            
                            full_url = urljoin('https://www.scmp.com', href)
                            title_elem = article.find(['h2', 'h3', 'h4', 'span'])
                            if not title_elem:
                                title_elem = article
                            title = title_elem.get_text(strip=True)
                            
                            if title and len(title) > 10:
                                news_list.append({
                                    'title': title,
                                    'url': full_url,
                                    'source': 'SCMP',
                                    'published_at': datetime.now(HKT).isoformat()
                                })
                        
                        time.sleep(1)
                    except Exception as e:
                        print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            # é‡è¤‡é™¤å»
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_thestandard(self) -> List[Dict]:
        """The Standard ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° The Standard ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.thestandard.com.hk/section/2/latest',
                'https://www.thestandard.com.hk/section/4/latest',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='networkidle', timeout=30000)
                            page.wait_for_timeout(2000)
                            
                            articles = page.query_selector_all('a[href*="/article/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href:
                                        continue
                                    
                                    full_url = urljoin('https://www.thestandard.com.hk', href)
                                    title = article.inner_text().strip()
                                    
                                    if title and len(title) > 10:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'The Standard',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            elif self.session:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                for url in urls:
                    try:
                        response = self.session.get(url, timeout=10)
                        response.raise_for_status()
                        soup = self.BeautifulSoup(response.content, 'html.parser')
                        
                        articles = soup.find_all('a', href=re.compile(r'/[a-z-]+/article/\d+'))
                        
                        for article in articles[:30]:
                            href = article.get('href')
                            if not href:
                                continue
                            
                            full_url = urljoin('https://www.thestandard.com.hk', href)
                            title_elem = article.find(['h2', 'h3', 'h4'])
                            if not title_elem:
                                title_elem = article
                            title = title_elem.get_text(strip=True)
                            
                            if title and len(title) > 10:
                                news_list.append({
                                    'title': title,
                                    'url': full_url,
                                    'source': 'The Standard',
                                    'published_at': datetime.now(HKT).isoformat()
                                })
                        
                        time.sleep(1)
                    except Exception as e:
                        print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_rthk_news(self) -> List[Dict]:
        """RTHK English News ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° RTHK News ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            url = 'https://news.rthk.hk/rthk/en/component/k2/index-archive.htm'
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    try:
                        print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                        page.goto(url, wait_until='networkidle', timeout=30000)
                        page.wait_for_timeout(2000)
                        
                        articles = page.query_selector_all('a[href*="/component/k2/"]')
                        
                        for article in articles[:50]:
                            try:
                                href = article.get_attribute('href')
                                if not href:
                                    continue
                                
                                full_url = urljoin('https://news.rthk.hk', href)
                                title = article.inner_text().strip()
                                
                                if title and len(title) > 10:
                                    news_list.append({
                                        'title': title,
                                        'url': full_url,
                                        'source': 'RTHK',
                                        'published_at': datetime.now(HKT).isoformat()
                                    })
                            except Exception:
                                continue
                    except Exception as e:
                        print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    browser.close()
            elif self.session:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = self.BeautifulSoup(response.content, 'html.parser')
                    
                    articles = soup.find_all('a', href=re.compile(r'/rthk/en/component/k2/\d+'))
                    
                    for article in articles[:40]:
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://news.rthk.hk', href)
                        title = article.get_text(strip=True)
                        
                        if title and len(title) > 10:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'RTHK',
                                'published_at': datetime.now(HKT).isoformat()
                            })
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _deduplicate_by_url(self, news_list: List[Dict]) -> List[Dict]:
        """URLé‡è¤‡ã‚’é™¤å»"""
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            url = news.get('url', '')
            normalized_url = url.split('?')[0]
            
            if normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_news.append(news)
        
        return unique_news
    
    def fetch_all_news(self) -> List[Dict]:
        """å…¨ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        print("\n" + "=" * 60)
        print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        if self.use_playwright:
            print("ğŸ“¦ Playwrightä½¿ç”¨")
        else:
            print("ğŸ“¦ Requestsä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        print("=" * 60)
        
        all_news = []
        
        # å„ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—ï¼ˆRSSãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯å–å¾—ã§ããªã„ã‚µã‚¤ãƒˆã®ã¿ï¼‰
        scrapers = [
            self.scrape_hk01,  # HK01 - RSSãŒå­˜åœ¨ã—ãªã„
            self.scrape_mingpao,  # æ˜å ± - RSSãŒå­˜åœ¨ã—ãªã„
            self.scrape_am730,  # am730 - RSSãŒå­˜åœ¨ã—ãªã„
            # SCMP, The Standard, RTHKã¯RSSã§å–å¾—æ¸ˆã¿ã®ãŸã‚é™¤å¤–
        ]
        
        for scraper in scrapers:
            try:
                news = scraper()
                all_news.extend(news)
            except Exception as e:
                print(f"  âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # å…¨ä½“ã§é‡è¤‡é™¤å»
        unique_news = self._deduplicate_by_url(all_news)
        
        print("\n" + "=" * 60)
        print(f"âœ… åˆè¨ˆ {len(unique_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        print("=" * 60)
        
        # ç¾åœ¨æ™‚åˆ»ã¨ã‚½ãƒ¼ã‚¹çµ±è¨ˆã‚’è¿½åŠ 
        result = []
        for news in unique_news:
            news['description'] = news.get('title', '')  # å¾Œã§å…¨æ–‡å–å¾—ã§ä¸Šæ›¸ã
            news['api_source'] = 'web_scraping'
            result.append(news)
        
        return result

if __name__ == "__main__":
    import json
    
    scraper = NewsListScraper()
    news_list = scraper.fetch_all_news()
    
    if news_list:
        # JSONã§ä¿å­˜
        output_file = f'daily-articles/scraped_news_{datetime.now(HKT).strftime("%Y-%m-%d_%H-%M-%S")}.json'
        
        output_data = {
            'fetch_time': datetime.now(HKT).isoformat(),
            'total_count': len(news_list),
            'news': news_list
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        from collections import Counter
        sources = Counter([n.get('source', 'Unknown') for n in news_list])
        
        print("\nğŸ“Š ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"  {source}: {count}ä»¶")
    else:
        print("\nâŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
