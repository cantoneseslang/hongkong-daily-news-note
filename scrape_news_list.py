#!/usr/bin/env python3
"""
é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆPlaywrightä½¿ç”¨ï¼‰
"""

import os
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import time
import re
from urllib.parse import urljoin

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

# Playwrightã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸  Playwright not available, falling back to requests")

class NewsListScraper:
    def __init__(self):
        self.use_playwright = PLAYWRIGHT_AVAILABLE
        self.requests = None
        self.BeautifulSoup = None
        self.session = None
        
        if not self.use_playwright:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: requests + BeautifulSoup
            try:
                import requests
                from bs4 import BeautifulSoup
                self.requests = requests
                self.BeautifulSoup = BeautifulSoup
                self.headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh;q=0.7',
                }
                self.session = requests.Session()
                self.session.headers.update(self.headers)
            except ImportError:
                print("âš ï¸  requests/BeautifulSoup also not available")
                self.session = None
    
    def scrape_hk01(self) -> List[Dict]:
        """HK01ï¼ˆé¦™æ¸¯01ï¼‰ã‹ã‚‰å–å¾— - RSSãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("\nğŸ“° HK01 ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.hk01.com/zone/1/æ¸¯è',  # æ¸¯è
                'https://www.hk01.com/channel/2/ç¤¾æœƒæ–°è',  # ç¤¾æœƒæ–°è
                'https://www.hk01.com/channel/310/æ”¿æƒ…',  # æ”¿æƒ…
                'https://www.hk01.com/channel/4/ç¶“æ¿Ÿ',  # ç¶“æ¿Ÿ
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            page.wait_for_timeout(5000)  # JavaScriptã®å®Ÿè¡Œã‚’å¾…ã¤
                            
                            # HK01ã®è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
                            articles = page.query_selector_all('a[href*="/article/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href:
                                        continue
                                    
                                    full_url = urljoin('https://www.hk01.com', href)
                                    
                                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                                    title_elem = article.query_selector('h2, h3, h4, .article-title')
                                    if not title_elem:
                                        title = article.inner_text().strip()
                                    else:
                                        title = title_elem.inner_text().strip()
                                    
                                    if title and len(title) > 5:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'HK01',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆrequestsã§ã¯å–å¾—å›°é›£ï¼‰
                print("  âš ï¸  HK01ã¯JavaScriptã§å‹•çš„ç”Ÿæˆã•ã‚Œã‚‹ãŸã‚ã€PlaywrightãŒå¿…è¦ã§ã™")
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_mingpao(self) -> List[Dict]:
        """æ˜å ±ï¼ˆMing Paoï¼‰ã‹ã‚‰å–å¾— - RSSãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("\nğŸ“° æ˜å ± ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://news.mingpao.com/pns/æ¸¯è',
                'https://news.mingpao.com/pns/è¦è',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            page.wait_for_timeout(5000)
                            
                            articles = page.query_selector_all('a[href*="/pns/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href or '/article/' not in href:
                                        continue
                                    
                                    full_url = urljoin('https://news.mingpao.com', href)
                                    title = article.inner_text().strip()
                                    
                                    if title and len(title) > 5:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'æ˜å ±',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_am730(self) -> List[Dict]:
        """am730ã‹ã‚‰å–å¾— - RSSãŒå­˜åœ¨ã—ãªã„ãŸã‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        print("\nğŸ“° am730 ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.am730.com.hk/news',
                'https://www.am730.com.hk/news/local',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            page.wait_for_timeout(5000)
                            
                            articles = page.query_selector_all('a[href*="/news/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href or '/news/' not in href:
                                        continue
                                    
                                    full_url = urljoin('https://www.am730.com.hk', href)
                                    title = article.inner_text().strip()
                                    
                                    if title and len(title) > 5:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'am730',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_scmp_hongkong(self) -> List[Dict]:
        """SCMP é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° SCMP Hong Kong ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.scmp.com/news/hong-kong',
                'https://www.scmp.com/news/hong-kong/politics',
                'https://www.scmp.com/news/hong-kong/society',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='networkidle', timeout=30000)
                            page.wait_for_timeout(2000)  # JavaScriptã®å®Ÿè¡Œã‚’å¾…ã¤
                            
                            # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’å–å¾—
                            articles = page.query_selector_all('a[href*="/article/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href:
                                        continue
                                    
                                    full_url = urljoin('https://www.scmp.com', href)
                                    
                                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                                    title_elem = article.query_selector('h2, h3, h4, span')
                                    if not title_elem:
                                        title = article.inner_text().strip()
                                    else:
                                        title = title_elem.inner_text().strip()
                                    
                                    if title and len(title) > 10:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'SCMP',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception as e:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            elif self.session:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: requests
                for url in urls:
                    try:
                        response = self.session.get(url, timeout=10)
                        response.raise_for_status()
                        soup = self.BeautifulSoup(response.content, 'html.parser')
                        
                        articles = soup.find_all('a', href=re.compile(r'/article/\d+'))
                        
                        for article in articles[:20]:
                            href = article.get('href')
                            if not href:
                                continue
                            
                            full_url = urljoin('https://www.scmp.com', href)
                            title_elem = article.find(['h2', 'h3', 'h4', 'span'])
                            if not title_elem:
                                title_elem = article
                            title = title_elem.get_text(strip=True)
                            
                            if title and len(title) > 10:
                                news_list.append({
                                    'title': title,
                                    'url': full_url,
                                    'source': 'SCMP',
                                    'published_at': datetime.now(HKT).isoformat()
                                })
                        
                        time.sleep(1)
                    except Exception as e:
                        print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            # é‡è¤‡é™¤å»
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_thestandard(self) -> List[Dict]:
        """The Standard ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° The Standard ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.thestandard.com.hk/section/2/latest',
                'https://www.thestandard.com.hk/section/4/latest',
            ]
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    for url in urls:
                        try:
                            print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                            page.goto(url, wait_until='networkidle', timeout=30000)
                            page.wait_for_timeout(2000)
                            
                            articles = page.query_selector_all('a[href*="/article/"]')
                            
                            for article in articles[:30]:
                                try:
                                    href = article.get_attribute('href')
                                    if not href:
                                        continue
                                    
                                    full_url = urljoin('https://www.thestandard.com.hk', href)
                                    title = article.inner_text().strip()
                                    
                                    if title and len(title) > 10:
                                        news_list.append({
                                            'title': title,
                                            'url': full_url,
                                            'source': 'The Standard',
                                            'published_at': datetime.now(HKT).isoformat()
                                        })
                                except Exception:
                                    continue
                            
                            time.sleep(1)
                        except Exception as e:
                            print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    browser.close()
            elif self.session:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                for url in urls:
                    try:
                        response = self.session.get(url, timeout=10)
                        response.raise_for_status()
                        soup = self.BeautifulSoup(response.content, 'html.parser')
                        
                        articles = soup.find_all('a', href=re.compile(r'/[a-z-]+/article/\d+'))
                        
                        for article in articles[:30]:
                            href = article.get('href')
                            if not href:
                                continue
                            
                            full_url = urljoin('https://www.thestandard.com.hk', href)
                            title_elem = article.find(['h2', 'h3', 'h4'])
                            if not title_elem:
                                title_elem = article
                            title = title_elem.get_text(strip=True)
                            
                            if title and len(title) > 10:
                                news_list.append({
                                    'title': title,
                                    'url': full_url,
                                    'source': 'The Standard',
                                    'published_at': datetime.now(HKT).isoformat()
                                })
                        
                        time.sleep(1)
                    except Exception as e:
                        print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def scrape_rthk_news(self) -> List[Dict]:
        """RTHK English News ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° RTHK News ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            url = 'https://news.rthk.hk/rthk/en/component/k2/index-archive.htm'
            
            if self.use_playwright:
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()
                    
                    try:
                        print(f"  ğŸ“„ {url} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
                        page.goto(url, wait_until='networkidle', timeout=30000)
                        page.wait_for_timeout(2000)
                        
                        articles = page.query_selector_all('a[href*="/component/k2/"]')
                        
                        for article in articles[:50]:
                            try:
                                href = article.get_attribute('href')
                                if not href:
                                    continue
                                
                                full_url = urljoin('https://news.rthk.hk', href)
                                title = article.inner_text().strip()
                                
                                if title and len(title) > 10:
                                    news_list.append({
                                        'title': title,
                                        'url': full_url,
                                        'source': 'RTHK',
                                        'published_at': datetime.now(HKT).isoformat()
                                    })
                            except Exception:
                                continue
                    except Exception as e:
                        print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    browser.close()
            elif self.session:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = self.BeautifulSoup(response.content, 'html.parser')
                    
                    articles = soup.find_all('a', href=re.compile(r'/rthk/en/component/k2/\d+'))
                    
                    for article in articles[:40]:
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://news.rthk.hk', href)
                        title = article.get_text(strip=True)
                        
                        if title and len(title) > 10:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'RTHK',
                                'published_at': datetime.now(HKT).isoformat()
                            })
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _deduplicate_by_url(self, news_list: List[Dict]) -> List[Dict]:
        """URLé‡è¤‡ã‚’é™¤å»"""
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            url = news.get('url', '')
            normalized_url = url.split('?')[0]
            
            if normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_news.append(news)
        
        return unique_news
    
    def fetch_all_news(self) -> List[Dict]:
        """å…¨ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        print("\n" + "=" * 60)
        print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        if self.use_playwright:
            print("ğŸ“¦ Playwrightä½¿ç”¨")
        else:
            print("ğŸ“¦ Requestsä½¿ç”¨ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰")
        print("=" * 60)
        
        all_news = []
        
        # å„ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—ï¼ˆRSSãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯å–å¾—ã§ããªã„ã‚µã‚¤ãƒˆã®ã¿ï¼‰
        scrapers = [
            self.scrape_hk01,  # HK01 - RSSãŒå­˜åœ¨ã—ãªã„
            self.scrape_mingpao,  # æ˜å ± - RSSãŒå­˜åœ¨ã—ãªã„
            self.scrape_am730,  # am730 - RSSãŒå­˜åœ¨ã—ãªã„
            # SCMP, The Standard, RTHKã¯RSSã§å–å¾—æ¸ˆã¿ã®ãŸã‚é™¤å¤–
        ]
        
        for scraper in scrapers:
            try:
                news = scraper()
                all_news.extend(news)
            except Exception as e:
                print(f"  âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # å…¨ä½“ã§é‡è¤‡é™¤å»
        unique_news = self._deduplicate_by_url(all_news)
        
        print("\n" + "=" * 60)
        print(f"âœ… åˆè¨ˆ {len(unique_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        print("=" * 60)
        
        # ç¾åœ¨æ™‚åˆ»ã¨ã‚½ãƒ¼ã‚¹çµ±è¨ˆã‚’è¿½åŠ 
        result = []
        for news in unique_news:
            news['description'] = news.get('title', '')  # å¾Œã§å…¨æ–‡å–å¾—ã§ä¸Šæ›¸ã
            news['api_source'] = 'web_scraping'
            result.append(news)
        
        return result

if __name__ == "__main__":
    import json
    
    scraper = NewsListScraper()
    news_list = scraper.fetch_all_news()
    
    if news_list:
        # JSONã§ä¿å­˜
        output_file = f'daily-articles/scraped_news_{datetime.now(HKT).strftime("%Y-%m-%d_%H-%M-%S")}.json'
        
        output_data = {
            'fetch_time': datetime.now(HKT).isoformat(),
            'total_count': len(news_list),
            'news': news_list
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        from collections import Counter
        sources = Counter([n.get('source', 'Unknown') for n in news_list])
        
        print("\nğŸ“Š ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"  {source}: {count}ä»¶")
    else:
        print("\nâŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
