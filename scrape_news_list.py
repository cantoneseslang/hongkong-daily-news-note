#!/usr/bin/env python3
"""
é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‹ã‚‰ç›´æ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
from typing import List, Dict
import time
import re
from urllib.parse import urljoin, urlparse

# HKTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ï¼ˆUTC+8ï¼‰
HKT = timezone(timedelta(hours=8))

class NewsListScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh;q=0.7',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def scrape_scmp_hongkong(self) -> List[Dict]:
        """SCMP é¦™æ¸¯ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° SCMP Hong Kong ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.scmp.com/news/hong-kong',
                'https://www.scmp.com/news/hong-kong/politics',
                'https://www.scmp.com/news/hong-kong/society',
                'https://www.scmp.com/news/hong-kong/health-environment',
                'https://www.scmp.com/business/companies',
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # SCMPã®è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    articles = soup.find_all('a', href=re.compile(r'/article/\d+'))
                    
                    for article in articles[:20]:  # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰20ä»¶
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://www.scmp.com', href)
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                        title_elem = article.find(['h2', 'h3', 'h4', 'span'])
                        if not title_elem:
                            title_elem = article
                        title = title_elem.get_text(strip=True)
                        
                        if title and len(title) > 10:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'SCMP',
                                'category': url.split('/')[-1]
                            })
                    
                    time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # é‡è¤‡é™¤å»
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_thestandard(self) -> List[Dict]:
        """The Standard ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° The Standard ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.thestandard.com.hk/section/2/latest',
                'https://www.thestandard.com.hk/section/4/latest',
                'https://www.thestandard.com.hk/section/11/latest',
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    articles = soup.find_all('a', href=re.compile(r'/[a-z-]+/article/\d+'))
                    
                    for article in articles[:30]:
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://www.thestandard.com.hk', href)
                        
                        title_elem = article.find(['h2', 'h3', 'h4'])
                        if not title_elem:
                            title_elem = article
                        title = title_elem.get_text(strip=True)
                        
                        if title and len(title) > 10:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'The Standard'
                            })
                    
                    time.sleep(1)
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_rthk_news(self) -> List[Dict]:
        """RTHK English News ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° RTHK News ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://news.rthk.hk/rthk/en/component/k2/index-archive.htm',
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # RTHKã®è¨˜äº‹æ§‹é€ ã‚’æ¢ã™
                    articles = soup.find_all('a', href=re.compile(r'/rthk/en/component/k2/\d+'))
                    
                    for article in articles[:40]:
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://news.rthk.hk', href)
                        title = article.get_text(strip=True)
                        
                        if title and len(title) > 10:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'RTHK'
                            })
                    
                    time.sleep(1)
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_hk01(self) -> List[Dict]:
        """HK01 ã‹ã‚‰å–å¾—ï¼ˆä¸­å›½èªã‚µã‚¤ãƒˆï¼‰"""
        print("\nğŸ“° HK01 ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.hk01.com/zone/1/æ¸¯è',
                'https://www.hk01.com/channel/2/ç¤¾æœƒæ–°è',
                'https://www.hk01.com/channel/310/æ”¿æƒ…',
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # HK01ã®è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    articles = soup.find_all('a', href=re.compile(r'/article/\d+'))
                    
                    for article in articles[:30]:
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://www.hk01.com', href)
                        
                        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                        title_elem = article.find(['h2', 'h3', 'h4'])
                        if not title_elem:
                            title_elem = article
                        title = title_elem.get_text(strip=True)
                        
                        if title and len(title) > 5:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'HK01'
                            })
                    
                    time.sleep(1)
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def scrape_chinadaily_hk(self) -> List[Dict]:
        """China Daily HK Edition ã‹ã‚‰å–å¾—"""
        print("\nğŸ“° China Daily HK ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...")
        news_list = []
        
        try:
            urls = [
                'https://www.chinadailyhk.com/hk',
                'https://www.chinadailyhk.com/hong-kong',
            ]
            
            for url in urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # è¨˜äº‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    articles = soup.find_all('a', href=re.compile(r'/article/\d+|/articles/\d+'))
                    
                    for article in articles[:30]:
                        href = article.get('href')
                        if not href:
                            continue
                        
                        full_url = urljoin('https://www.chinadailyhk.com', href)
                        
                        title_elem = article.find(['h2', 'h3', 'h4'])
                        if not title_elem:
                            title_elem = article
                        title = title_elem.get_text(strip=True)
                        
                        if title and len(title) > 10:
                            news_list.append({
                                'title': title,
                                'url': full_url,
                                'source': 'China Daily HK'
                            })
                    
                    time.sleep(1)
                except Exception as e:
                    print(f"  âš ï¸  {url} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            unique_news = self._deduplicate_by_url(news_list)
            print(f"  âœ… {len(unique_news)}ä»¶å–å¾—")
            return unique_news
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _deduplicate_by_url(self, news_list: List[Dict]) -> List[Dict]:
        """URLé‡è¤‡ã‚’é™¤å»"""
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            url = news.get('url', '')
            # URLã‚’æ­£è¦åŒ–ï¼ˆã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é™¤å»ï¼‰
            normalized_url = url.split('?')[0]
            
            if normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_news.append(news)
        
        return unique_news
    
    def fetch_all_news(self) -> List[Dict]:
        """å…¨ã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        print("\n" + "=" * 60)
        print("ğŸš€ ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        print("=" * 60)
        
        all_news = []
        
        # å„ã‚µã‚¤ãƒˆã‹ã‚‰å–å¾—
        scrapers = [
            self.scrape_scmp_hongkong,
            self.scrape_thestandard,
            self.scrape_rthk_news,
            self.scrape_hk01,
            self.scrape_chinadaily_hk,
        ]
        
        for scraper in scrapers:
            try:
                news = scraper()
                all_news.extend(news)
            except Exception as e:
                print(f"  âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # å…¨ä½“ã§é‡è¤‡é™¤å»
        unique_news = self._deduplicate_by_url(all_news)
        
        print("\n" + "=" * 60)
        print(f"âœ… åˆè¨ˆ {len(unique_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—")
        print("=" * 60)
        
        # ç¾åœ¨æ™‚åˆ»ã¨ã‚½ãƒ¼ã‚¹çµ±è¨ˆã‚’è¿½åŠ 
        result = []
        for news in unique_news:
            news['published_at'] = datetime.now(HKT).isoformat()
            news['description'] = news.get('title', '')  # å¾Œã§å…¨æ–‡å–å¾—ã§ä¸Šæ›¸ã
            news['api_source'] = 'web_scraping'
            result.append(news)
        
        return result

if __name__ == "__main__":
    import json
    import sys
    
    scraper = NewsListScraper()
    news_list = scraper.fetch_all_news()
    
    if news_list:
        # JSONã§ä¿å­˜
        output_file = f'daily-articles/scraped_news_{datetime.now(HKT).strftime("%Y-%m-%d_%H-%M-%S")}.json'
        
        output_data = {
            'fetch_time': datetime.now(HKT).isoformat(),
            'total_count': len(news_list),
            'news': news_list
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: {output_file}")
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        sources = {}
        for news in news_list:
            source = news.get('source', 'Unknown')
            sources[source] = sources.get(source, 0) + 1
        
        print("\nğŸ“Š ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ:")
        for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True):
            print(f"  {source}: {count}ä»¶")
    else:
        print("\nâŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        sys.exit(1)

